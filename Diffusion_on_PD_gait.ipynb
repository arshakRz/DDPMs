{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVkFBzW3g1BB",
        "outputId": "182d02e3-bca5-40c3-b40d-c4f38ec44c35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QRV7otltE1oa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udl_ZfaNMXyX",
        "outputId": "70888023-3ec7-42b9-e361-49333c0aee7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34,)\n",
            "(29,)\n",
            "(29,)\n"
          ]
        }
      ],
      "source": [
        "subjects_PD=['S01','S02','S03','S04','S05','S06','S07','S09',\n",
        "'S10','S11','S12','S13','S14','S16','S17','S18','S19',\n",
        "'S21','S22','S23','S24','S28','S29','S30','S31',\n",
        "'S32','S33','S34','S35']\n",
        "\n",
        "subjects_All=['S01','S02','S03','S04','S05','S06','S07','S08','S09',\n",
        "'S10','S11','S12','S13','S14','S16','S17','S18','S19','S20',\n",
        "'S21','S22','S23','S24','S25','S26','S27','S28','S29','S30','S31',\n",
        "'S32','S33','S34','S35']\n",
        "\n",
        "subjects_All_date=['20210223','20191114','20191120','20191112','20191119','20200220','20191121',\n",
        "'20191126','20191128','20191203','20191204','20200108','20200109','20200121','20200122','20200123',\n",
        "'20200124','20200127','20200130','20200205','20200206_9339','20200207','20200213','20200214','20200218',\n",
        "'26','20200221','20210706','20210804','20200206_9629','202108','20191210','20191212','20191218','20200227']\n",
        "\n",
        "healthy_controls=['S08','S20','S27','S25','S26']\n",
        "\n",
        "Y_true=np.asarray([1,1,0,1,1,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,1,0,0,1,1,0,1,0,1])\n",
        "\n",
        "print(np.array(subjects_All).shape)\n",
        "print(np.array(subjects_PD).shape)\n",
        "print(Y_true.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_KhAHcGE3dt",
        "outputId": "89fc3c6b-1e4e-4baa-9d70-bd99359a7ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10925, 30, 45)\n",
            "(10925,)\n",
            "(1215, 30, 45)\n",
            "(1215,)\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "window_len = 30\n",
        "\n",
        "for idx,(subj,subj_date) in enumerate(zip(subjects_All,subjects_All_date)):\n",
        "\n",
        "    data=np.load('/content/drive/MyDrive/outputs_finetuned/Predictions_'+subj+'.npy')\n",
        "\n",
        "    data_normal = data\n",
        "\n",
        "    for ii in range(15):\n",
        "        for jj in range(3):\n",
        "            data_normal[:,ii,jj]=savgol_filter(data_normal[:,ii,jj],11,3)\n",
        "\n",
        "    x_vec=data_normal[:,1]-data_normal[:,4]\n",
        "    y_vec=data_normal[:,7]-data_normal[:,0]\n",
        "\n",
        "    x_vec/=np.linalg.norm(x_vec,keepdims=True,axis=-1)\n",
        "    y_vec/=np.linalg.norm(y_vec,keepdims=True,axis=-1)\n",
        "\n",
        "    z_vec=np.cross(x_vec,y_vec)\n",
        "\n",
        "    rotation_matrix=np.ones((len(x_vec),3,3))\n",
        "    rotation_matrix[:,:,0]=x_vec\n",
        "    rotation_matrix[:,:,1]=y_vec\n",
        "    rotation_matrix[:,:,2]=z_vec\n",
        "\n",
        "    data_normal=np.matmul(data_normal,rotation_matrix)\n",
        "\n",
        "    label = None\n",
        "\n",
        "    if subj in healthy_controls:\n",
        "      label = 0\n",
        "    elif subj in subjects_PD:\n",
        "      index = subjects_PD.index(subj)\n",
        "      label = Y_true[index]\n",
        "\n",
        "    data_normal = data_normal.reshape([data_normal.shape[0], data_normal.shape[1]*data_normal.shape[2]])\n",
        "\n",
        "    for ii in range(window_len,len(data)):\n",
        "      X.append(data_normal[ii-window_len:ii, :])\n",
        "      y.append(label)\n",
        "\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "# Split the data into two separate arrays: one for label 0 and one for label 1.\n",
        "y_0 = y[y == 0]\n",
        "y_1 = y[y == 1]\n",
        "X_0 = X[y == 0]\n",
        "X_1 = X[y == 1]\n",
        "\n",
        "# Split each label's data into training and test sets.\n",
        "test_size = 0.10  # 10% for the test data\n",
        "\n",
        "X_0_train, X_0_test, y_0_train, y_0_test = train_test_split(\n",
        "    X_0, y_0, test_size=test_size, random_state=42)\n",
        "\n",
        "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split(\n",
        "    X_1, y_1, test_size=test_size, random_state=42)\n",
        "\n",
        "# Combine the training and test sets for both y.\n",
        "X_train = np.concatenate((X_0_train, X_1_train))\n",
        "y_train = np.concatenate((y_0_train, y_1_train))\n",
        "X_test = np.concatenate((X_0_test, X_1_test))\n",
        "y_test = np.concatenate((y_0_test, y_1_test))\n",
        "\n",
        "rp = np.random.permutation(len(y_train))\n",
        "\n",
        "X_train = X_train[rp, : ,:]\n",
        "y_train = y_train[rp]\n",
        "\n",
        "orig_shape_train = X_train.shape\n",
        "orig_shape_test = X_test.shape\n",
        "\n",
        "X_train_flatten = X_train.reshape(orig_shape_train[0], orig_shape_train[1]*orig_shape_train[2])\n",
        "X_test_flatten = X_test.reshape(orig_shape_test[0], orig_shape_test[1]*orig_shape_test[2])\n",
        "\n",
        "X_train_flatten_normal = scaler.fit_transform(X_train_flatten)\n",
        "X_test_flatten_normal =scaler.transform(X_test_flatten)\n",
        "\n",
        "X_train = X_train_flatten_normal.reshape(orig_shape_train[0], orig_shape_train[1], orig_shape_train[2])\n",
        "X_test = X_test_flatten_normal.reshape(orig_shape_test[0], orig_shape_test[1], orig_shape_test[2])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e37yPEUfrGgS"
      },
      "outputs": [],
      "source": [
        "class Diffusion():\n",
        "\n",
        "  def __init__(self, input_size, steps = 1000, beta_start = 1e-4, beta_end = 0.02, num_classes = 2):\n",
        "    self.steps = steps\n",
        "    self.beta_start = beta_start\n",
        "    self.beta_end = beta_end\n",
        "    self.beta = np.linspace(beta_start, beta_end, steps)\n",
        "    self.alpha = 1-self.beta\n",
        "    self.alpha_hat = np.cumprod(self.alpha)\n",
        "    self.input_size = input_size\n",
        "    self.num_classes = num_classes\n",
        "    self.model = self.make_net(self.input_size)\n",
        "    self.discriminator_model = self.make_discriminator_net(self.input_size, self.num_classes)\n",
        "    self.coeff1 =  tf.Variable(np.divide(1, np.sqrt(self.alpha)))\n",
        "    self.coeff2 =  tf.Variable(np.divide(self.beta, np.sqrt(1-self.alpha_hat)))\n",
        "\n",
        "  def forward(self, x_t0, step):\n",
        "    noise = np.random.randn(*x_t0.shape)\n",
        "    return noise, np.sqrt(self.alpha_hat[step, np.newaxis, np.newaxis]) * x_t0 + np.sqrt(1-self.alpha_hat[step, np.newaxis, np.newaxis]) * noise\n",
        "\n",
        "  def load_net(self, checkpoint_name_model, checkpoint_name_discriminator_model, checkpoint_dir = \"/content/drive/MyDrive/checkpoints/\"):\n",
        "\n",
        "    self.model.load_weights(checkpoint_dir+checkpoint_name_model)\n",
        "    self.discriminator_model.load_weights(checkpoint_dir+checkpoint_name_discriminator_model)\n",
        "\n",
        "\n",
        "  def embedder(self, x, size):\n",
        "    x = layers.Dense(size)(x)\n",
        "    x = x_res = layers.Activation('relu')(x)\n",
        "    x = layers.Dense(size)(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dense(size)(x)\n",
        "    x = layers.Add()([x, x_res])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "  def transformer(self, x, info, num_heads, size):\n",
        "    att = layers.MultiHeadAttention(num_heads, size)(x, info)\n",
        "    x = layers.Add()([att, x])\n",
        "    x = x_res = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = self.embedder(x, x.shape[2])\n",
        "    x = layers.Add()([x_res, x])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def make_net(self, input_size):\n",
        "\n",
        "    x_input = layers.Input(shape=(input_size[0], input_size[1]))\n",
        "\n",
        "    x_ts_input = layers.Input(shape=(1,))\n",
        "\n",
        "    x_cg_input = layers.Input(shape=(1,))\n",
        "\n",
        "    size = 64\n",
        "\n",
        "    num_heads = 40\n",
        "\n",
        "    #x_ts = self.embedder(x_ts_input, size)\n",
        "    #x_ts = layers.Reshape((size, 1))(x_ts)\n",
        "    #x_ts = self.embedder(x_ts, input_size[0])\n",
        "    #x_ts = layers.Permute((2, 1))(x_ts)\n",
        "\n",
        "    x_ts = self.embedder(x_ts_input, size*input_size[0]//4)\n",
        "    x_ts = layers.Reshape((input_size[0], size//4))(x_ts)\n",
        "    x_ts = self.embedder(x_ts, size)\n",
        "\n",
        "\n",
        "    x_cg = layers.Masking(mask_value=-1.0)(x_cg_input)\n",
        "    x_cg = self.embedder(x_cg, size)\n",
        "    x_cg = layers.Reshape((size, 1))(x_cg)\n",
        "    x_cg = self.embedder(x_cg, input_size[0])\n",
        "    x_cg = layers.Permute((2, 1))(x_cg)\n",
        "\n",
        "    #x_cg = layers.Masking(mask_value=-1.0)(x_cg_input)\n",
        "    #x_cg = self.embedder(x_ts_input, size*input_size[0]//4)\n",
        "    #x_cg = layers.Reshape((input_size[0], size//4))(x_cg)\n",
        "    #x_cg = self.embedder(x_cg, size)\n",
        "\n",
        "    x = self.embedder(x_input, size)\n",
        "\n",
        "    pos_enc = self.positional_encoding(input_size[0], size)\n",
        "\n",
        "    x = x + pos_enc\n",
        "\n",
        "    #x = self.transformer(x, x, num_heads, size)\n",
        "\n",
        "    x = self.transformer(x, x, num_heads, size)\n",
        "\n",
        "    #x = self.transformer(x, x_ts, num_heads, size)\n",
        "\n",
        "    x = self.transformer(x, x_ts, num_heads, size)\n",
        "\n",
        "    x = self.transformer(x, x_cg, num_heads, size)\n",
        "\n",
        "    x = layers.Dense(input_size[1])(x)\n",
        "    #x = layers.Activation('relu')(x)\n",
        "    #x = layers.Dense(input_size[1])(x)\n",
        "    #x = layers.Activation('tanh')(x)\n",
        "\n",
        "    print(x.shape)\n",
        "\n",
        "    model = Model([x_input, x_ts_input, x_cg_input], x)\n",
        "\n",
        "    return model\n",
        "\n",
        "  def make_discriminator_net(self, input_size, num_classes):\n",
        "\n",
        "    x_input = layers.Input(shape=(input_size[0], input_size[1]))\n",
        "\n",
        "    size = 64\n",
        "\n",
        "    num_heads = 40\n",
        "\n",
        "    x = self.embedder(x_input, size)\n",
        "\n",
        "    pos_enc = self.positional_encoding(input_size[0], size)\n",
        "\n",
        "    x = x + pos_enc\n",
        "\n",
        "    x = self.transformer(x, x, num_heads, size)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    x = layers.Dense(size//2)(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dense(size//4)(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    print(x.shape)\n",
        "\n",
        "    model = Model(x_input, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train_discriminator(self, X, y, num_epochs, batch_size=64, learning_rate=0.001):\n",
        "      # Compile the model\n",
        "      self.discriminator_model.compile(\n",
        "          optimizer = tf.keras.optimizers.Adam(learning_rate),\n",
        "          loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "          metrics = ['accuracy']\n",
        "      )\n",
        "\n",
        "      # Train the model\n",
        "      self.discriminator_model.fit(\n",
        "          X, y,\n",
        "          epochs=num_epochs,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1\n",
        "      )\n",
        "\n",
        "      return\n",
        "\n",
        "  def compute_mu_from_epsilon(self, epsilon, x, t):\n",
        "    # Use tf.gather to select elements from coeff1 and coeff2\n",
        "    coeff1_t = tf.gather(self.coeff1, t)\n",
        "    coeff2_t = tf.gather(self.coeff2, t)\n",
        "\n",
        "    # Cast coeff1_t and coeff2_t to the same dtype as x and epsilon\n",
        "    coeff1_t = tf.cast(coeff1_t, dtype=x.dtype)\n",
        "    coeff2_t = tf.cast(coeff2_t, dtype=x.dtype)\n",
        "\n",
        "    # Expand dimensions to match the image dimensions\n",
        "    coeff1_t = tf.expand_dims(tf.expand_dims(coeff1_t, -1), -1)  # Shape becomes [batch_size, 1, 1]\n",
        "    coeff2_t = tf.expand_dims(tf.expand_dims(coeff2_t, -1), -1)  # Shape becomes [batch_size, 1, 1]\n",
        "\n",
        "    # Broadcast multiplication and subtraction across the image dimensions\n",
        "    return coeff1_t * (x - coeff2_t * epsilon)\n",
        "\n",
        "  def train(self, x, c, epochs, batch_size=64, checkpoint_dir='/content/drive/MyDrive/checkpoints'):\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    loss_crs_ent = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Ensure the checkpoint directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Build the optimizer with all trainable variables\n",
        "    all_trainable_variables = self.model.trainable_variables + self.discriminator_model.trainable_variables\n",
        "    optimizer.build(all_trainable_variables)\n",
        "\n",
        "    loss_ = []\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, t, c, epsilon):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "          epsilon_pred = self.model([x, t, c])\n",
        "          loss_simple = loss_fn(epsilon_pred, epsilon)\n",
        "          x = tf.cast(x, dtype=epsilon_pred.dtype)\n",
        "          mu_pred = self.compute_mu_from_epsilon(epsilon_pred, x, t)\n",
        "          c_pred = self.discriminator_model(mu_pred)\n",
        "          loss_diss = loss_crs_ent(c, c_pred)\n",
        "          loss_diss = tf.cast(loss_diss, dtype=loss_simple.dtype)\n",
        "          loss = tf.add(loss_simple, 0.01*loss_diss)\n",
        "\n",
        "        gradients_model = tape.gradient(loss, self.model.trainable_variables)\n",
        "        gradients_discriminator_model = tape.gradient(loss, self.discriminator_model.trainable_variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients_model, self.model.trainable_variables))\n",
        "        optimizer.apply_gradients(zip(gradients_discriminator_model, self.discriminator_model.trainable_variables))\n",
        "\n",
        "        del tape  # Dispose of the persistent tape\n",
        "\n",
        "        return loss\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Epoch:\", epoch + 1)\n",
        "      for batch_start in range(0, x.shape[0], batch_size):\n",
        "        batch_end = batch_start + batch_size\n",
        "\n",
        "        x0_batch = x[batch_start:batch_end, :]\n",
        "\n",
        "        t_batch = np.random.randint(low=1, high=self.steps, size=(x0_batch.shape[0],))\n",
        "\n",
        "        c_batch = c[batch_start:batch_end]\n",
        "\n",
        "        #if np.random.uniform(0, 1) < 0.2:\n",
        "        #    c_batch = -np.ones(c_batch.shape)\n",
        "\n",
        "        epsilon_batch, noisy_x_batch = self.forward(x0_batch, t_batch)\n",
        "\n",
        "        loss = train_step(noisy_x_batch, t_batch, c_batch, epsilon_batch)\n",
        "\n",
        "        if batch_start % 10 == 0:\n",
        "            print(\"Batch:\", batch_start, \"Loss:\", loss.numpy())\n",
        "\n",
        "      if (epoch + 1) % 50 == 0:\n",
        "        #checkpoint_callback.on_epoch_end(epoch, logs={'loss': float(loss)})\n",
        "        self.model.save_weights(os.path.join(checkpoint_dir, 'model_checkpoint.h50'))\n",
        "        self.discriminator_model.save_weights(os.path.join(checkpoint_dir, 'discriminator_model_checkpoint.h50'))\n",
        "\n",
        "\n",
        "      loss_.append(loss.numpy())\n",
        "\n",
        "    return np.asarray(loss_)\n",
        "\n",
        "  def positional_encoding(self, seq_length, embedding_dim):\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
        "    pos_enc = np.zeros((seq_length, embedding_dim))\n",
        "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
        "    pos_enc = pos_enc[np.newaxis, ...]\n",
        "    return tf.constant(pos_enc, dtype=tf.float32)\n",
        "\n",
        "  def sample(self, c = 2, w = 3, plot = 0):\n",
        "    im_list = []\n",
        "    xt = np.random.randn(self.input_size, self.input_size, 1)\n",
        "    xt = xt.reshape(-1, self.input_size, self.input_size, 1)\n",
        "    xt = tf.convert_to_tensor(xt)\n",
        "    im_list.append(np.squeeze(xt))\n",
        "    if plot == 1:\n",
        "      plt.figure(figsize=(10,8))\n",
        "    for t in range(self.steps-1, -1, -1):\n",
        "      if t>0:\n",
        "        z = np.random.randn(self.input_size, self.input_size, 1)\n",
        "        z = z.reshape(-1, self.input_size, self.input_size, 1)\n",
        "      else:\n",
        "        z = 0\n",
        "      t_m = tf.convert_to_tensor(np.reshape(t, (1,)))\n",
        "      pred = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(c, (1,)))], verbose = 0)\n",
        "      if w > 0:\n",
        "        pred_ng = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(-1, (1,)))], verbose = 0)\n",
        "        pred = (1+w)*pred - w*pred_ng\n",
        "      xt = 1/np.sqrt(self.alpha[t]) * (xt-((1-self.alpha[t])/np.sqrt(1-self.alpha_hat[t]))*pred) + np.sqrt(self.beta[t]) * z\n",
        "      if plot == 1:\n",
        "        if np.mod(t,100) == 0:\n",
        "          plt.subplot(1,10,t//100 + 1)\n",
        "          plt.imshow(np.squeeze(xt))\n",
        "          plt.axis(\"off\")\n",
        "      im_list.append(np.squeeze(xt))\n",
        "    if plot == 1:\n",
        "      plt.show()\n",
        "    return xt, np.asarray(im_list)\n",
        "\n",
        "  def sample_batch(self, w = 3, size=20):\n",
        "    xt = np.random.randn(self.input_size, self.input_size, size)\n",
        "    xt = xt.reshape(-1, self.input_size, self.input_size, 1)\n",
        "    xt = tf.convert_to_tensor(xt)\n",
        "    c = np.random.randint(0, 10, size)\n",
        "    for t in range(self.steps-1, -1, -1):\n",
        "      if t>0:\n",
        "        z = np.random.randn(self.input_size, self.input_size, size)\n",
        "        z = z.reshape(-1, self.input_size, self.input_size, 1)\n",
        "      else:\n",
        "        z = 0\n",
        "      t_m = tf.convert_to_tensor(np.reshape(np.repeat(t, size), (size,)))\n",
        "      pred = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(c, (size,)))], verbose = 0)\n",
        "      if w > 0:\n",
        "        pred_ng = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(np.repeat(-1, size), (size,)))], verbose = 0)\n",
        "        pred = (1+w)*pred - w*pred_ng\n",
        "      xt = 1/np.sqrt(self.alpha[t]) * (xt-((1-self.alpha[t])/np.sqrt(1-self.alpha_hat[t]))*pred) + np.sqrt(self.beta[t]) * z\n",
        "    return xt, c\n",
        "\n",
        "  def ddim_sample(self, steps, w = 5, size = 20, ddim_eta = 1, plot = 0):\n",
        "\n",
        "    c = np.random.randint(0, self.num_classes, size)\n",
        "    tao = np.asarray(list(range(0, self.steps, self.steps//steps))) + 1\n",
        "\n",
        "    ddim_alpha = self.alpha_hat[tao]\n",
        "    ddim_alpha_sqrt = np.sqrt(ddim_alpha)\n",
        "    ddim_alpha_prev = np.concatenate([self.alpha_hat[0:1], self.alpha_hat[tao[:-1]]])\n",
        "    ddim_sigma = (ddim_eta*((1-ddim_alpha_prev)/(1 -ddim_alpha)*(1-ddim_alpha/ddim_alpha_prev))**.5)\n",
        "    ddim_sqrt_one_minus_alpha = (1. - ddim_alpha) ** .5\n",
        "\n",
        "    xt = np.random.randn(self.input_size[0], self.input_size[1], size)\n",
        "    xt = xt.reshape(-1, self.input_size[0], self.input_size[1])\n",
        "    xt = tf.convert_to_tensor(xt)\n",
        "\n",
        "    for i in range(steps-1, -1, -1):\n",
        "      t_m = tf.convert_to_tensor(np.reshape(np.repeat(tao[i], size), (size,)))\n",
        "      eps = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(c, (size,)))], verbose = 0)\n",
        "      if w > 0:\n",
        "        eps_ng = self.model.predict([xt, t_m, tf.convert_to_tensor(np.reshape(np.repeat(-1, size), (size,)))], verbose = 0)\n",
        "        eps = (1+w)*eps - w*eps_ng\n",
        "      noise = np.random.randn(self.input_size[0], self.input_size[1], size)\n",
        "      noise = noise.reshape(-1, self.input_size[0], self.input_size[1])\n",
        "      xt = (ddim_alpha_prev[i] ** 0.5) * (xt - ddim_sqrt_one_minus_alpha[i] * eps) / (ddim_alpha[i] ** 0.5) + ((1 - ddim_alpha_prev[i] - ddim_sigma[i] **2 ) ** 0.5) * eps + ddim_sigma[i] * noise\n",
        "\n",
        "    return xt, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bS-SVjmoFaZ",
        "outputId": "e41f0a1a-493b-45c5-a30e-b0484dda9150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 30, 45)\n",
            "(None, 2)\n"
          ]
        }
      ],
      "source": [
        "diffusion = Diffusion(input_size = [30, 45])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYTEsmD3oKAH",
        "outputId": "17acc49e-1154-4341-c3d1-472df6919a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 30, 45)]             0         []                            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 30, 64)               2944      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 30, 64)               0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 30, 64)               4160      ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 30, 64)               0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 30, 64)               4160      ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 30, 64)               0         ['dense_14[0][0]',            \n",
            "                                                                     'activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 480)                  960       ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 30, 64)               0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 480)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 30, 64)               128       ['activation_14[0][0]']       \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 480)                  230880    ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.add (TFOpLambda)    (None, 30, 64)               0         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 480)                  0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 30, 64)               663104    ['tf.math.add[0][0]',         \n",
            " iHeadAttention)                                                     'tf.math.add[0][0]']         \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 480)                  230880    ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 30, 64)               0         ['multi_head_attention[0][0]',\n",
            "                                                                     'tf.math.add[0][0]']         \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 480)                  0         ['dense_2[0][0]',             \n",
            "                                                                     'activation[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 30, 64)               128       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 480)                  0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 30, 64)               4160      ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 480)                  960       ['activation_2[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 30, 64)               0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 30, 16)               0         ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 30, 64)               4160      ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 30, 64)               1088      ['reshape[0][0]']             \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 30, 64)               0         ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 30, 64)               0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 30, 64)               4160      ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 30, 64)               4160      ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " masking (Masking)           (None, 1)                    0         ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 30, 64)               0         ['dense_17[0][0]',            \n",
            "                                                                     'activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 30, 64)               0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 64)                   128       ['masking[0][0]']             \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 30, 64)               0         ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 30, 64)               4160      ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 64)                   0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 30, 64)               128       ['activation_17[0][0]']       \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 30, 64)               0         ['dense_5[0][0]',             \n",
            "                                                                     'activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 64)                   4160      ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 30, 64)               0         ['layer_normalization_5[0][0]'\n",
            "                                                                    , 'layer_normalization_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 30, 64)               0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 64)                   0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 30, 64)               128       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 30, 64)               128       ['activation_5[0][0]']        \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 64)                   4160      ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 30, 64)               663104    ['layer_normalization_7[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 64)                   0         ['dense_8[0][0]',             \n",
            "                                                                     'activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 30, 64)               0         ['multi_head_attention_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 64)                   0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 30, 64)               128       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 64)                   128       ['activation_8[0][0]']        \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 30, 64)               4160      ['layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 64, 1)                0         ['layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 30, 64)               0         ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 64, 30)               60        ['reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 30, 64)               4160      ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 64, 30)               0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 30, 64)               0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 64, 30)               930       ['activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 30, 64)               4160      ['activation_19[0][0]']       \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 64, 30)               0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 30, 64)               0         ['dense_20[0][0]',            \n",
            "                                                                     'activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 64, 30)               930       ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 30, 64)               0         ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 64, 30)               0         ['dense_11[0][0]',            \n",
            "                                                                     'activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 30, 64)               128       ['activation_20[0][0]']       \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 64, 30)               0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 30, 64)               0         ['layer_normalization_8[0][0]'\n",
            "                                                                    , 'layer_normalization_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 64, 30)               60        ['activation_11[0][0]']       \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 30, 64)               128       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " permute (Permute)           (None, 30, 64)               0         ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 30, 64)               663104    ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'permute[0][0]']             \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 30, 64)               0         ['multi_head_attention_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 30, 64)               128       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 30, 64)               4160      ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 30, 64)               0         ['dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 30, 64)               4160      ['activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 30, 64)               0         ['dense_22[0][0]']            \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 30, 64)               4160      ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 30, 64)               0         ['dense_23[0][0]',            \n",
            "                                                                     'activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 30, 64)               0         ['add_12[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 30, 64)               128       ['activation_23[0][0]']       \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 30, 64)               0         ['layer_normalization_11[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_13 (La  (None, 30, 64)               128       ['add_13[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 30, 45)               2925      ['layer_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2525993 (9.64 MB)\n",
            "Trainable params: 2525993 (9.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "diffusion.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McU449v1oKeR",
        "outputId": "2ad84926-c1a7-4301-8023-4fa6e4d6bea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch: 960 Loss: 0.0640061012469414\n",
            "Batch: 1280 Loss: 0.04436071938021287\n",
            "Batch: 1600 Loss: 0.0643644503673419\n",
            "Batch: 1920 Loss: 0.040945458125744495\n",
            "Batch: 2240 Loss: 0.05056986245296275\n",
            "Batch: 2560 Loss: 0.03665716370860031\n",
            "Batch: 2880 Loss: 0.06523302034973896\n",
            "Batch: 3200 Loss: 0.03498392859295973\n",
            "Batch: 3520 Loss: 0.05670480821673103\n",
            "Batch: 3840 Loss: 0.04371689000015907\n",
            "Batch: 4160 Loss: 0.052864115054337274\n",
            "Batch: 4480 Loss: 0.04154680887317953\n",
            "Batch: 4800 Loss: 0.0472204736660971\n",
            "Batch: 5120 Loss: 0.043762879711617436\n",
            "Batch: 5440 Loss: 0.05118525604211382\n",
            "Batch: 5760 Loss: 0.03773885077364563\n",
            "Batch: 6080 Loss: 0.0528619957806739\n",
            "Batch: 6400 Loss: 0.03681486774253607\n",
            "Batch: 6720 Loss: 0.036127410172334146\n",
            "Batch: 7040 Loss: 0.0653271540847289\n",
            "Batch: 7360 Loss: 0.05532983949611085\n",
            "Batch: 7680 Loss: 0.045047529175282215\n",
            "Batch: 8000 Loss: 0.06758853154822217\n",
            "Batch: 8320 Loss: 0.06328429481573644\n",
            "Batch: 8640 Loss: 0.06798134395099996\n",
            "Batch: 8960 Loss: 0.05632715615707494\n",
            "Batch: 9280 Loss: 0.043792255497015764\n",
            "Batch: 9600 Loss: 0.03926938494187848\n",
            "Batch: 9920 Loss: 0.05671952826252259\n",
            "Batch: 10240 Loss: 0.06424323956676457\n",
            "Batch: 10560 Loss: 0.04038893305418194\n",
            "Batch: 10880 Loss: 0.06399129029506913\n",
            "Epoch: 263\n",
            "Batch: 0 Loss: 0.06189782693331941\n",
            "Batch: 320 Loss: 0.052457456827676725\n",
            "Batch: 640 Loss: 0.06712501078180223\n",
            "Batch: 960 Loss: 0.05592982230082266\n",
            "Batch: 1280 Loss: 0.03523647087137392\n",
            "Batch: 1600 Loss: 0.06806828284699425\n",
            "Batch: 1920 Loss: 0.0634804173911071\n",
            "Batch: 2240 Loss: 0.04005371495363108\n",
            "Batch: 2560 Loss: 0.05221614495309904\n",
            "Batch: 2880 Loss: 0.05653121809394728\n",
            "Batch: 3200 Loss: 0.061694274536918385\n",
            "Batch: 3520 Loss: 0.07287671697369846\n",
            "Batch: 3840 Loss: 0.024322853990822905\n",
            "Batch: 4160 Loss: 0.05614316075419716\n",
            "Batch: 4480 Loss: 0.0435760386118594\n",
            "Batch: 4800 Loss: 0.048080777954453154\n",
            "Batch: 5120 Loss: 0.03074439691323297\n",
            "Batch: 5440 Loss: 0.04198289780676401\n",
            "Batch: 5760 Loss: 0.05220039951123236\n",
            "Batch: 6080 Loss: 0.050816711579999084\n",
            "Batch: 6400 Loss: 0.04780941417036493\n",
            "Batch: 6720 Loss: 0.045092902938843465\n",
            "Batch: 7040 Loss: 0.04202366996580137\n",
            "Batch: 7360 Loss: 0.03850640732845397\n",
            "Batch: 7680 Loss: 0.035030718458071285\n",
            "Batch: 8000 Loss: 0.055646560585454925\n",
            "Batch: 8320 Loss: 0.0418777084694684\n",
            "Batch: 8640 Loss: 0.06392463285776832\n",
            "Batch: 8960 Loss: 0.06019797764463342\n",
            "Batch: 9280 Loss: 0.0787708960879649\n",
            "Batch: 9600 Loss: 0.07356460443655057\n",
            "Batch: 9920 Loss: 0.0480127070354258\n",
            "Batch: 10240 Loss: 0.03169815704964165\n",
            "Batch: 10560 Loss: 0.05550230493025726\n",
            "Batch: 10880 Loss: 0.05227500731699129\n",
            "Epoch: 264\n",
            "Batch: 0 Loss: 0.06617922513340854\n",
            "Batch: 320 Loss: 0.059465140122866036\n",
            "Batch: 640 Loss: 0.04866113927597761\n",
            "Batch: 960 Loss: 0.0535441605579443\n",
            "Batch: 1280 Loss: 0.0576790714798635\n",
            "Batch: 1600 Loss: 0.03109108890876939\n",
            "Batch: 1920 Loss: 0.046744587609968684\n",
            "Batch: 2240 Loss: 0.04876128224605879\n",
            "Batch: 2560 Loss: 0.056629100532175076\n",
            "Batch: 2880 Loss: 0.060249404943020675\n",
            "Batch: 3200 Loss: 0.04672684803434078\n",
            "Batch: 3520 Loss: 0.04765180211429895\n",
            "Batch: 3840 Loss: 0.08232888745167023\n",
            "Batch: 4160 Loss: 0.08848247028771274\n",
            "Batch: 4480 Loss: 0.05564828121650505\n",
            "Batch: 4800 Loss: 0.048223506766427414\n",
            "Batch: 5120 Loss: 0.04063042831347326\n",
            "Batch: 5440 Loss: 0.04528521373089027\n",
            "Batch: 5760 Loss: 0.0626745909043703\n",
            "Batch: 6080 Loss: 0.042919936276645254\n",
            "Batch: 6400 Loss: 0.048656889972551916\n",
            "Batch: 6720 Loss: 0.05465024056479929\n",
            "Batch: 7040 Loss: 0.0502568164499125\n",
            "Batch: 7360 Loss: 0.06031451636446766\n",
            "Batch: 7680 Loss: 0.03246092351824338\n",
            "Batch: 8000 Loss: 0.05010884712180093\n",
            "Batch: 8320 Loss: 0.0370684848005223\n",
            "Batch: 8640 Loss: 0.047473090440199615\n",
            "Batch: 8960 Loss: 0.07721280011303862\n",
            "Batch: 9280 Loss: 0.06536242945996816\n",
            "Batch: 9600 Loss: 0.04840759911424306\n",
            "Batch: 9920 Loss: 0.05193455276988669\n",
            "Batch: 10240 Loss: 0.05837942724434048\n",
            "Batch: 10560 Loss: 0.05127384130834694\n",
            "Batch: 10880 Loss: 0.10157480214943325\n",
            "Epoch: 265\n",
            "Batch: 0 Loss: 0.05793553932962627\n",
            "Batch: 320 Loss: 0.06623571898627291\n",
            "Batch: 640 Loss: 0.055004685023756224\n",
            "Batch: 960 Loss: 0.05040016554104793\n",
            "Batch: 1280 Loss: 0.04394082293093058\n",
            "Batch: 1600 Loss: 0.04509488035643691\n",
            "Batch: 1920 Loss: 0.04561690056515352\n",
            "Batch: 2240 Loss: 0.045303904496691236\n",
            "Batch: 2560 Loss: 0.054206388211545546\n",
            "Batch: 2880 Loss: 0.039496838302773\n",
            "Batch: 3200 Loss: 0.06080660923523702\n",
            "Batch: 3520 Loss: 0.044380954897056235\n",
            "Batch: 3840 Loss: 0.07534573573222056\n",
            "Batch: 4160 Loss: 0.04501247521362302\n",
            "Batch: 4480 Loss: 0.04504009489664887\n",
            "Batch: 4800 Loss: 0.057859310373361575\n",
            "Batch: 5120 Loss: 0.04209722534021552\n",
            "Batch: 5440 Loss: 0.040724469046027424\n",
            "Batch: 5760 Loss: 0.064163267624745\n",
            "Batch: 6080 Loss: 0.029597085452321144\n",
            "Batch: 6400 Loss: 0.05461541956122022\n",
            "Batch: 6720 Loss: 0.026698317619380612\n",
            "Batch: 7040 Loss: 0.06193945238487084\n",
            "Batch: 7360 Loss: 0.04854497198232151\n",
            "Batch: 7680 Loss: 0.03204363755265717\n",
            "Batch: 8000 Loss: 0.03324831167095834\n",
            "Batch: 8320 Loss: 0.044900209854454434\n",
            "Batch: 8640 Loss: 0.057305026442323666\n",
            "Batch: 8960 Loss: 0.03216427160791412\n",
            "Batch: 9280 Loss: 0.05023962871722165\n",
            "Batch: 9600 Loss: 0.045722577185429526\n",
            "Batch: 9920 Loss: 0.04486868082306625\n",
            "Batch: 10240 Loss: 0.05108614420847325\n",
            "Batch: 10560 Loss: 0.05511233650774007\n",
            "Batch: 10880 Loss: 0.038651850108556284\n",
            "Epoch: 266\n",
            "Batch: 0 Loss: 0.050849667791329406\n",
            "Batch: 320 Loss: 0.05164166317454295\n",
            "Batch: 640 Loss: 0.041084072458285825\n",
            "Batch: 960 Loss: 0.06690572721932103\n",
            "Batch: 1280 Loss: 0.04844930651251595\n",
            "Batch: 1600 Loss: 0.05040122330520115\n",
            "Batch: 1920 Loss: 0.05331078185519252\n",
            "Batch: 2240 Loss: 0.0601797750653121\n",
            "Batch: 2560 Loss: 0.08478299098258259\n",
            "Batch: 2880 Loss: 0.044027474657346494\n",
            "Batch: 3200 Loss: 0.05579431943680571\n",
            "Batch: 3520 Loss: 0.10123536896463055\n",
            "Batch: 3840 Loss: 0.059466224943001474\n",
            "Batch: 4160 Loss: 0.07200909634575875\n",
            "Batch: 4480 Loss: 0.06156384898441424\n",
            "Batch: 4800 Loss: 0.05316618444644711\n",
            "Batch: 5120 Loss: 0.05868159316789146\n",
            "Batch: 5440 Loss: 0.05845007769275251\n",
            "Batch: 5760 Loss: 0.07283421016702665\n",
            "Batch: 6080 Loss: 0.03825450737210885\n",
            "Batch: 6400 Loss: 0.06571132194646015\n",
            "Batch: 6720 Loss: 0.04908903231031426\n",
            "Batch: 7040 Loss: 0.04184946238793395\n",
            "Batch: 7360 Loss: 0.041981411498520964\n",
            "Batch: 7680 Loss: 0.04838790566134943\n",
            "Batch: 8000 Loss: 0.04073584284659556\n",
            "Batch: 8320 Loss: 0.04659218805593853\n",
            "Batch: 8640 Loss: 0.05133740487447235\n",
            "Batch: 8960 Loss: 0.029444649532279488\n",
            "Batch: 9280 Loss: 0.04383056775977069\n",
            "Batch: 9600 Loss: 0.04845185882110447\n",
            "Batch: 9920 Loss: 0.05259594752461507\n",
            "Batch: 10240 Loss: 0.042462589202912904\n",
            "Batch: 10560 Loss: 0.04367823167605785\n",
            "Batch: 10880 Loss: 0.05961568163351677\n",
            "Epoch: 267\n",
            "Batch: 0 Loss: 0.06205363997614626\n",
            "Batch: 320 Loss: 0.06629332374565683\n",
            "Batch: 640 Loss: 0.043778391236159696\n",
            "Batch: 960 Loss: 0.05828319975522181\n",
            "Batch: 1280 Loss: 0.034712442534391694\n",
            "Batch: 1600 Loss: 0.06868913912212463\n",
            "Batch: 1920 Loss: 0.06455578723898756\n",
            "Batch: 2240 Loss: 0.037928102392349956\n",
            "Batch: 2560 Loss: 0.030857902557762135\n",
            "Batch: 2880 Loss: 0.04318308099361534\n",
            "Batch: 3200 Loss: 0.055634607513856876\n",
            "Batch: 3520 Loss: 0.05925819805591901\n",
            "Batch: 3840 Loss: 0.08581131595547703\n",
            "Batch: 4160 Loss: 0.046171053701085644\n",
            "Batch: 4480 Loss: 0.05069873372507052\n",
            "Batch: 4800 Loss: 0.03340768806160063\n",
            "Batch: 5120 Loss: 0.056974633591278606\n",
            "Batch: 5440 Loss: 0.04439367727490564\n",
            "Batch: 5760 Loss: 0.04260806916560077\n",
            "Batch: 6080 Loss: 0.044377544082089715\n",
            "Batch: 6400 Loss: 0.04285715742439115\n",
            "Batch: 6720 Loss: 0.04109654805395721\n",
            "Batch: 7040 Loss: 0.05798430785722739\n",
            "Batch: 7360 Loss: 0.03767589851499178\n",
            "Batch: 7680 Loss: 0.061899740223862174\n",
            "Batch: 8000 Loss: 0.06797298578290827\n",
            "Batch: 8320 Loss: 0.05518085852253356\n",
            "Batch: 8640 Loss: 0.07323913313605127\n",
            "Batch: 8960 Loss: 0.0745096558466506\n",
            "Batch: 9280 Loss: 0.052635562496293344\n",
            "Batch: 9600 Loss: 0.04755663409174757\n",
            "Batch: 9920 Loss: 0.04871706761608372\n",
            "Batch: 10240 Loss: 0.06598508742778628\n",
            "Batch: 10560 Loss: 0.045787101868466386\n",
            "Batch: 10880 Loss: 0.044160902031920427\n",
            "Epoch: 268\n",
            "Batch: 0 Loss: 0.02723930621051259\n",
            "Batch: 320 Loss: 0.03773071570664767\n",
            "Batch: 640 Loss: 0.0477161272994881\n",
            "Batch: 960 Loss: 0.028465463498746648\n",
            "Batch: 1280 Loss: 0.06502304402622952\n",
            "Batch: 1600 Loss: 0.06588855654087984\n",
            "Batch: 1920 Loss: 0.02955355590702546\n",
            "Batch: 2240 Loss: 0.06986460448542989\n",
            "Batch: 2560 Loss: 0.05808911495850969\n",
            "Batch: 2880 Loss: 0.05473400345581979\n",
            "Batch: 3200 Loss: 0.04254426741286032\n",
            "Batch: 3520 Loss: 0.04951937514013722\n",
            "Batch: 3840 Loss: 0.051746502656032554\n",
            "Batch: 4160 Loss: 0.029441918726766048\n",
            "Batch: 4480 Loss: 0.0797978656496634\n",
            "Batch: 4800 Loss: 0.03144488894046997\n",
            "Batch: 5120 Loss: 0.03994490950442792\n",
            "Batch: 5440 Loss: 0.04362913082567967\n",
            "Batch: 5760 Loss: 0.055625126589385857\n",
            "Batch: 6080 Loss: 0.06411995649166748\n",
            "Batch: 6400 Loss: 0.06058483996488855\n",
            "Batch: 6720 Loss: 0.05463599800850338\n",
            "Batch: 7040 Loss: 0.038003813245828265\n",
            "Batch: 7360 Loss: 0.056140435845612\n",
            "Batch: 7680 Loss: 0.04616231492014254\n",
            "Batch: 8000 Loss: 0.05433125267127758\n",
            "Batch: 8320 Loss: 0.04310770595716969\n",
            "Batch: 8640 Loss: 0.029096634058700867\n",
            "Batch: 8960 Loss: 0.03987410952416747\n",
            "Batch: 9280 Loss: 0.07430518840631474\n",
            "Batch: 9600 Loss: 0.0646618866006205\n",
            "Batch: 9920 Loss: 0.03915119491712759\n",
            "Batch: 10240 Loss: 0.04640946692808685\n",
            "Batch: 10560 Loss: 0.033287735392398744\n",
            "Batch: 10880 Loss: 0.048440946735185474\n",
            "Epoch: 269\n",
            "Batch: 0 Loss: 0.05186726093696353\n",
            "Batch: 320 Loss: 0.06913477894980237\n",
            "Batch: 640 Loss: 0.048131744756570116\n",
            "Batch: 960 Loss: 0.04378816023169069\n",
            "Batch: 1280 Loss: 0.05838752859606062\n",
            "Batch: 1600 Loss: 0.045666640803560185\n",
            "Batch: 1920 Loss: 0.04762758436184258\n",
            "Batch: 2240 Loss: 0.03384155807196709\n",
            "Batch: 2560 Loss: 0.03585441024037525\n",
            "Batch: 2880 Loss: 0.05994612012714877\n",
            "Batch: 3200 Loss: 0.04070167656791087\n",
            "Batch: 3520 Loss: 0.06030177337216786\n",
            "Batch: 3840 Loss: 0.07533520200788313\n",
            "Batch: 4160 Loss: 0.047018874260259004\n",
            "Batch: 4480 Loss: 0.06369323434872362\n",
            "Batch: 4800 Loss: 0.04849206298755422\n",
            "Batch: 5120 Loss: 0.06281879546947026\n",
            "Batch: 5440 Loss: 0.060136044048160545\n",
            "Batch: 5760 Loss: 0.046085697489898735\n",
            "Batch: 6080 Loss: 0.04847038501795979\n",
            "Batch: 6400 Loss: 0.06626813069901201\n",
            "Batch: 6720 Loss: 0.0323082998497891\n",
            "Batch: 7040 Loss: 0.0555628760398835\n",
            "Batch: 7360 Loss: 0.0433461773163663\n",
            "Batch: 7680 Loss: 0.03460597818048011\n",
            "Batch: 8000 Loss: 0.06291070005238616\n",
            "Batch: 8320 Loss: 0.040453360622874406\n",
            "Batch: 8640 Loss: 0.047325521138481465\n",
            "Batch: 8960 Loss: 0.05408271026886273\n",
            "Batch: 9280 Loss: 0.038952920863003\n",
            "Batch: 9600 Loss: 0.08420358796041712\n",
            "Batch: 9920 Loss: 0.05006791467579925\n",
            "Batch: 10240 Loss: 0.04851249431979744\n",
            "Batch: 10560 Loss: 0.04734437155161987\n",
            "Batch: 10880 Loss: 0.05600733129598327\n",
            "Epoch: 270\n",
            "Batch: 0 Loss: 0.060833953239839506\n",
            "Batch: 320 Loss: 0.04885774934339941\n",
            "Batch: 640 Loss: 0.031948642943952194\n",
            "Batch: 960 Loss: 0.04699599562736147\n",
            "Batch: 1280 Loss: 0.049815750410393406\n",
            "Batch: 1600 Loss: 0.0653327651823084\n",
            "Batch: 1920 Loss: 0.06703032847417607\n",
            "Batch: 2240 Loss: 0.05321883737906011\n",
            "Batch: 2560 Loss: 0.04540958842156898\n",
            "Batch: 2880 Loss: 0.04984670549032677\n",
            "Batch: 3200 Loss: 0.07305361047748343\n",
            "Batch: 3520 Loss: 0.04759049629511734\n",
            "Batch: 3840 Loss: 0.05154752196804138\n",
            "Batch: 4160 Loss: 0.04292820415914666\n",
            "Batch: 4480 Loss: 0.03758926723370049\n",
            "Batch: 4800 Loss: 0.06324578290192523\n",
            "Batch: 5120 Loss: 0.05943164270863334\n",
            "Batch: 5440 Loss: 0.08069959863986727\n",
            "Batch: 5760 Loss: 0.04902207802723128\n",
            "Batch: 6080 Loss: 0.05279699125274285\n",
            "Batch: 6400 Loss: 0.03558477907393019\n",
            "Batch: 6720 Loss: 0.05430777367118983\n",
            "Batch: 7040 Loss: 0.08201375923790225\n",
            "Batch: 7360 Loss: 0.04235038227287432\n",
            "Batch: 7680 Loss: 0.03891051791953273\n",
            "Batch: 8000 Loss: 0.05955862893876762\n",
            "Batch: 8320 Loss: 0.05328417404051141\n",
            "Batch: 8640 Loss: 0.04131805489679889\n",
            "Batch: 8960 Loss: 0.043053474823954684\n",
            "Batch: 9280 Loss: 0.06939281561376401\n",
            "Batch: 9600 Loss: 0.036191080612404315\n",
            "Batch: 9920 Loss: 0.03295083851981388\n",
            "Batch: 10240 Loss: 0.04909475736076094\n",
            "Batch: 10560 Loss: 0.03167642493919248\n",
            "Batch: 10880 Loss: 0.10404769978961924\n",
            "Epoch: 271\n",
            "Batch: 0 Loss: 0.04470101628534284\n",
            "Batch: 320 Loss: 0.050231268825108234\n",
            "Batch: 640 Loss: 0.04606066997670469\n",
            "Batch: 960 Loss: 0.03858880637825784\n",
            "Batch: 1280 Loss: 0.04648169222798623\n",
            "Batch: 1600 Loss: 0.03701806622805016\n",
            "Batch: 1920 Loss: 0.047773036709458275\n",
            "Batch: 2240 Loss: 0.054318830137343385\n",
            "Batch: 2560 Loss: 0.052762706790166196\n",
            "Batch: 2880 Loss: 0.06336599043769853\n",
            "Batch: 3200 Loss: 0.03715695596000476\n",
            "Batch: 3520 Loss: 0.04595545708332817\n",
            "Batch: 3840 Loss: 0.037440518396825405\n",
            "Batch: 4160 Loss: 0.061609905249855705\n",
            "Batch: 4480 Loss: 0.047548813640476674\n",
            "Batch: 4800 Loss: 0.05309375581915679\n",
            "Batch: 5120 Loss: 0.0503485623115745\n",
            "Batch: 5440 Loss: 0.043783380098908545\n",
            "Batch: 5760 Loss: 0.05171762580687659\n",
            "Batch: 6080 Loss: 0.03748192190082772\n",
            "Batch: 6400 Loss: 0.03624277752364857\n",
            "Batch: 6720 Loss: 0.05101791809208681\n",
            "Batch: 7040 Loss: 0.0388495368167686\n",
            "Batch: 7360 Loss: 0.0502077844882054\n",
            "Batch: 7680 Loss: 0.08156727679435186\n",
            "Batch: 8000 Loss: 0.036012206507525374\n",
            "Batch: 8320 Loss: 0.053235937980409764\n",
            "Batch: 8640 Loss: 0.033496771792610285\n",
            "Batch: 8960 Loss: 0.05128286474517357\n",
            "Batch: 9280 Loss: 0.04688937676963196\n",
            "Batch: 9600 Loss: 0.03858786636101104\n",
            "Batch: 9920 Loss: 0.05548485698114644\n",
            "Batch: 10240 Loss: 0.044443052456294395\n",
            "Batch: 10560 Loss: 0.05836624944998505\n",
            "Batch: 10880 Loss: 0.035736092551778696\n",
            "Epoch: 272\n",
            "Batch: 0 Loss: 0.03977975582895173\n",
            "Batch: 320 Loss: 0.047765790992845454\n",
            "Batch: 640 Loss: 0.040504327525944475\n",
            "Batch: 960 Loss: 0.04381259626717074\n",
            "Batch: 1280 Loss: 0.03180881353843984\n",
            "Batch: 1600 Loss: 0.04195678720945858\n",
            "Batch: 1920 Loss: 0.04664307930257894\n",
            "Batch: 2240 Loss: 0.05467195525174578\n",
            "Batch: 2560 Loss: 0.06982655085056579\n",
            "Batch: 2880 Loss: 0.05590257983556696\n",
            "Batch: 3200 Loss: 0.046580080470949914\n",
            "Batch: 3520 Loss: 0.04956449454516629\n",
            "Batch: 3840 Loss: 0.06765830016761211\n",
            "Batch: 4160 Loss: 0.06420902142088177\n",
            "Batch: 4480 Loss: 0.05023886497086707\n",
            "Batch: 4800 Loss: 0.0550988313222152\n",
            "Batch: 5120 Loss: 0.027468666063194033\n",
            "Batch: 5440 Loss: 0.06155825444411053\n",
            "Batch: 5760 Loss: 0.04156942695788335\n",
            "Batch: 6080 Loss: 0.05347424088320162\n",
            "Batch: 6400 Loss: 0.05116559673416233\n",
            "Batch: 6720 Loss: 0.06163481562712525\n",
            "Batch: 7040 Loss: 0.05551186985869572\n",
            "Batch: 7360 Loss: 0.04428228017403074\n",
            "Batch: 7680 Loss: 0.04945961308212005\n",
            "Batch: 8000 Loss: 0.049875015467385445\n",
            "Batch: 8320 Loss: 0.05857566916462179\n",
            "Batch: 8640 Loss: 0.0605867286812691\n",
            "Batch: 8960 Loss: 0.05691673214320578\n",
            "Batch: 9280 Loss: 0.058511122699758805\n",
            "Batch: 9600 Loss: 0.03858573424971925\n",
            "Batch: 9920 Loss: 0.05786417337736504\n",
            "Batch: 10240 Loss: 0.0528383999985132\n",
            "Batch: 10560 Loss: 0.058225219711380574\n",
            "Batch: 10880 Loss: 0.05442178250322656\n",
            "Epoch: 273\n",
            "Batch: 0 Loss: 0.07062088900131724\n",
            "Batch: 320 Loss: 0.04677673456027891\n",
            "Batch: 640 Loss: 0.05537170193829567\n",
            "Batch: 960 Loss: 0.039211541999496884\n",
            "Batch: 1280 Loss: 0.03839612066477529\n",
            "Batch: 1600 Loss: 0.038957459470890396\n",
            "Batch: 1920 Loss: 0.07622958455870019\n",
            "Batch: 2240 Loss: 0.03270948885394421\n",
            "Batch: 2560 Loss: 0.0709662609444191\n",
            "Batch: 2880 Loss: 0.0683307114223696\n",
            "Batch: 3200 Loss: 0.06860478336781486\n",
            "Batch: 3520 Loss: 0.04454196234893187\n",
            "Batch: 3840 Loss: 0.05659915954101316\n",
            "Batch: 4160 Loss: 0.06397822040871311\n",
            "Batch: 4480 Loss: 0.031546239408230106\n",
            "Batch: 4800 Loss: 0.09853133909925774\n",
            "Batch: 5120 Loss: 0.05113793714222198\n",
            "Batch: 5440 Loss: 0.04359235028265579\n",
            "Batch: 5760 Loss: 0.05051642182088\n",
            "Batch: 6080 Loss: 0.036276013215531484\n",
            "Batch: 6400 Loss: 0.04919248241910494\n",
            "Batch: 6720 Loss: 0.0425730484419599\n",
            "Batch: 7040 Loss: 0.05409270336333721\n",
            "Batch: 7360 Loss: 0.056812625739458736\n",
            "Batch: 7680 Loss: 0.05904178731298142\n",
            "Batch: 8000 Loss: 0.038486017943219986\n",
            "Batch: 8320 Loss: 0.06922206387348118\n",
            "Batch: 8640 Loss: 0.03552686731467673\n",
            "Batch: 8960 Loss: 0.04605334101021059\n",
            "Batch: 9280 Loss: 0.04362508552476021\n",
            "Batch: 9600 Loss: 0.03272610189623693\n",
            "Batch: 9920 Loss: 0.06935497331355003\n",
            "Batch: 10240 Loss: 0.059162479063487074\n",
            "Batch: 10560 Loss: 0.05601837667885497\n",
            "Batch: 10880 Loss: 0.04066627525299348\n",
            "Epoch: 274\n",
            "Batch: 0 Loss: 0.032069344174242086\n",
            "Batch: 320 Loss: 0.06105226342691084\n",
            "Batch: 640 Loss: 0.05738420352741493\n",
            "Batch: 960 Loss: 0.0478636281571086\n",
            "Batch: 1280 Loss: 0.05150429925933684\n",
            "Batch: 1600 Loss: 0.04519909857443083\n",
            "Batch: 1920 Loss: 0.050893940629623144\n",
            "Batch: 2240 Loss: 0.04600075497995474\n",
            "Batch: 2560 Loss: 0.05286899055654362\n",
            "Batch: 2880 Loss: 0.06684881253158143\n",
            "Batch: 3200 Loss: 0.03796163773286998\n",
            "Batch: 3520 Loss: 0.04394609871372144\n",
            "Batch: 3840 Loss: 0.04628137255640689\n",
            "Batch: 4160 Loss: 0.06218218016221373\n",
            "Batch: 4480 Loss: 0.06765610412599701\n",
            "Batch: 4800 Loss: 0.06070123327689567\n",
            "Batch: 5120 Loss: 0.06164760277919631\n",
            "Batch: 5440 Loss: 0.03529328827611288\n",
            "Batch: 5760 Loss: 0.03924785555616974\n",
            "Batch: 6080 Loss: 0.07793855786490651\n",
            "Batch: 6400 Loss: 0.05638048767202839\n",
            "Batch: 6720 Loss: 0.06237202617186097\n",
            "Batch: 7040 Loss: 0.0582727153224336\n",
            "Batch: 7360 Loss: 0.05627760485494618\n",
            "Batch: 7680 Loss: 0.07186748163339347\n",
            "Batch: 8000 Loss: 0.02967862035202313\n",
            "Batch: 8320 Loss: 0.03878280518561528\n",
            "Batch: 8640 Loss: 0.07250875583725747\n",
            "Batch: 8960 Loss: 0.0540752695751917\n",
            "Batch: 9280 Loss: 0.05515269363685557\n",
            "Batch: 9600 Loss: 0.047661199733587956\n",
            "Batch: 9920 Loss: 0.06245830971924832\n",
            "Batch: 10240 Loss: 0.06323638927906952\n",
            "Batch: 10560 Loss: 0.046468846040350884\n",
            "Batch: 10880 Loss: 0.06035217473121773\n",
            "Epoch: 275\n",
            "Batch: 0 Loss: 0.05488137773341592\n",
            "Batch: 320 Loss: 0.0551794819250021\n",
            "Batch: 640 Loss: 0.056317050960587964\n",
            "Batch: 960 Loss: 0.044990105641567135\n",
            "Batch: 1280 Loss: 0.05837956958898397\n",
            "Batch: 1600 Loss: 0.04221612196143881\n",
            "Batch: 1920 Loss: 0.04946400981415818\n",
            "Batch: 2240 Loss: 0.048672968418106484\n",
            "Batch: 2560 Loss: 0.036461762828685275\n",
            "Batch: 2880 Loss: 0.07170347455870997\n",
            "Batch: 3200 Loss: 0.0558266630828392\n",
            "Batch: 3520 Loss: 0.042280360782377235\n",
            "Batch: 3840 Loss: 0.03470290998596304\n",
            "Batch: 4160 Loss: 0.03383990303570676\n",
            "Batch: 4480 Loss: 0.052404362318774524\n",
            "Batch: 4800 Loss: 0.04296430517362391\n",
            "Batch: 5120 Loss: 0.05891468101463731\n",
            "Batch: 5440 Loss: 0.06437868388234247\n",
            "Batch: 5760 Loss: 0.04484836611172666\n",
            "Batch: 6080 Loss: 0.04817409876926754\n",
            "Batch: 6400 Loss: 0.03849825719512087\n",
            "Batch: 6720 Loss: 0.039592717287001256\n",
            "Batch: 7040 Loss: 0.06001304738792021\n",
            "Batch: 7360 Loss: 0.04112176222924257\n",
            "Batch: 7680 Loss: 0.047592077261726935\n",
            "Batch: 8000 Loss: 0.0368014386852329\n",
            "Batch: 8320 Loss: 0.05201447825907623\n",
            "Batch: 8640 Loss: 0.0606448067176216\n",
            "Batch: 8960 Loss: 0.034554336083409104\n",
            "Batch: 9280 Loss: 0.037157186626787256\n",
            "Batch: 9600 Loss: 0.05726656388574712\n",
            "Batch: 9920 Loss: 0.04169251778004201\n",
            "Batch: 10240 Loss: 0.06519522906622789\n",
            "Batch: 10560 Loss: 0.054899416132023975\n",
            "Batch: 10880 Loss: 0.044405525263764185\n",
            "Epoch: 276\n",
            "Batch: 0 Loss: 0.04404704590576334\n",
            "Batch: 320 Loss: 0.04363250907494825\n",
            "Batch: 640 Loss: 0.042857411781558025\n",
            "Batch: 960 Loss: 0.06271568952837765\n",
            "Batch: 1280 Loss: 0.07498656662942997\n",
            "Batch: 1600 Loss: 0.07481132231146694\n",
            "Batch: 1920 Loss: 0.054069862379945904\n",
            "Batch: 2240 Loss: 0.044024767216489805\n",
            "Batch: 2560 Loss: 0.04129059238236078\n",
            "Batch: 2880 Loss: 0.03663540652277752\n",
            "Batch: 3200 Loss: 0.04446681012344718\n",
            "Batch: 3520 Loss: 0.04053010331014897\n",
            "Batch: 3840 Loss: 0.046273378159446515\n",
            "Batch: 4160 Loss: 0.06289506384074721\n",
            "Batch: 4480 Loss: 0.042771421851440124\n",
            "Batch: 4800 Loss: 0.06708792570778646\n",
            "Batch: 5120 Loss: 0.039159141036228376\n",
            "Batch: 5440 Loss: 0.06959251449688102\n",
            "Batch: 5760 Loss: 0.04272392911470172\n",
            "Batch: 6080 Loss: 0.03925325030182965\n",
            "Batch: 6400 Loss: 0.05284817820723242\n",
            "Batch: 6720 Loss: 0.0418481031386943\n",
            "Batch: 7040 Loss: 0.06601630913276925\n",
            "Batch: 7360 Loss: 0.05495010713575728\n",
            "Batch: 7680 Loss: 0.05496853280149441\n",
            "Batch: 8000 Loss: 0.05365061354561974\n",
            "Batch: 8320 Loss: 0.05727881108083262\n",
            "Batch: 8640 Loss: 0.05663613625960537\n",
            "Batch: 8960 Loss: 0.049440257378593706\n",
            "Batch: 9280 Loss: 0.06384840294153551\n",
            "Batch: 9600 Loss: 0.039559315618626065\n",
            "Batch: 9920 Loss: 0.05145306114668475\n",
            "Batch: 10240 Loss: 0.04786946133924571\n",
            "Batch: 10560 Loss: 0.04392761826093442\n",
            "Batch: 10880 Loss: 0.039227807509809254\n",
            "Epoch: 277\n",
            "Batch: 0 Loss: 0.045045479999873975\n",
            "Batch: 320 Loss: 0.06059599870598315\n",
            "Batch: 640 Loss: 0.046118155906853336\n",
            "Batch: 960 Loss: 0.06317318591891928\n",
            "Batch: 1280 Loss: 0.05316344883928866\n",
            "Batch: 1600 Loss: 0.04406506729981547\n",
            "Batch: 1920 Loss: 0.07777734258756842\n",
            "Batch: 2240 Loss: 0.04997096026638634\n",
            "Batch: 2560 Loss: 0.054843544407697356\n",
            "Batch: 2880 Loss: 0.06251315628586125\n",
            "Batch: 3200 Loss: 0.048687543512081884\n",
            "Batch: 3520 Loss: 0.07493455626811518\n",
            "Batch: 3840 Loss: 0.045752641686702185\n",
            "Batch: 4160 Loss: 0.04238440830197136\n",
            "Batch: 4480 Loss: 0.05154907467323767\n",
            "Batch: 4800 Loss: 0.04036129961704598\n",
            "Batch: 5120 Loss: 0.07006013571798667\n",
            "Batch: 5440 Loss: 0.04687476188231673\n",
            "Batch: 5760 Loss: 0.07169574502550233\n",
            "Batch: 6080 Loss: 0.030060497614223174\n",
            "Batch: 6400 Loss: 0.043861573461601945\n",
            "Batch: 6720 Loss: 0.06816177607765622\n",
            "Batch: 7040 Loss: 0.05342344881261749\n",
            "Batch: 7360 Loss: 0.061254435290331734\n",
            "Batch: 7680 Loss: 0.053029214327655616\n",
            "Batch: 8000 Loss: 0.06088467290943771\n",
            "Batch: 8320 Loss: 0.040191466388040484\n",
            "Batch: 8640 Loss: 0.03126343551370565\n",
            "Batch: 8960 Loss: 0.03600006850226169\n",
            "Batch: 9280 Loss: 0.04779180263002184\n",
            "Batch: 9600 Loss: 0.0470800945900414\n",
            "Batch: 9920 Loss: 0.07977978920183587\n",
            "Batch: 10240 Loss: 0.061661946904908074\n",
            "Batch: 10560 Loss: 0.05158102133672702\n",
            "Batch: 10880 Loss: 0.039746232971504196\n",
            "Epoch: 278\n",
            "Batch: 0 Loss: 0.0592433570008946\n",
            "Batch: 320 Loss: 0.03723148478474172\n",
            "Batch: 640 Loss: 0.06137375116425228\n",
            "Batch: 960 Loss: 0.04778222483107593\n",
            "Batch: 1280 Loss: 0.04326387456620622\n",
            "Batch: 1600 Loss: 0.0384839687681737\n",
            "Batch: 1920 Loss: 0.06004623569006828\n",
            "Batch: 2240 Loss: 0.04134636798165259\n",
            "Batch: 2560 Loss: 0.0724492499114784\n",
            "Batch: 2880 Loss: 0.035342778752362726\n",
            "Batch: 3200 Loss: 0.03668174050977227\n",
            "Batch: 3520 Loss: 0.047250165608731305\n",
            "Batch: 3840 Loss: 0.04670306258684395\n",
            "Batch: 4160 Loss: 0.03944259087323102\n",
            "Batch: 4480 Loss: 0.03920245884863903\n",
            "Batch: 4800 Loss: 0.04421562633091376\n",
            "Batch: 5120 Loss: 0.03873475327827891\n",
            "Batch: 5440 Loss: 0.04635117440227639\n",
            "Batch: 5760 Loss: 0.04376041862795796\n",
            "Batch: 6080 Loss: 0.05982033340404942\n",
            "Batch: 6400 Loss: 0.042214978011068605\n",
            "Batch: 6720 Loss: 0.050326986755949085\n",
            "Batch: 7040 Loss: 0.06402209398404314\n",
            "Batch: 7360 Loss: 0.04040427684442557\n",
            "Batch: 7680 Loss: 0.083798130403167\n",
            "Batch: 8000 Loss: 0.039924387510716995\n",
            "Batch: 8320 Loss: 0.03470059050890319\n",
            "Batch: 8640 Loss: 0.05341508126598544\n",
            "Batch: 8960 Loss: 0.05105908684908018\n",
            "Batch: 9280 Loss: 0.04726263242394259\n",
            "Batch: 9600 Loss: 0.05923007378938505\n",
            "Batch: 9920 Loss: 0.06208517913815054\n",
            "Batch: 10240 Loss: 0.06591204917021663\n",
            "Batch: 10560 Loss: 0.04879475475050174\n",
            "Batch: 10880 Loss: 0.03682938947557929\n",
            "Epoch: 279\n",
            "Batch: 0 Loss: 0.0598331183128173\n",
            "Batch: 320 Loss: 0.056211618408210345\n",
            "Batch: 640 Loss: 0.06845663988227113\n",
            "Batch: 960 Loss: 0.04745279531635034\n",
            "Batch: 1280 Loss: 0.061864063383887696\n",
            "Batch: 1600 Loss: 0.0830941815563223\n",
            "Batch: 1920 Loss: 0.052936046819679616\n",
            "Batch: 2240 Loss: 0.037501786011885606\n",
            "Batch: 2560 Loss: 0.03631919351492416\n",
            "Batch: 2880 Loss: 0.03365343294548159\n",
            "Batch: 3200 Loss: 0.07491038450739632\n",
            "Batch: 3520 Loss: 0.06829931050434603\n",
            "Batch: 3840 Loss: 0.05281405291387356\n",
            "Batch: 4160 Loss: 0.03651245590747978\n",
            "Batch: 4480 Loss: 0.0357347061188629\n",
            "Batch: 4800 Loss: 0.033108656610996386\n",
            "Batch: 5120 Loss: 0.05086339440685861\n",
            "Batch: 5440 Loss: 0.047361016714662287\n",
            "Batch: 5760 Loss: 0.07089233472718473\n",
            "Batch: 6080 Loss: 0.06166094882425124\n",
            "Batch: 6400 Loss: 0.05358731669921386\n",
            "Batch: 6720 Loss: 0.035906646830132866\n",
            "Batch: 7040 Loss: 0.06563549985542833\n",
            "Batch: 7360 Loss: 0.03489157899410616\n",
            "Batch: 7680 Loss: 0.03334589320038163\n",
            "Batch: 8000 Loss: 0.05612681326379997\n",
            "Batch: 8320 Loss: 0.05483124345761407\n",
            "Batch: 8640 Loss: 0.05070399662957689\n",
            "Batch: 8960 Loss: 0.03563230896995647\n",
            "Batch: 9280 Loss: 0.05664959979335714\n",
            "Batch: 9600 Loss: 0.04541277461123381\n",
            "Batch: 9920 Loss: 0.04547672161379196\n",
            "Batch: 10240 Loss: 0.05891057815117016\n",
            "Batch: 10560 Loss: 0.05780830985659978\n",
            "Batch: 10880 Loss: 0.043520421436567376\n",
            "Epoch: 280\n",
            "Batch: 0 Loss: 0.08719621282203761\n",
            "Batch: 320 Loss: 0.07328149918399414\n",
            "Batch: 640 Loss: 0.04593077554636779\n",
            "Batch: 960 Loss: 0.040452293619299966\n",
            "Batch: 1280 Loss: 0.04946001541480996\n",
            "Batch: 1600 Loss: 0.03842448255480826\n",
            "Batch: 1920 Loss: 0.05552258643263276\n",
            "Batch: 2240 Loss: 0.049551080180841596\n",
            "Batch: 2560 Loss: 0.029802225630253178\n",
            "Batch: 2880 Loss: 0.0632816666568127\n",
            "Batch: 3200 Loss: 0.038366760965718394\n",
            "Batch: 3520 Loss: 0.035978436437874885\n",
            "Batch: 3840 Loss: 0.06327916669314235\n",
            "Batch: 4160 Loss: 0.055802578832949916\n",
            "Batch: 4480 Loss: 0.057392050331751876\n",
            "Batch: 4800 Loss: 0.06524666450059667\n",
            "Batch: 5120 Loss: 0.06480303687027603\n",
            "Batch: 5440 Loss: 0.06123618502652014\n",
            "Batch: 5760 Loss: 0.05255657650376472\n",
            "Batch: 6080 Loss: 0.05205389929597665\n",
            "Batch: 6400 Loss: 0.04873868118136703\n",
            "Batch: 6720 Loss: 0.04011263750649866\n",
            "Batch: 7040 Loss: 0.08431558528886177\n",
            "Batch: 7360 Loss: 0.08057756100797134\n",
            "Batch: 7680 Loss: 0.05026714484054101\n",
            "Batch: 8000 Loss: 0.06613924186577469\n",
            "Batch: 8320 Loss: 0.04190899770622933\n",
            "Batch: 8640 Loss: 0.043950749998093334\n",
            "Batch: 8960 Loss: 0.06756476027627051\n",
            "Batch: 9280 Loss: 0.04934745109728776\n",
            "Batch: 9600 Loss: 0.054875940948848706\n",
            "Batch: 9920 Loss: 0.041166309120869604\n",
            "Batch: 10240 Loss: 0.051052568389192296\n",
            "Batch: 10560 Loss: 0.06695701719679577\n",
            "Batch: 10880 Loss: 0.04125814030627628\n",
            "Epoch: 281\n",
            "Batch: 0 Loss: 0.03592586750957969\n",
            "Batch: 320 Loss: 0.04648542709359038\n",
            "Batch: 640 Loss: 0.035961634284476116\n",
            "Batch: 960 Loss: 0.05188719178703989\n",
            "Batch: 1280 Loss: 0.059124744703416214\n",
            "Batch: 1600 Loss: 0.04108812699969236\n",
            "Batch: 1920 Loss: 0.04312054222263238\n",
            "Batch: 2240 Loss: 0.045166829165200165\n",
            "Batch: 2560 Loss: 0.045223752725011196\n",
            "Batch: 2880 Loss: 0.047692965449739944\n",
            "Batch: 3200 Loss: 0.04735305332004981\n",
            "Batch: 3520 Loss: 0.04564057828813598\n",
            "Batch: 3840 Loss: 0.06460410779065816\n",
            "Batch: 4160 Loss: 0.04760097834781826\n",
            "Batch: 4480 Loss: 0.046321364333117954\n",
            "Batch: 4800 Loss: 0.034410071929983105\n",
            "Batch: 5120 Loss: 0.06822841504045599\n",
            "Batch: 5440 Loss: 0.045422160870844865\n",
            "Batch: 5760 Loss: 0.05789740058502452\n",
            "Batch: 6080 Loss: 0.04611740278235379\n",
            "Batch: 6400 Loss: 0.06597757088995927\n",
            "Batch: 6720 Loss: 0.04531115504106987\n",
            "Batch: 7040 Loss: 0.040486828094859977\n",
            "Batch: 7360 Loss: 0.07513339470167538\n",
            "Batch: 7680 Loss: 0.058831902061773855\n",
            "Batch: 8000 Loss: 0.05449883891386746\n",
            "Batch: 8320 Loss: 0.06249484768292755\n",
            "Batch: 8640 Loss: 0.05182329910508431\n",
            "Batch: 8960 Loss: 0.05525259559742311\n",
            "Batch: 9280 Loss: 0.05202169224884541\n",
            "Batch: 9600 Loss: 0.047811077559509\n",
            "Batch: 9920 Loss: 0.07858673739131539\n",
            "Batch: 10240 Loss: 0.060433331525729835\n",
            "Batch: 10560 Loss: 0.054815170948737967\n",
            "Batch: 10880 Loss: 0.04004317678411685\n",
            "Epoch: 282\n",
            "Batch: 0 Loss: 0.023769204544196985\n",
            "Batch: 320 Loss: 0.07395182961144992\n",
            "Batch: 640 Loss: 0.05064352616011993\n",
            "Batch: 960 Loss: 0.047587877302437034\n",
            "Batch: 1280 Loss: 0.04882174584790454\n",
            "Batch: 1600 Loss: 0.03589651527389169\n",
            "Batch: 1920 Loss: 0.06854566116762786\n",
            "Batch: 2240 Loss: 0.03346778055129606\n",
            "Batch: 2560 Loss: 0.08470926751240007\n",
            "Batch: 2880 Loss: 0.03792466044783454\n",
            "Batch: 3200 Loss: 0.024496941716444478\n",
            "Batch: 3520 Loss: 0.06178016201853194\n",
            "Batch: 3840 Loss: 0.039435890290545575\n",
            "Batch: 4160 Loss: 0.04227528381499334\n",
            "Batch: 4480 Loss: 0.03746778696652314\n",
            "Batch: 4800 Loss: 0.07244345405452865\n",
            "Batch: 5120 Loss: 0.04283172916345216\n",
            "Batch: 5440 Loss: 0.042429339424065876\n",
            "Batch: 5760 Loss: 0.03657007159997391\n",
            "Batch: 6080 Loss: 0.039915762783600456\n",
            "Batch: 6400 Loss: 0.04178299743832031\n",
            "Batch: 6720 Loss: 0.07835772782026558\n",
            "Batch: 7040 Loss: 0.03935757356647541\n",
            "Batch: 7360 Loss: 0.03409978150731948\n",
            "Batch: 7680 Loss: 0.05262958468266772\n",
            "Batch: 8000 Loss: 0.031876770077060156\n",
            "Batch: 8320 Loss: 0.03378200404514849\n",
            "Batch: 8640 Loss: 0.03926380484306965\n",
            "Batch: 8960 Loss: 0.05078452403971734\n",
            "Batch: 9280 Loss: 0.05879922527260506\n",
            "Batch: 9600 Loss: 0.0299349567891888\n",
            "Batch: 9920 Loss: 0.06067555235044095\n",
            "Batch: 10240 Loss: 0.06118640154628681\n",
            "Batch: 10560 Loss: 0.045293722371126624\n",
            "Batch: 10880 Loss: 0.059981876017412665\n",
            "Epoch: 283\n",
            "Batch: 0 Loss: 0.07013717185927679\n",
            "Batch: 320 Loss: 0.028247374013290766\n",
            "Batch: 640 Loss: 0.06740801434920003\n",
            "Batch: 960 Loss: 0.055284038473621455\n",
            "Batch: 1280 Loss: 0.04897882652564385\n",
            "Batch: 1600 Loss: 0.03795578222411202\n",
            "Batch: 1920 Loss: 0.036443584632949694\n",
            "Batch: 2240 Loss: 0.08374572880607166\n",
            "Batch: 2560 Loss: 0.062464508805898926\n",
            "Batch: 2880 Loss: 0.05420680766100425\n",
            "Batch: 3200 Loss: 0.07037194206164867\n",
            "Batch: 3520 Loss: 0.0560112152119753\n",
            "Batch: 3840 Loss: 0.0433680805095302\n",
            "Batch: 4160 Loss: 0.04150728720213401\n",
            "Batch: 4480 Loss: 0.05654870176313256\n",
            "Batch: 4800 Loss: 0.04218057097255473\n",
            "Batch: 5120 Loss: 0.06211507582992906\n",
            "Batch: 5440 Loss: 0.0564400495979503\n",
            "Batch: 5760 Loss: 0.03134456536894535\n",
            "Batch: 6080 Loss: 0.04923640761692589\n",
            "Batch: 6400 Loss: 0.05543118795878922\n",
            "Batch: 6720 Loss: 0.06529893780854792\n",
            "Batch: 7040 Loss: 0.03723846447995785\n",
            "Batch: 7360 Loss: 0.060245415766198025\n",
            "Batch: 7680 Loss: 0.049441785900451625\n",
            "Batch: 8000 Loss: 0.034390935619499616\n",
            "Batch: 8320 Loss: 0.07065830165437312\n",
            "Batch: 8640 Loss: 0.06359229815824873\n",
            "Batch: 8960 Loss: 0.030276237309568597\n",
            "Batch: 9280 Loss: 0.05759490760814622\n",
            "Batch: 9600 Loss: 0.03486715278580783\n",
            "Batch: 9920 Loss: 0.04595546935000471\n",
            "Batch: 10240 Loss: 0.06354963493167343\n",
            "Batch: 10560 Loss: 0.04032273012163559\n",
            "Batch: 10880 Loss: 0.03893742582581099\n",
            "Epoch: 284\n",
            "Batch: 0 Loss: 0.030662957260719707\n",
            "Batch: 320 Loss: 0.044145355631351885\n",
            "Batch: 640 Loss: 0.03454032576032714\n",
            "Batch: 960 Loss: 0.046671790960786545\n",
            "Batch: 1280 Loss: 0.05271486624370824\n",
            "Batch: 1600 Loss: 0.03546415283945128\n",
            "Batch: 1920 Loss: 0.0435193160755039\n",
            "Batch: 2240 Loss: 0.04993352568656657\n",
            "Batch: 2560 Loss: 0.042109187531395656\n",
            "Batch: 2880 Loss: 0.07131370897067862\n",
            "Batch: 3200 Loss: 0.044339915155643006\n",
            "Batch: 3520 Loss: 0.04904142549874324\n",
            "Batch: 3840 Loss: 0.05676309483417741\n",
            "Batch: 4160 Loss: 0.03109327499538643\n",
            "Batch: 4480 Loss: 0.07720843518717863\n",
            "Batch: 4800 Loss: 0.04448592083002078\n",
            "Batch: 5120 Loss: 0.04975248673998153\n",
            "Batch: 5440 Loss: 0.05963722761891951\n",
            "Batch: 5760 Loss: 0.05235552504547407\n",
            "Batch: 6080 Loss: 0.06863033033890568\n",
            "Batch: 6400 Loss: 0.07272898808180273\n",
            "Batch: 6720 Loss: 0.028843319126439025\n",
            "Batch: 7040 Loss: 0.06781435782324377\n",
            "Batch: 7360 Loss: 0.05677450599943459\n",
            "Batch: 7680 Loss: 0.05782938563776733\n",
            "Batch: 8000 Loss: 0.05007173371522601\n",
            "Batch: 8320 Loss: 0.0393338420364389\n",
            "Batch: 8640 Loss: 0.04973001001648932\n",
            "Batch: 8960 Loss: 0.03917289803028251\n",
            "Batch: 9280 Loss: 0.04518840015473348\n",
            "Batch: 9600 Loss: 0.06011694903822656\n",
            "Batch: 9920 Loss: 0.04718773878691722\n",
            "Batch: 10240 Loss: 0.07214687124393669\n",
            "Batch: 10560 Loss: 0.060892757858016046\n",
            "Batch: 10880 Loss: 0.03950002540504114\n",
            "Epoch: 285\n",
            "Batch: 0 Loss: 0.045407803359474766\n",
            "Batch: 320 Loss: 0.07521611610883575\n",
            "Batch: 640 Loss: 0.057737334627634175\n",
            "Batch: 960 Loss: 0.05196360073441095\n",
            "Batch: 1280 Loss: 0.035707085647794845\n",
            "Batch: 1600 Loss: 0.05088891476211617\n",
            "Batch: 1920 Loss: 0.06004195365672977\n",
            "Batch: 2240 Loss: 0.04846661497495462\n",
            "Batch: 2560 Loss: 0.036298401439469656\n",
            "Batch: 2880 Loss: 0.0402216189865012\n",
            "Batch: 3200 Loss: 0.06831829413749772\n",
            "Batch: 3520 Loss: 0.026559096422772113\n",
            "Batch: 3840 Loss: 0.03005580991920559\n",
            "Batch: 4160 Loss: 0.03923637141609815\n",
            "Batch: 4480 Loss: 0.04685021708683413\n",
            "Batch: 4800 Loss: 0.05481479533313669\n",
            "Batch: 5120 Loss: 0.030935823628110107\n",
            "Batch: 5440 Loss: 0.03682065315985311\n",
            "Batch: 5760 Loss: 0.10369261105673347\n",
            "Batch: 6080 Loss: 0.053769245600101705\n",
            "Batch: 6400 Loss: 0.034640580692029944\n",
            "Batch: 6720 Loss: 0.06986797668840936\n",
            "Batch: 7040 Loss: 0.03640888538351051\n",
            "Batch: 7360 Loss: 0.042455583843444916\n",
            "Batch: 7680 Loss: 0.07119601760413546\n",
            "Batch: 8000 Loss: 0.04718417947240234\n",
            "Batch: 8320 Loss: 0.04162348356044729\n",
            "Batch: 8640 Loss: 0.041310467532201814\n",
            "Batch: 8960 Loss: 0.04798018967965487\n",
            "Batch: 9280 Loss: 0.03621024087873885\n",
            "Batch: 9600 Loss: 0.06576947392998923\n",
            "Batch: 9920 Loss: 0.0656007502058327\n",
            "Batch: 10240 Loss: 0.07964463695408307\n",
            "Batch: 10560 Loss: 0.0611174907193851\n",
            "Batch: 10880 Loss: 0.07722282373506598\n",
            "Epoch: 286\n",
            "Batch: 0 Loss: 0.03708521120573389\n",
            "Batch: 320 Loss: 0.08036925021098024\n",
            "Batch: 640 Loss: 0.05777188145844118\n",
            "Batch: 960 Loss: 0.049110325626520526\n",
            "Batch: 1280 Loss: 0.06495461671849333\n",
            "Batch: 1600 Loss: 0.044202750250111625\n",
            "Batch: 1920 Loss: 0.0691173588238894\n",
            "Batch: 2240 Loss: 0.04738281080432593\n",
            "Batch: 2560 Loss: 0.05532718857409194\n",
            "Batch: 2880 Loss: 0.048256569399842165\n",
            "Batch: 3200 Loss: 0.05000491644543424\n",
            "Batch: 3520 Loss: 0.057762610778960735\n",
            "Batch: 3840 Loss: 0.05814157124205731\n",
            "Batch: 4160 Loss: 0.03888886720406925\n",
            "Batch: 4480 Loss: 0.044645203556184274\n",
            "Batch: 4800 Loss: 0.03831219168092001\n",
            "Batch: 5120 Loss: 0.060417248522789256\n",
            "Batch: 5440 Loss: 0.05784530531059192\n",
            "Batch: 5760 Loss: 0.04883363880628738\n",
            "Batch: 6080 Loss: 0.040834638175170836\n",
            "Batch: 6400 Loss: 0.04249067159198341\n",
            "Batch: 6720 Loss: 0.045784539767768126\n",
            "Batch: 7040 Loss: 0.03665521991800499\n",
            "Batch: 7360 Loss: 0.05543041287995604\n",
            "Batch: 7680 Loss: 0.08031183049600575\n",
            "Batch: 8000 Loss: 0.03635340981894929\n",
            "Batch: 8320 Loss: 0.03691121173067069\n",
            "Batch: 8640 Loss: 0.05321312929888354\n",
            "Batch: 8960 Loss: 0.05807432544598689\n",
            "Batch: 9280 Loss: 0.05449687031903712\n",
            "Batch: 9600 Loss: 0.07363597368370971\n",
            "Batch: 9920 Loss: 0.0527031023455335\n",
            "Batch: 10240 Loss: 0.049972435086649024\n",
            "Batch: 10560 Loss: 0.05873350070132536\n",
            "Batch: 10880 Loss: 0.037814775217872826\n",
            "Epoch: 287\n",
            "Batch: 0 Loss: 0.048034499739242814\n",
            "Batch: 320 Loss: 0.042458286420675825\n",
            "Batch: 640 Loss: 0.054934103097124216\n",
            "Batch: 960 Loss: 0.04277634355483027\n",
            "Batch: 1280 Loss: 0.04393265426003697\n",
            "Batch: 1600 Loss: 0.049665487461695614\n",
            "Batch: 1920 Loss: 0.04530382847021349\n",
            "Batch: 2240 Loss: 0.03892341495805585\n",
            "Batch: 2560 Loss: 0.05723981134809196\n",
            "Batch: 2880 Loss: 0.06353113698435969\n",
            "Batch: 3200 Loss: 0.0565103573591101\n",
            "Batch: 3520 Loss: 0.04965068121439864\n",
            "Batch: 3840 Loss: 0.041040686892358645\n",
            "Batch: 4160 Loss: 0.06392313633533898\n",
            "Batch: 4480 Loss: 0.06053777461039621\n",
            "Batch: 4800 Loss: 0.03962116259976707\n",
            "Batch: 5120 Loss: 0.05439160997211829\n",
            "Batch: 5440 Loss: 0.06006624056041471\n",
            "Batch: 5760 Loss: 0.07615985601164643\n",
            "Batch: 6080 Loss: 0.07293764097999841\n",
            "Batch: 6400 Loss: 0.052778720523142815\n",
            "Batch: 6720 Loss: 0.057528367959936356\n",
            "Batch: 7040 Loss: 0.053434315642879086\n",
            "Batch: 7360 Loss: 0.036283537998634145\n",
            "Batch: 7680 Loss: 0.056996889185667324\n",
            "Batch: 8000 Loss: 0.05713748126562782\n",
            "Batch: 8320 Loss: 0.04574694393379812\n",
            "Batch: 8640 Loss: 0.044785527076299986\n",
            "Batch: 8960 Loss: 0.04730925106546517\n",
            "Batch: 9280 Loss: 0.035754825187723105\n",
            "Batch: 9600 Loss: 0.053988342546761485\n",
            "Batch: 9920 Loss: 0.04749999204227039\n",
            "Batch: 10240 Loss: 0.03553728781561257\n",
            "Batch: 10560 Loss: 0.03395719237390778\n",
            "Batch: 10880 Loss: 0.0761016229138084\n",
            "Epoch: 288\n",
            "Batch: 0 Loss: 0.030480022714036093\n",
            "Batch: 320 Loss: 0.047039536718960454\n",
            "Batch: 640 Loss: 0.05932475657067152\n",
            "Batch: 960 Loss: 0.031542194211969694\n",
            "Batch: 1280 Loss: 0.05569545566173932\n",
            "Batch: 1600 Loss: 0.033803326877811075\n",
            "Batch: 1920 Loss: 0.03848367244802691\n",
            "Batch: 2240 Loss: 0.0501325091223929\n",
            "Batch: 2560 Loss: 0.04758792257263886\n",
            "Batch: 2880 Loss: 0.03253801524640264\n",
            "Batch: 3200 Loss: 0.04331440456672225\n",
            "Batch: 3520 Loss: 0.08310630494832129\n",
            "Batch: 3840 Loss: 0.033735930306124415\n",
            "Batch: 4160 Loss: 0.044870802180697764\n",
            "Batch: 4480 Loss: 0.03595607822756576\n",
            "Batch: 4800 Loss: 0.056161892771334855\n",
            "Batch: 5120 Loss: 0.08428945149762325\n",
            "Batch: 5440 Loss: 0.06222130988203047\n",
            "Batch: 5760 Loss: 0.04814471933430068\n",
            "Batch: 6080 Loss: 0.0451590072637325\n",
            "Batch: 6400 Loss: 0.044299661094627255\n",
            "Batch: 6720 Loss: 0.046582914174372686\n",
            "Batch: 7040 Loss: 0.045291518763785336\n",
            "Batch: 7360 Loss: 0.054632645493816814\n",
            "Batch: 7680 Loss: 0.07371522993906703\n",
            "Batch: 8000 Loss: 0.038344909433267446\n",
            "Batch: 8320 Loss: 0.08976433240613596\n",
            "Batch: 8640 Loss: 0.04003305365025849\n",
            "Batch: 8960 Loss: 0.046527714072611775\n",
            "Batch: 9280 Loss: 0.0428541317190106\n",
            "Batch: 9600 Loss: 0.043745503956668846\n",
            "Batch: 9920 Loss: 0.04871889039243105\n",
            "Batch: 10240 Loss: 0.042310190948481353\n",
            "Batch: 10560 Loss: 0.06909127483680846\n",
            "Batch: 10880 Loss: 0.05248160704582268\n",
            "Epoch: 289\n",
            "Batch: 0 Loss: 0.0537880033475167\n",
            "Batch: 320 Loss: 0.030780480322208162\n",
            "Batch: 640 Loss: 0.06656988209153264\n",
            "Batch: 960 Loss: 0.0717589777470504\n",
            "Batch: 1280 Loss: 0.0589293501738645\n",
            "Batch: 1600 Loss: 0.049009167992714466\n",
            "Batch: 1920 Loss: 0.0507923076978909\n",
            "Batch: 2240 Loss: 0.048860250721569944\n",
            "Batch: 2560 Loss: 0.04316659624704426\n",
            "Batch: 2880 Loss: 0.043135757299717706\n",
            "Batch: 3200 Loss: 0.06721955344889391\n",
            "Batch: 3520 Loss: 0.0370593664620516\n",
            "Batch: 3840 Loss: 0.04052220611395202\n",
            "Batch: 4160 Loss: 0.048580124584370346\n",
            "Batch: 4480 Loss: 0.058779629096639065\n",
            "Batch: 4800 Loss: 0.04962300048784212\n",
            "Batch: 5120 Loss: 0.05868140251100985\n",
            "Batch: 5440 Loss: 0.0606861958782553\n",
            "Batch: 5760 Loss: 0.0599337062331244\n",
            "Batch: 6080 Loss: 0.05486198400898248\n",
            "Batch: 6400 Loss: 0.030851126981403583\n",
            "Batch: 6720 Loss: 0.045311113350353795\n",
            "Batch: 7040 Loss: 0.05144178136308958\n",
            "Batch: 7360 Loss: 0.05254839796045227\n",
            "Batch: 7680 Loss: 0.05576823947863248\n",
            "Batch: 8000 Loss: 0.052332005528845156\n",
            "Batch: 8320 Loss: 0.04278450087943861\n",
            "Batch: 8640 Loss: 0.04813769689598498\n",
            "Batch: 8960 Loss: 0.058858990567722445\n",
            "Batch: 9280 Loss: 0.03181805364221267\n",
            "Batch: 9600 Loss: 0.03862103634056798\n",
            "Batch: 9920 Loss: 0.03180869377030205\n",
            "Batch: 10240 Loss: 0.044272133398410615\n",
            "Batch: 10560 Loss: 0.049454480337106074\n",
            "Batch: 10880 Loss: 0.03555562930116587\n",
            "Epoch: 290\n",
            "Batch: 0 Loss: 0.040921160011152063\n",
            "Batch: 320 Loss: 0.05663485483702677\n",
            "Batch: 640 Loss: 0.04400826220398376\n",
            "Batch: 960 Loss: 0.040973740424429125\n",
            "Batch: 1280 Loss: 0.0482355714949154\n",
            "Batch: 1600 Loss: 0.03693964264586116\n",
            "Batch: 1920 Loss: 0.08289173253649101\n",
            "Batch: 2240 Loss: 0.0667587464387008\n",
            "Batch: 2560 Loss: 0.06208656358375467\n",
            "Batch: 2880 Loss: 0.06418275528726448\n",
            "Batch: 3200 Loss: 0.05021162972970769\n",
            "Batch: 3520 Loss: 0.035058742140543504\n",
            "Batch: 3840 Loss: 0.04972121571689024\n",
            "Batch: 4160 Loss: 0.04363191480962198\n",
            "Batch: 4480 Loss: 0.05073885747954674\n",
            "Batch: 4800 Loss: 0.043810179698135325\n",
            "Batch: 5120 Loss: 0.05311600108125005\n",
            "Batch: 5440 Loss: 0.045369382193299\n",
            "Batch: 5760 Loss: 0.05127656290084398\n",
            "Batch: 6080 Loss: 0.05687952845298071\n",
            "Batch: 6400 Loss: 0.05529578903534932\n",
            "Batch: 6720 Loss: 0.07043923341565032\n",
            "Batch: 7040 Loss: 0.04188162640908209\n",
            "Batch: 7360 Loss: 0.03845348566194777\n",
            "Batch: 7680 Loss: 0.10120875917705467\n",
            "Batch: 8000 Loss: 0.03977939371385309\n",
            "Batch: 8320 Loss: 0.0445654699973167\n",
            "Batch: 8640 Loss: 0.056039132004119835\n",
            "Batch: 8960 Loss: 0.04008304698249499\n",
            "Batch: 9280 Loss: 0.045506477126691855\n",
            "Batch: 9600 Loss: 0.04492752100004036\n",
            "Batch: 9920 Loss: 0.04556821841406988\n",
            "Batch: 10240 Loss: 0.04388138642648008\n",
            "Batch: 10560 Loss: 0.0486547114836597\n",
            "Batch: 10880 Loss: 0.04550772815710166\n",
            "Epoch: 291\n",
            "Batch: 0 Loss: 0.05053707950762617\n",
            "Batch: 320 Loss: 0.03197890006159227\n",
            "Batch: 640 Loss: 0.04411917049462603\n",
            "Batch: 960 Loss: 0.0368757235456943\n",
            "Batch: 1280 Loss: 0.03572428632379285\n",
            "Batch: 1600 Loss: 0.04185811199517224\n",
            "Batch: 1920 Loss: 0.061248614095553676\n",
            "Batch: 2240 Loss: 0.040260333596977116\n",
            "Batch: 2560 Loss: 0.05045084110599108\n",
            "Batch: 2880 Loss: 0.07103727956393427\n",
            "Batch: 3200 Loss: 0.03177738729266596\n",
            "Batch: 3520 Loss: 0.0429213695038643\n",
            "Batch: 3840 Loss: 0.0319264308483721\n",
            "Batch: 4160 Loss: 0.05436414246254887\n",
            "Batch: 4480 Loss: 0.047518276926906314\n",
            "Batch: 4800 Loss: 0.051867966182084446\n",
            "Batch: 5120 Loss: 0.03818182158335581\n",
            "Batch: 5440 Loss: 0.038270373415101866\n",
            "Batch: 5760 Loss: 0.0501984711684877\n",
            "Batch: 6080 Loss: 0.04713653906467147\n",
            "Batch: 6400 Loss: 0.05602303567365863\n",
            "Batch: 6720 Loss: 0.0371838816304668\n",
            "Batch: 7040 Loss: 0.03892081002240168\n",
            "Batch: 7360 Loss: 0.04290150706399099\n",
            "Batch: 7680 Loss: 0.047799149766023924\n",
            "Batch: 8000 Loss: 0.05201282646744237\n",
            "Batch: 8320 Loss: 0.06163418635235682\n",
            "Batch: 8640 Loss: 0.054798128224474094\n",
            "Batch: 8960 Loss: 0.05528533521908319\n",
            "Batch: 9280 Loss: 0.0429491680423982\n",
            "Batch: 9600 Loss: 0.04038796170949675\n",
            "Batch: 9920 Loss: 0.041411598839795775\n",
            "Batch: 10240 Loss: 0.036204744667411554\n",
            "Batch: 10560 Loss: 0.05416714990750572\n",
            "Batch: 10880 Loss: 0.031775993079126486\n",
            "Epoch: 292\n",
            "Batch: 0 Loss: 0.06336954204554544\n",
            "Batch: 320 Loss: 0.03348882608912977\n",
            "Batch: 640 Loss: 0.0461871496318801\n",
            "Batch: 960 Loss: 0.050155768701036975\n",
            "Batch: 1280 Loss: 0.035787544181319346\n",
            "Batch: 1600 Loss: 0.05068530617295823\n",
            "Batch: 1920 Loss: 0.04824435974375909\n",
            "Batch: 2240 Loss: 0.05041839180827073\n",
            "Batch: 2560 Loss: 0.04764336865335439\n",
            "Batch: 2880 Loss: 0.05791273738732878\n",
            "Batch: 3200 Loss: 0.04847264346225688\n",
            "Batch: 3520 Loss: 0.05223470703413434\n",
            "Batch: 3840 Loss: 0.041216798718471795\n",
            "Batch: 4160 Loss: 0.04743089998300677\n",
            "Batch: 4480 Loss: 0.052533714077885006\n",
            "Batch: 4800 Loss: 0.062887618568884\n",
            "Batch: 5120 Loss: 0.038829596221649346\n",
            "Batch: 5440 Loss: 0.049800940779713296\n",
            "Batch: 5760 Loss: 0.05042768282190753\n",
            "Batch: 6080 Loss: 0.06225509781553999\n",
            "Batch: 6400 Loss: 0.07161674120614707\n",
            "Batch: 6720 Loss: 0.04229488433180963\n",
            "Batch: 7040 Loss: 0.0800550612112363\n",
            "Batch: 7360 Loss: 0.0397670712396429\n",
            "Batch: 7680 Loss: 0.053061155406416954\n",
            "Batch: 8000 Loss: 0.05989650103355052\n",
            "Batch: 8320 Loss: 0.05099481572129428\n",
            "Batch: 8640 Loss: 0.05107281339733194\n",
            "Batch: 8960 Loss: 0.07491296141943658\n",
            "Batch: 9280 Loss: 0.0604208869025033\n",
            "Batch: 9600 Loss: 0.04015120580623925\n",
            "Batch: 9920 Loss: 0.05955279169584252\n",
            "Batch: 10240 Loss: 0.04363507076428001\n",
            "Batch: 10560 Loss: 0.04083418362591951\n",
            "Batch: 10880 Loss: 0.04203395477517084\n",
            "Epoch: 293\n",
            "Batch: 0 Loss: 0.043958640424563905\n",
            "Batch: 320 Loss: 0.03663230414523038\n",
            "Batch: 640 Loss: 0.05727388064893063\n",
            "Batch: 960 Loss: 0.05364724237751367\n",
            "Batch: 1280 Loss: 0.04463489885938985\n",
            "Batch: 1600 Loss: 0.05496833055961542\n",
            "Batch: 1920 Loss: 0.03405521657880872\n",
            "Batch: 2240 Loss: 0.04447357490426869\n",
            "Batch: 2560 Loss: 0.05111972172055011\n",
            "Batch: 2880 Loss: 0.04911111025999836\n",
            "Batch: 3200 Loss: 0.04786478944663442\n",
            "Batch: 3520 Loss: 0.03689689330688072\n",
            "Batch: 3840 Loss: 0.04898705722070645\n",
            "Batch: 4160 Loss: 0.05712307851341708\n",
            "Batch: 4480 Loss: 0.05494742997736889\n",
            "Batch: 4800 Loss: 0.0345733553336372\n",
            "Batch: 5120 Loss: 0.04800054264572797\n",
            "Batch: 5440 Loss: 0.04081175165265645\n",
            "Batch: 5760 Loss: 0.04344424139775613\n",
            "Batch: 6080 Loss: 0.039116511919289845\n",
            "Batch: 6400 Loss: 0.027389410450527518\n",
            "Batch: 6720 Loss: 0.03881223974577808\n",
            "Batch: 7040 Loss: 0.04516140190700909\n",
            "Batch: 7360 Loss: 0.07148721005933101\n",
            "Batch: 7680 Loss: 0.05398366694385745\n",
            "Batch: 8000 Loss: 0.042014385753705356\n",
            "Batch: 8320 Loss: 0.04318526564785052\n",
            "Batch: 8640 Loss: 0.0445328139723805\n",
            "Batch: 8960 Loss: 0.03562966099865115\n",
            "Batch: 9280 Loss: 0.03143063491644176\n",
            "Batch: 9600 Loss: 0.04133634301164507\n",
            "Batch: 9920 Loss: 0.04866257366078073\n",
            "Batch: 10240 Loss: 0.05753247661693196\n",
            "Batch: 10560 Loss: 0.03403558580558758\n",
            "Batch: 10880 Loss: 0.03951708583567193\n",
            "Epoch: 294\n",
            "Batch: 0 Loss: 0.030043016901213616\n",
            "Batch: 320 Loss: 0.05590723939077543\n",
            "Batch: 640 Loss: 0.03693975203385821\n",
            "Batch: 960 Loss: 0.04403388415696776\n",
            "Batch: 1280 Loss: 0.06979508256065793\n",
            "Batch: 1600 Loss: 0.03370154486304778\n",
            "Batch: 1920 Loss: 0.046829237645112785\n",
            "Batch: 2240 Loss: 0.03894269550784525\n",
            "Batch: 2560 Loss: 0.05853660294491869\n",
            "Batch: 2880 Loss: 0.05003772009167257\n",
            "Batch: 3200 Loss: 0.048402717645485704\n",
            "Batch: 3520 Loss: 0.03606640233111515\n",
            "Batch: 3840 Loss: 0.02578352132154148\n",
            "Batch: 4160 Loss: 0.052089045247857145\n",
            "Batch: 4480 Loss: 0.07575953340393904\n",
            "Batch: 4800 Loss: 0.04573158519967156\n",
            "Batch: 5120 Loss: 0.03774217168410624\n",
            "Batch: 5440 Loss: 0.04056881681323858\n",
            "Batch: 5760 Loss: 0.05110799392640869\n",
            "Batch: 6080 Loss: 0.04570978538648353\n",
            "Batch: 6400 Loss: 0.04030570289497533\n",
            "Batch: 6720 Loss: 0.04063035536534784\n",
            "Batch: 7040 Loss: 0.032593794754183225\n",
            "Batch: 7360 Loss: 0.04678363218600559\n",
            "Batch: 7680 Loss: 0.06505219030884352\n",
            "Batch: 8000 Loss: 0.06445015690953615\n",
            "Batch: 8320 Loss: 0.07018189605089352\n",
            "Batch: 8640 Loss: 0.057986627174991874\n",
            "Batch: 8960 Loss: 0.06576094625971154\n",
            "Batch: 9280 Loss: 0.05839920403430136\n",
            "Batch: 9600 Loss: 0.03723350447537172\n",
            "Batch: 9920 Loss: 0.046945419190534673\n",
            "Batch: 10240 Loss: 0.039098881780098696\n",
            "Batch: 10560 Loss: 0.05106832016311143\n",
            "Batch: 10880 Loss: 0.04113811587113409\n",
            "Epoch: 295\n",
            "Batch: 0 Loss: 0.04669253049136798\n",
            "Batch: 320 Loss: 0.05864126081457713\n",
            "Batch: 640 Loss: 0.05754157316352488\n",
            "Batch: 960 Loss: 0.04538755924173972\n",
            "Batch: 1280 Loss: 0.033061074340477004\n",
            "Batch: 1600 Loss: 0.038778106303041265\n",
            "Batch: 1920 Loss: 0.04940769661750951\n",
            "Batch: 2240 Loss: 0.048184886855493665\n",
            "Batch: 2560 Loss: 0.04697642056638117\n",
            "Batch: 2880 Loss: 0.04643417772457786\n",
            "Batch: 3200 Loss: 0.041300625999135705\n",
            "Batch: 3520 Loss: 0.04371468852361952\n",
            "Batch: 3840 Loss: 0.048658989248388025\n",
            "Batch: 4160 Loss: 0.05751068674242501\n",
            "Batch: 4480 Loss: 0.03651228883606818\n",
            "Batch: 4800 Loss: 0.04501420694219863\n",
            "Batch: 5120 Loss: 0.0464608642522444\n",
            "Batch: 5440 Loss: 0.0525844758913121\n",
            "Batch: 5760 Loss: 0.0733762501990803\n",
            "Batch: 6080 Loss: 0.04608266327035147\n",
            "Batch: 6400 Loss: 0.04999552650006808\n",
            "Batch: 6720 Loss: 0.035760179304292004\n",
            "Batch: 7040 Loss: 0.04976415208485408\n",
            "Batch: 7360 Loss: 0.02972804961107635\n",
            "Batch: 7680 Loss: 0.04682935472771483\n",
            "Batch: 8000 Loss: 0.04813261412555493\n",
            "Batch: 8320 Loss: 0.0396412667571684\n",
            "Batch: 8640 Loss: 0.031642176745626596\n",
            "Batch: 8960 Loss: 0.033947734949158766\n",
            "Batch: 9280 Loss: 0.042330042175154736\n",
            "Batch: 9600 Loss: 0.045143802330287225\n",
            "Batch: 9920 Loss: 0.05583382568331336\n",
            "Batch: 10240 Loss: 0.05452556313350444\n",
            "Batch: 10560 Loss: 0.05725541049649958\n",
            "Batch: 10880 Loss: 0.06430989189943942\n",
            "Epoch: 296\n",
            "Batch: 0 Loss: 0.04317518553228642\n",
            "Batch: 320 Loss: 0.055209342462103676\n",
            "Batch: 640 Loss: 0.06869766721407822\n",
            "Batch: 960 Loss: 0.05405045441978238\n",
            "Batch: 1280 Loss: 0.026366144772661273\n",
            "Batch: 1600 Loss: 0.04552545719357275\n",
            "Batch: 1920 Loss: 0.0763395610897612\n",
            "Batch: 2240 Loss: 0.04692859261215891\n",
            "Batch: 2560 Loss: 0.04902016317092871\n",
            "Batch: 2880 Loss: 0.054962447579357955\n",
            "Batch: 3200 Loss: 0.06406048407236553\n",
            "Batch: 3520 Loss: 0.036731407504721654\n",
            "Batch: 3840 Loss: 0.03850965574218831\n",
            "Batch: 4160 Loss: 0.04195075841556498\n",
            "Batch: 4480 Loss: 0.07207948322717737\n",
            "Batch: 4800 Loss: 0.023717733866937905\n",
            "Batch: 5120 Loss: 0.042397973197186266\n",
            "Batch: 5440 Loss: 0.055912870332987474\n",
            "Batch: 5760 Loss: 0.056995687323921386\n",
            "Batch: 6080 Loss: 0.05482574332680927\n",
            "Batch: 6400 Loss: 0.039520725036295334\n",
            "Batch: 6720 Loss: 0.07382780843347898\n",
            "Batch: 7040 Loss: 0.06641646713067148\n",
            "Batch: 7360 Loss: 0.05542781828486699\n",
            "Batch: 7680 Loss: 0.05912545849149454\n",
            "Batch: 8000 Loss: 0.05592250370107754\n",
            "Batch: 8320 Loss: 0.05712621931803902\n",
            "Batch: 8640 Loss: 0.051560922310663435\n",
            "Batch: 8960 Loss: 0.0406783248564435\n",
            "Batch: 9280 Loss: 0.05017649533350217\n",
            "Batch: 9600 Loss: 0.05495330516406221\n",
            "Batch: 9920 Loss: 0.049027131073554525\n",
            "Batch: 10240 Loss: 0.045159273505263206\n",
            "Batch: 10560 Loss: 0.04103299088435455\n",
            "Batch: 10880 Loss: 0.030707172583539125\n",
            "Epoch: 297\n",
            "Batch: 0 Loss: 0.04882585745495855\n",
            "Batch: 320 Loss: 0.05972379967902041\n",
            "Batch: 640 Loss: 0.039344052219493636\n",
            "Batch: 960 Loss: 0.047624712489867785\n",
            "Batch: 1280 Loss: 0.029376862137636628\n",
            "Batch: 1600 Loss: 0.060105421290531556\n",
            "Batch: 1920 Loss: 0.04589947694691377\n",
            "Batch: 2240 Loss: 0.04838725925093239\n",
            "Batch: 2560 Loss: 0.04786969101051403\n",
            "Batch: 2880 Loss: 0.042430731079731034\n",
            "Batch: 3200 Loss: 0.03674977272007961\n",
            "Batch: 3520 Loss: 0.041049903231679376\n",
            "Batch: 3840 Loss: 0.03955343239018356\n",
            "Batch: 4160 Loss: 0.0684311035023435\n",
            "Batch: 4480 Loss: 0.043969897675140174\n",
            "Batch: 4800 Loss: 0.05474732485393558\n",
            "Batch: 5120 Loss: 0.0547756001349047\n",
            "Batch: 5440 Loss: 0.03692146707282547\n",
            "Batch: 5760 Loss: 0.06622464225116653\n",
            "Batch: 6080 Loss: 0.04930676905034883\n",
            "Batch: 6400 Loss: 0.06319059682192811\n",
            "Batch: 6720 Loss: 0.035868946101548324\n",
            "Batch: 7040 Loss: 0.041804063837562466\n",
            "Batch: 7360 Loss: 0.0564509427003411\n",
            "Batch: 7680 Loss: 0.047743966280546835\n",
            "Batch: 8000 Loss: 0.053368550099976646\n",
            "Batch: 8320 Loss: 0.05397392726367201\n",
            "Batch: 8640 Loss: 0.05893480878806357\n",
            "Batch: 8960 Loss: 0.04510385179800182\n",
            "Batch: 9280 Loss: 0.06134630693092908\n",
            "Batch: 9600 Loss: 0.05987929135645522\n",
            "Batch: 9920 Loss: 0.041297425022488995\n",
            "Batch: 10240 Loss: 0.0478965292287295\n",
            "Batch: 10560 Loss: 0.0883547517026919\n",
            "Batch: 10880 Loss: 0.04118646901094362\n",
            "Epoch: 298\n",
            "Batch: 0 Loss: 0.0429405709776121\n",
            "Batch: 320 Loss: 0.052956298176892555\n",
            "Batch: 640 Loss: 0.06358067086352102\n",
            "Batch: 960 Loss: 0.03102225306893391\n",
            "Batch: 1280 Loss: 0.05273900029370087\n",
            "Batch: 1600 Loss: 0.0661083118418087\n",
            "Batch: 1920 Loss: 0.0439933522376404\n",
            "Batch: 2240 Loss: 0.038830443174519155\n",
            "Batch: 2560 Loss: 0.04170210331304991\n",
            "Batch: 2880 Loss: 0.06047785692595801\n",
            "Batch: 3200 Loss: 0.06481410170069068\n",
            "Batch: 3520 Loss: 0.035713204409886665\n",
            "Batch: 3840 Loss: 0.045029609716756\n",
            "Batch: 4160 Loss: 0.038723088379611394\n",
            "Batch: 4480 Loss: 0.04576481860499645\n",
            "Batch: 4800 Loss: 0.04280351007040093\n",
            "Batch: 5120 Loss: 0.06203576293618\n",
            "Batch: 5440 Loss: 0.03912558539813005\n",
            "Batch: 5760 Loss: 0.051184770670007394\n",
            "Batch: 6080 Loss: 0.07296079477480631\n",
            "Batch: 6400 Loss: 0.040161292243286316\n",
            "Batch: 6720 Loss: 0.04309164551987271\n",
            "Batch: 7040 Loss: 0.04771042400470503\n",
            "Batch: 7360 Loss: 0.07311249731260726\n",
            "Batch: 7680 Loss: 0.05775559905316065\n",
            "Batch: 8000 Loss: 0.03619134360414347\n",
            "Batch: 8320 Loss: 0.04271365420996689\n",
            "Batch: 8640 Loss: 0.07384207533418863\n",
            "Batch: 8960 Loss: 0.06994916885360594\n",
            "Batch: 9280 Loss: 0.07888950558024732\n",
            "Batch: 9600 Loss: 0.042311919245354566\n",
            "Batch: 9920 Loss: 0.06916044847722334\n",
            "Batch: 10240 Loss: 0.05412604560039407\n",
            "Batch: 10560 Loss: 0.056296825750668494\n",
            "Batch: 10880 Loss: 0.058151680293218766\n",
            "Epoch: 299\n",
            "Batch: 0 Loss: 0.04635996234577466\n",
            "Batch: 320 Loss: 0.06673935480415327\n",
            "Batch: 640 Loss: 0.059931476331174995\n",
            "Batch: 960 Loss: 0.03139415058346635\n",
            "Batch: 1280 Loss: 0.03691749941813284\n",
            "Batch: 1600 Loss: 0.0471251656960203\n",
            "Batch: 1920 Loss: 0.04545268237865283\n",
            "Batch: 2240 Loss: 0.03824076209139779\n",
            "Batch: 2560 Loss: 0.05067092042506846\n",
            "Batch: 2880 Loss: 0.04372449355326215\n",
            "Batch: 3200 Loss: 0.052082800606460214\n",
            "Batch: 3520 Loss: 0.046058758097923896\n",
            "Batch: 3840 Loss: 0.05619664244553273\n",
            "Batch: 4160 Loss: 0.06257798333494824\n",
            "Batch: 4480 Loss: 0.06338308861950265\n",
            "Batch: 4800 Loss: 0.06932989008048472\n",
            "Batch: 5120 Loss: 0.05291242341383724\n",
            "Batch: 5440 Loss: 0.05193649205267452\n",
            "Batch: 5760 Loss: 0.04767603851667269\n",
            "Batch: 6080 Loss: 0.060841668044325496\n",
            "Batch: 6400 Loss: 0.05083730053749808\n",
            "Batch: 6720 Loss: 0.0764097968152117\n",
            "Batch: 7040 Loss: 0.048314424614152605\n",
            "Batch: 7360 Loss: 0.05793440662054535\n",
            "Batch: 7680 Loss: 0.058522724053299655\n",
            "Batch: 8000 Loss: 0.06343191945829403\n",
            "Batch: 8320 Loss: 0.04548513873117877\n",
            "Batch: 8640 Loss: 0.04875582072066996\n",
            "Batch: 8960 Loss: 0.05348518466582563\n",
            "Batch: 9280 Loss: 0.05339854678970653\n",
            "Batch: 9600 Loss: 0.03348669342090435\n",
            "Batch: 9920 Loss: 0.04204843953775642\n",
            "Batch: 10240 Loss: 0.06281134696187522\n",
            "Batch: 10560 Loss: 0.04486357009400522\n",
            "Batch: 10880 Loss: 0.04324996782455036\n",
            "Epoch: 300\n",
            "Batch: 0 Loss: 0.04400138057066882\n",
            "Batch: 320 Loss: 0.050690417147419495\n",
            "Batch: 640 Loss: 0.052123959423599306\n",
            "Batch: 960 Loss: 0.060747705170489344\n",
            "Batch: 1280 Loss: 0.06233491721626838\n",
            "Batch: 1600 Loss: 0.053473317299222334\n",
            "Batch: 1920 Loss: 0.030380209994174293\n",
            "Batch: 2240 Loss: 0.07186613993323834\n",
            "Batch: 2560 Loss: 0.038781648907792295\n",
            "Batch: 2880 Loss: 0.033674830947913945\n",
            "Batch: 3200 Loss: 0.050069470015320576\n",
            "Batch: 3520 Loss: 0.032649874085582994\n",
            "Batch: 3840 Loss: 0.04480132777843275\n",
            "Batch: 4160 Loss: 0.04569141945724683\n",
            "Batch: 4480 Loss: 0.06099592487427497\n",
            "Batch: 4800 Loss: 0.04298716926071667\n",
            "Batch: 5120 Loss: 0.06783119373741359\n",
            "Batch: 5440 Loss: 0.047493870301134646\n",
            "Batch: 5760 Loss: 0.04570124720390847\n",
            "Batch: 6080 Loss: 0.042448636009445956\n",
            "Batch: 6400 Loss: 0.04562757949522093\n",
            "Batch: 6720 Loss: 0.055623394877711695\n",
            "Batch: 7040 Loss: 0.04695268561333112\n",
            "Batch: 7360 Loss: 0.05159780129696291\n",
            "Batch: 7680 Loss: 0.05113191516418504\n",
            "Batch: 8000 Loss: 0.04244672790941793\n",
            "Batch: 8320 Loss: 0.05122046042468914\n",
            "Batch: 8640 Loss: 0.05464613329019758\n",
            "Batch: 8960 Loss: 0.044868744321227685\n",
            "Batch: 9280 Loss: 0.054810263433027695\n",
            "Batch: 9600 Loss: 0.06614873933008641\n",
            "Batch: 9920 Loss: 0.04806850779915589\n",
            "Batch: 10240 Loss: 0.04850044902288604\n",
            "Batch: 10560 Loss: 0.05759942020201755\n",
            "Batch: 10880 Loss: 0.023985298454134715\n",
            "Epoch: 301\n",
            "Batch: 0 Loss: 0.042494003535908445\n",
            "Batch: 320 Loss: 0.03751299960552712\n",
            "Batch: 640 Loss: 0.05711004450563796\n",
            "Batch: 960 Loss: 0.04609293013401479\n",
            "Batch: 1280 Loss: 0.05617183120709623\n",
            "Batch: 1600 Loss: 0.035884232850605206\n",
            "Batch: 1920 Loss: 0.052880126199931106\n",
            "Batch: 2240 Loss: 0.03984327492381381\n",
            "Batch: 2560 Loss: 0.0321515404663488\n",
            "Batch: 2880 Loss: 0.061339620901655394\n",
            "Batch: 3200 Loss: 0.0390224085807441\n",
            "Batch: 3520 Loss: 0.03871669067350064\n",
            "Batch: 3840 Loss: 0.04609644328158804\n",
            "Batch: 4160 Loss: 0.038761199419069775\n",
            "Batch: 4480 Loss: 0.0642071103411243\n",
            "Batch: 4800 Loss: 0.05385454815494826\n",
            "Batch: 5120 Loss: 0.04186231721478055\n",
            "Batch: 5440 Loss: 0.06337332726474044\n",
            "Batch: 5760 Loss: 0.03091292830840383\n",
            "Batch: 6080 Loss: 0.04965985898155323\n",
            "Batch: 6400 Loss: 0.0317352749922834\n",
            "Batch: 6720 Loss: 0.04111179583442058\n",
            "Batch: 7040 Loss: 0.056596747392583295\n",
            "Batch: 7360 Loss: 0.050453473494376955\n",
            "Batch: 7680 Loss: 0.046073712818801564\n",
            "Batch: 8000 Loss: 0.07041363046813394\n",
            "Batch: 8320 Loss: 0.04109006294684416\n",
            "Batch: 8640 Loss: 0.047349175765590674\n",
            "Batch: 8960 Loss: 0.056011783752435644\n",
            "Batch: 9280 Loss: 0.026061174381177526\n",
            "Batch: 9600 Loss: 0.04542708996324613\n",
            "Batch: 9920 Loss: 0.03717302263640463\n",
            "Batch: 10240 Loss: 0.04106597073525326\n",
            "Batch: 10560 Loss: 0.04870980512331697\n",
            "Batch: 10880 Loss: 0.037504543225804696\n",
            "Epoch: 302\n",
            "Batch: 0 Loss: 0.06200213614264446\n",
            "Batch: 320 Loss: 0.04481667648165729\n",
            "Batch: 640 Loss: 0.04581012063760514\n",
            "Batch: 960 Loss: 0.06529846994958229\n",
            "Batch: 1280 Loss: 0.04271168964028953\n",
            "Batch: 1600 Loss: 0.03901463430269559\n",
            "Batch: 1920 Loss: 0.06611685941491471\n",
            "Batch: 2240 Loss: 0.04710610381596729\n",
            "Batch: 2560 Loss: 0.03797282431333202\n",
            "Batch: 2880 Loss: 0.045785297085169326\n",
            "Batch: 3200 Loss: 0.05238559677841506\n",
            "Batch: 3520 Loss: 0.04788388073657478\n",
            "Batch: 3840 Loss: 0.05763577520540945\n",
            "Batch: 4160 Loss: 0.06613347459091637\n",
            "Batch: 4480 Loss: 0.038307375061188816\n",
            "Batch: 4800 Loss: 0.06110817780257571\n",
            "Batch: 5120 Loss: 0.0409554019533378\n",
            "Batch: 5440 Loss: 0.05933830305166849\n",
            "Batch: 5760 Loss: 0.050101171173381996\n",
            "Batch: 6080 Loss: 0.06144882831470767\n",
            "Batch: 6400 Loss: 0.03874381904131432\n",
            "Batch: 6720 Loss: 0.046764802203951156\n",
            "Batch: 7040 Loss: 0.0555919289100021\n",
            "Batch: 7360 Loss: 0.048295191300635615\n",
            "Batch: 7680 Loss: 0.05011167144741555\n",
            "Batch: 8000 Loss: 0.04111165505491958\n",
            "Batch: 8320 Loss: 0.05422957999046703\n",
            "Batch: 8640 Loss: 0.07248781289697113\n",
            "Batch: 8960 Loss: 0.04800371155900078\n",
            "Batch: 9280 Loss: 0.05319538761550151\n",
            "Batch: 9600 Loss: 0.060191603400709184\n",
            "Batch: 9920 Loss: 0.0410128824086379\n",
            "Batch: 10240 Loss: 0.050879889853581474\n",
            "Batch: 10560 Loss: 0.04284775366094381\n",
            "Batch: 10880 Loss: 0.07988384866179507\n",
            "Epoch: 303\n",
            "Batch: 0 Loss: 0.04045639775727032\n",
            "Batch: 320 Loss: 0.0531349717566566\n",
            "Batch: 640 Loss: 0.03862864552553235\n",
            "Batch: 960 Loss: 0.04880692406073068\n",
            "Batch: 1280 Loss: 0.03714609751947686\n",
            "Batch: 1600 Loss: 0.03566859666999033\n",
            "Batch: 1920 Loss: 0.0676939799385626\n",
            "Batch: 2240 Loss: 0.044631049938909095\n",
            "Batch: 2560 Loss: 0.043414927877327814\n",
            "Batch: 2880 Loss: 0.03692614742870597\n",
            "Batch: 3200 Loss: 0.05422982397687832\n",
            "Batch: 3520 Loss: 0.048964690214359535\n",
            "Batch: 3840 Loss: 0.03909687975138885\n",
            "Batch: 4160 Loss: 0.052000162283502675\n",
            "Batch: 4480 Loss: 0.041369835021184936\n",
            "Batch: 4800 Loss: 0.047427816282031715\n",
            "Batch: 5120 Loss: 0.060258098153957926\n",
            "Batch: 5440 Loss: 0.06213611187760259\n",
            "Batch: 5760 Loss: 0.06004117374247463\n",
            "Batch: 6080 Loss: 0.04372605200530352\n",
            "Batch: 6400 Loss: 0.04144834511270153\n",
            "Batch: 6720 Loss: 0.0619566987748736\n",
            "Batch: 7040 Loss: 0.034511453267983286\n",
            "Batch: 7360 Loss: 0.057116395675474295\n",
            "Batch: 7680 Loss: 0.03784752511524219\n",
            "Batch: 8000 Loss: 0.03464462477526566\n",
            "Batch: 8320 Loss: 0.05641944152913675\n",
            "Batch: 8640 Loss: 0.06404610166812112\n",
            "Batch: 8960 Loss: 0.041348755147280786\n",
            "Batch: 9280 Loss: 0.08546966975681818\n",
            "Batch: 9600 Loss: 0.04550034665175399\n",
            "Batch: 9920 Loss: 0.05227983211244833\n",
            "Batch: 10240 Loss: 0.0535463284815761\n",
            "Batch: 10560 Loss: 0.06944189513027024\n",
            "Batch: 10880 Loss: 0.023486577356336906\n",
            "Epoch: 304\n",
            "Batch: 0 Loss: 0.0473960668622519\n",
            "Batch: 320 Loss: 0.03792537937260374\n",
            "Batch: 640 Loss: 0.03572760900749259\n",
            "Batch: 960 Loss: 0.03335123755980165\n",
            "Batch: 1280 Loss: 0.06276516495035721\n",
            "Batch: 1600 Loss: 0.03804087341245194\n",
            "Batch: 1920 Loss: 0.05314281244769477\n",
            "Batch: 2240 Loss: 0.03864503973999924\n",
            "Batch: 2560 Loss: 0.04307270895896001\n",
            "Batch: 2880 Loss: 0.06587490408096308\n",
            "Batch: 3200 Loss: 0.07081587627287667\n",
            "Batch: 3520 Loss: 0.07745123724141428\n",
            "Batch: 3840 Loss: 0.04221625273130229\n",
            "Batch: 4160 Loss: 0.05724372406997441\n",
            "Batch: 4480 Loss: 0.06391541165380081\n",
            "Batch: 4800 Loss: 0.04364383155607082\n",
            "Batch: 5120 Loss: 0.03832303174237789\n",
            "Batch: 5440 Loss: 0.0446923144500019\n",
            "Batch: 5760 Loss: 0.05229018051951475\n",
            "Batch: 6080 Loss: 0.05694538363437578\n",
            "Batch: 6400 Loss: 0.04358760960560848\n",
            "Batch: 6720 Loss: 0.035137443712279254\n",
            "Batch: 7040 Loss: 0.04291440755744356\n",
            "Batch: 7360 Loss: 0.05358822713374959\n",
            "Batch: 7680 Loss: 0.04949366549724787\n",
            "Batch: 8000 Loss: 0.06274835437195969\n",
            "Batch: 8320 Loss: 0.06892256210023619\n",
            "Batch: 8640 Loss: 0.06587026258163826\n",
            "Batch: 8960 Loss: 0.08112029759136907\n",
            "Batch: 9280 Loss: 0.07619783918363884\n",
            "Batch: 9600 Loss: 0.043050843970181024\n",
            "Batch: 9920 Loss: 0.06290080734690222\n",
            "Batch: 10240 Loss: 0.0412315841193403\n",
            "Batch: 10560 Loss: 0.060826897284611256\n",
            "Batch: 10880 Loss: 0.06749537557984872\n",
            "Epoch: 305\n",
            "Batch: 0 Loss: 0.06321815534647723\n",
            "Batch: 320 Loss: 0.07241237149147857\n",
            "Batch: 640 Loss: 0.051724675938836805\n",
            "Batch: 960 Loss: 0.04740880971604273\n",
            "Batch: 1280 Loss: 0.05507510218794957\n",
            "Batch: 1600 Loss: 0.0555413003578864\n",
            "Batch: 1920 Loss: 0.05891778619980826\n",
            "Batch: 2240 Loss: 0.05788490091918074\n",
            "Batch: 2560 Loss: 0.02982528151232689\n",
            "Batch: 2880 Loss: 0.05033433723642596\n",
            "Batch: 3200 Loss: 0.07025875585734435\n",
            "Batch: 3520 Loss: 0.05289964181793318\n",
            "Batch: 3840 Loss: 0.03945112542146105\n",
            "Batch: 4160 Loss: 0.043824240498705105\n",
            "Batch: 4480 Loss: 0.060495898150300376\n",
            "Batch: 4800 Loss: 0.03667620110012056\n",
            "Batch: 5120 Loss: 0.042403470923529266\n",
            "Batch: 5440 Loss: 0.03446603433500525\n",
            "Batch: 5760 Loss: 0.055745782784491285\n",
            "Batch: 6080 Loss: 0.05316539973621452\n",
            "Batch: 6400 Loss: 0.03452287313436525\n",
            "Batch: 6720 Loss: 0.07581176943858464\n",
            "Batch: 7040 Loss: 0.05249921142134908\n",
            "Batch: 7360 Loss: 0.036632941390494125\n",
            "Batch: 7680 Loss: 0.0483533108056881\n",
            "Batch: 8000 Loss: 0.05010765181068247\n",
            "Batch: 8320 Loss: 0.05660283951729418\n",
            "Batch: 8640 Loss: 0.04925103391375512\n",
            "Batch: 8960 Loss: 0.0654582757692169\n",
            "Batch: 9280 Loss: 0.02902724836065281\n",
            "Batch: 9600 Loss: 0.062296317461179265\n",
            "Batch: 9920 Loss: 0.06345342732880906\n",
            "Batch: 10240 Loss: 0.047402429349246426\n",
            "Batch: 10560 Loss: 0.04073644168830186\n",
            "Batch: 10880 Loss: 0.0523430957037561\n",
            "Epoch: 306\n",
            "Batch: 0 Loss: 0.07691898967663194\n",
            "Batch: 320 Loss: 0.05684309557382428\n",
            "Batch: 640 Loss: 0.057028731546013525\n",
            "Batch: 960 Loss: 0.043721300263739056\n",
            "Batch: 1280 Loss: 0.033705098421680395\n",
            "Batch: 1600 Loss: 0.050918084836095195\n",
            "Batch: 1920 Loss: 0.05836654830885296\n",
            "Batch: 2240 Loss: 0.0572193421222541\n",
            "Batch: 2560 Loss: 0.06740398711694146\n",
            "Batch: 2880 Loss: 0.05061527023370648\n",
            "Batch: 3200 Loss: 0.08887338802162309\n",
            "Batch: 3520 Loss: 0.055593408201106746\n",
            "Batch: 3840 Loss: 0.03194915015813691\n",
            "Batch: 4160 Loss: 0.04693988863368348\n",
            "Batch: 4480 Loss: 0.050687139038933154\n",
            "Batch: 4800 Loss: 0.031501107581064364\n",
            "Batch: 5120 Loss: 0.03921136467386897\n",
            "Batch: 5440 Loss: 0.07128385472043204\n",
            "Batch: 5760 Loss: 0.06094848724912794\n",
            "Batch: 6080 Loss: 0.039543102126372964\n",
            "Batch: 6400 Loss: 0.04427181813847122\n",
            "Batch: 6720 Loss: 0.060822574251302414\n",
            "Batch: 7040 Loss: 0.04657696972252293\n",
            "Batch: 7360 Loss: 0.04157145562191191\n",
            "Batch: 7680 Loss: 0.054476550869409605\n",
            "Batch: 8000 Loss: 0.06763718503981009\n",
            "Batch: 8320 Loss: 0.06714140303794579\n",
            "Batch: 8640 Loss: 0.08745542428364136\n",
            "Batch: 8960 Loss: 0.05110053069890057\n",
            "Batch: 9280 Loss: 0.05931433094020321\n",
            "Batch: 9600 Loss: 0.05647626127253136\n",
            "Batch: 9920 Loss: 0.05342102884833074\n",
            "Batch: 10240 Loss: 0.03153770969288485\n",
            "Batch: 10560 Loss: 0.06115542082183844\n",
            "Batch: 10880 Loss: 0.06329196278671449\n",
            "Epoch: 307\n",
            "Batch: 0 Loss: 0.05613286866043999\n",
            "Batch: 320 Loss: 0.06641550214287183\n",
            "Batch: 640 Loss: 0.03758282455416245\n",
            "Batch: 960 Loss: 0.055279505159961925\n",
            "Batch: 1280 Loss: 0.048813311539543686\n",
            "Batch: 1600 Loss: 0.06821717844906736\n",
            "Batch: 1920 Loss: 0.04945635217452873\n",
            "Batch: 2240 Loss: 0.061826725657119795\n",
            "Batch: 2560 Loss: 0.06184848838069616\n",
            "Batch: 2880 Loss: 0.058568635279539996\n",
            "Batch: 3200 Loss: 0.05554341774602939\n",
            "Batch: 3520 Loss: 0.038398839006768076\n",
            "Batch: 3840 Loss: 0.05247514612739131\n",
            "Batch: 4160 Loss: 0.040388908486626844\n",
            "Batch: 4480 Loss: 0.04304583934547114\n",
            "Batch: 4800 Loss: 0.062193609795690635\n",
            "Batch: 5120 Loss: 0.04204897755626519\n",
            "Batch: 5440 Loss: 0.06889552527073622\n",
            "Batch: 5760 Loss: 0.060415597861431\n",
            "Batch: 6080 Loss: 0.04902924079919135\n",
            "Batch: 6400 Loss: 0.03496417218625416\n",
            "Batch: 6720 Loss: 0.03829526856156334\n",
            "Batch: 7040 Loss: 0.03344682212066767\n",
            "Batch: 7360 Loss: 0.03953705968344347\n",
            "Batch: 7680 Loss: 0.048822098595158854\n",
            "Batch: 8000 Loss: 0.046146806054715064\n",
            "Batch: 8320 Loss: 0.05505197064810266\n",
            "Batch: 8640 Loss: 0.07321349648628417\n",
            "Batch: 8960 Loss: 0.04748051727243893\n",
            "Batch: 9280 Loss: 0.04585704726404667\n",
            "Batch: 9600 Loss: 0.06506031653961322\n",
            "Batch: 9920 Loss: 0.049530007786585235\n",
            "Batch: 10240 Loss: 0.04571391635873936\n",
            "Batch: 10560 Loss: 0.030210666556218662\n",
            "Batch: 10880 Loss: 0.06078158351710236\n",
            "Epoch: 308\n",
            "Batch: 0 Loss: 0.03574322686321303\n",
            "Batch: 320 Loss: 0.03917576743914957\n",
            "Batch: 640 Loss: 0.056704465470114596\n",
            "Batch: 960 Loss: 0.04518855813528907\n",
            "Batch: 1280 Loss: 0.050196337816490307\n",
            "Batch: 1600 Loss: 0.07370635641596726\n",
            "Batch: 1920 Loss: 0.06963892885331842\n",
            "Batch: 2240 Loss: 0.05878925009361172\n",
            "Batch: 2560 Loss: 0.03729782386952345\n",
            "Batch: 2880 Loss: 0.05636446329133438\n",
            "Batch: 3200 Loss: 0.04788292378817388\n",
            "Batch: 3520 Loss: 0.04428161861237441\n",
            "Batch: 3840 Loss: 0.04461451184697004\n",
            "Batch: 4160 Loss: 0.07349801535886868\n",
            "Batch: 4480 Loss: 0.0441767397484519\n",
            "Batch: 4800 Loss: 0.0460742226302768\n",
            "Batch: 5120 Loss: 0.059667124665060634\n",
            "Batch: 5440 Loss: 0.0396605581738619\n",
            "Batch: 5760 Loss: 0.035436692412040435\n",
            "Batch: 6080 Loss: 0.07166962062474941\n",
            "Batch: 6400 Loss: 0.06707369449010352\n",
            "Batch: 6720 Loss: 0.051626702853153336\n",
            "Batch: 7040 Loss: 0.0485475899028601\n",
            "Batch: 7360 Loss: 0.037391461319649495\n",
            "Batch: 7680 Loss: 0.04453707779864259\n",
            "Batch: 8000 Loss: 0.07517456755075069\n",
            "Batch: 8320 Loss: 0.03879906693543405\n",
            "Batch: 8640 Loss: 0.033530743249651065\n",
            "Batch: 8960 Loss: 0.043896480406114206\n",
            "Batch: 9280 Loss: 0.05011953595753457\n",
            "Batch: 9600 Loss: 0.0865061089933313\n",
            "Batch: 9920 Loss: 0.03781008066448143\n",
            "Batch: 10240 Loss: 0.051888652984420525\n",
            "Batch: 10560 Loss: 0.051040988456992224\n",
            "Batch: 10880 Loss: 0.04917842159722766\n",
            "Epoch: 309\n",
            "Batch: 0 Loss: 0.0665293090816347\n",
            "Batch: 320 Loss: 0.03832589648513032\n",
            "Batch: 640 Loss: 0.06121913276066338\n",
            "Batch: 960 Loss: 0.06284619589173815\n",
            "Batch: 1280 Loss: 0.06944825122537646\n",
            "Batch: 1600 Loss: 0.046014593659315366\n",
            "Batch: 1920 Loss: 0.02996343011653223\n",
            "Batch: 2240 Loss: 0.06482337770005064\n",
            "Batch: 2560 Loss: 0.05523488297063253\n",
            "Batch: 2880 Loss: 0.040433726872157245\n",
            "Batch: 3200 Loss: 0.0472758104724705\n",
            "Batch: 3520 Loss: 0.06854686447183544\n",
            "Batch: 3840 Loss: 0.0413893988374296\n",
            "Batch: 4160 Loss: 0.07926826296077918\n",
            "Batch: 4480 Loss: 0.041964973148880404\n",
            "Batch: 4800 Loss: 0.04197869976907108\n",
            "Batch: 5120 Loss: 0.041638604489716693\n",
            "Batch: 5440 Loss: 0.06146810697301774\n",
            "Batch: 5760 Loss: 0.04845770835512624\n",
            "Batch: 6080 Loss: 0.0603947756854538\n",
            "Batch: 6400 Loss: 0.03223258010606279\n",
            "Batch: 6720 Loss: 0.0526711223312094\n",
            "Batch: 7040 Loss: 0.04486781661515549\n",
            "Batch: 7360 Loss: 0.03334762260619091\n",
            "Batch: 7680 Loss: 0.05896961029296825\n",
            "Batch: 8000 Loss: 0.04952922993941678\n",
            "Batch: 8320 Loss: 0.058424352313622266\n",
            "Batch: 8640 Loss: 0.04468733613536086\n",
            "Batch: 8960 Loss: 0.051842416075257726\n",
            "Batch: 9280 Loss: 0.04374367896120573\n",
            "Batch: 9600 Loss: 0.05594308138191371\n",
            "Batch: 9920 Loss: 0.07871059165225154\n",
            "Batch: 10240 Loss: 0.03934226028398142\n",
            "Batch: 10560 Loss: 0.043068474914338364\n",
            "Batch: 10880 Loss: 0.04805529960543016\n",
            "Epoch: 310\n",
            "Batch: 0 Loss: 0.05595090787257534\n",
            "Batch: 320 Loss: 0.052482197790103664\n",
            "Batch: 640 Loss: 0.039298348543574564\n",
            "Batch: 960 Loss: 0.04360991889133963\n",
            "Batch: 1280 Loss: 0.03676951866338353\n",
            "Batch: 1600 Loss: 0.0796845055843484\n",
            "Batch: 1920 Loss: 0.056652403173174534\n",
            "Batch: 2240 Loss: 0.040958229140357245\n",
            "Batch: 2560 Loss: 0.05400255313570944\n",
            "Batch: 2880 Loss: 0.035768083391294725\n",
            "Batch: 3200 Loss: 0.046380244928238445\n",
            "Batch: 3520 Loss: 0.05339308434914402\n",
            "Batch: 3840 Loss: 0.07011046815820678\n",
            "Batch: 4160 Loss: 0.04597331913763714\n",
            "Batch: 4480 Loss: 0.03677436653462006\n",
            "Batch: 4800 Loss: 0.049840337686275354\n",
            "Batch: 5120 Loss: 0.03839643795317785\n",
            "Batch: 5440 Loss: 0.0458753735124667\n",
            "Batch: 5760 Loss: 0.03311739597129001\n",
            "Batch: 6080 Loss: 0.0410812577339387\n",
            "Batch: 6400 Loss: 0.03828155616443316\n",
            "Batch: 6720 Loss: 0.04869179248256522\n",
            "Batch: 7040 Loss: 0.06128409177513878\n",
            "Batch: 7360 Loss: 0.051330258135125706\n",
            "Batch: 7680 Loss: 0.04036356182358024\n",
            "Batch: 8000 Loss: 0.07433316109197773\n",
            "Batch: 8320 Loss: 0.04105201391840207\n",
            "Batch: 8640 Loss: 0.04317435270580042\n",
            "Batch: 8960 Loss: 0.02675993707884493\n",
            "Batch: 9280 Loss: 0.06370751063936515\n",
            "Batch: 9600 Loss: 0.04412460920372434\n",
            "Batch: 9920 Loss: 0.056028593052246084\n",
            "Batch: 10240 Loss: 0.04940885087293879\n",
            "Batch: 10560 Loss: 0.04087008962969272\n",
            "Batch: 10880 Loss: 0.048502485573227797\n",
            "Epoch: 311\n",
            "Batch: 0 Loss: 0.05322966407600851\n",
            "Batch: 320 Loss: 0.043537305647536946\n",
            "Batch: 640 Loss: 0.05519232649409853\n",
            "Batch: 960 Loss: 0.0441150447075308\n",
            "Batch: 1280 Loss: 0.034098685170867865\n",
            "Batch: 1600 Loss: 0.04153114023986903\n",
            "Batch: 1920 Loss: 0.06911071928498319\n",
            "Batch: 2240 Loss: 0.037171259915057\n",
            "Batch: 2560 Loss: 0.06590710338342069\n",
            "Batch: 2880 Loss: 0.057151339139074935\n",
            "Batch: 3200 Loss: 0.036864569384589235\n",
            "Batch: 3520 Loss: 0.05656654946348008\n",
            "Batch: 3840 Loss: 0.033073437286837794\n",
            "Batch: 4160 Loss: 0.03182069846026919\n",
            "Batch: 4480 Loss: 0.04020056174047342\n",
            "Batch: 4800 Loss: 0.0588441635700081\n",
            "Batch: 5120 Loss: 0.04830253741849638\n",
            "Batch: 5440 Loss: 0.05592203954883876\n",
            "Batch: 5760 Loss: 0.0418300471151791\n",
            "Batch: 6080 Loss: 0.03676426137367708\n",
            "Batch: 6400 Loss: 0.03333212326519334\n",
            "Batch: 6720 Loss: 0.04232336927080066\n",
            "Batch: 7040 Loss: 0.042520010053602716\n",
            "Batch: 7360 Loss: 0.047744823600519785\n",
            "Batch: 7680 Loss: 0.03031643551024766\n",
            "Batch: 8000 Loss: 0.050825263545559704\n",
            "Batch: 8320 Loss: 0.043041973607886794\n",
            "Batch: 8640 Loss: 0.045272019520043616\n",
            "Batch: 8960 Loss: 0.03085751728665989\n",
            "Batch: 9280 Loss: 0.05285015689680274\n",
            "Batch: 9600 Loss: 0.04059332900181854\n",
            "Batch: 9920 Loss: 0.03265953621509904\n",
            "Batch: 10240 Loss: 0.05781891103086247\n",
            "Batch: 10560 Loss: 0.05810009787354761\n",
            "Batch: 10880 Loss: 0.031866236560764157\n",
            "Epoch: 312\n",
            "Batch: 0 Loss: 0.08548558144288534\n",
            "Batch: 320 Loss: 0.05191144748509445\n",
            "Batch: 640 Loss: 0.05143465914808237\n",
            "Batch: 960 Loss: 0.0665486419172695\n",
            "Batch: 1280 Loss: 0.048694188762321386\n",
            "Batch: 1600 Loss: 0.039775685834469186\n",
            "Batch: 1920 Loss: 0.05491647920925971\n",
            "Batch: 2240 Loss: 0.04808719591818681\n",
            "Batch: 2560 Loss: 0.038575732792424086\n",
            "Batch: 2880 Loss: 0.07344101743244288\n",
            "Batch: 3200 Loss: 0.04922873352589847\n",
            "Batch: 3520 Loss: 0.04930789269053096\n",
            "Batch: 3840 Loss: 0.08361337718260964\n",
            "Batch: 4160 Loss: 0.045423708541711905\n",
            "Batch: 4480 Loss: 0.05851142884379165\n",
            "Batch: 4800 Loss: 0.07578213728151668\n",
            "Batch: 5120 Loss: 0.04592874521294631\n",
            "Batch: 5440 Loss: 0.04411340853429508\n",
            "Batch: 5760 Loss: 0.05657975850872469\n",
            "Batch: 6080 Loss: 0.03452193875664138\n",
            "Batch: 6400 Loss: 0.048543715239760636\n",
            "Batch: 6720 Loss: 0.060951962823483856\n",
            "Batch: 7040 Loss: 0.038685607481547965\n",
            "Batch: 7360 Loss: 0.047183908314685274\n",
            "Batch: 7680 Loss: 0.03396827127508166\n",
            "Batch: 8000 Loss: 0.07590054678629997\n",
            "Batch: 8320 Loss: 0.025425243984812978\n",
            "Batch: 8640 Loss: 0.035508062251270095\n",
            "Batch: 8960 Loss: 0.07234876803456261\n",
            "Batch: 9280 Loss: 0.0416376706688543\n",
            "Batch: 9600 Loss: 0.08944600119597569\n",
            "Batch: 9920 Loss: 0.03206415118314109\n",
            "Batch: 10240 Loss: 0.04428425532143819\n",
            "Batch: 10560 Loss: 0.034242402803056196\n",
            "Batch: 10880 Loss: 0.06527811950846958\n",
            "Epoch: 313\n",
            "Batch: 0 Loss: 0.06048290289154842\n",
            "Batch: 320 Loss: 0.03457976667123269\n",
            "Batch: 640 Loss: 0.06610053455376733\n",
            "Batch: 960 Loss: 0.08524127690567472\n",
            "Batch: 1280 Loss: 0.03421968246692806\n",
            "Batch: 1600 Loss: 0.04842320277188589\n",
            "Batch: 1920 Loss: 0.04698721267027007\n",
            "Batch: 2240 Loss: 0.0365322118495915\n",
            "Batch: 2560 Loss: 0.07006588863516551\n",
            "Batch: 2880 Loss: 0.0349577762156159\n",
            "Batch: 3200 Loss: 0.054828096949901976\n",
            "Batch: 3520 Loss: 0.05251030405934715\n",
            "Batch: 3840 Loss: 0.03467646582952372\n",
            "Batch: 4160 Loss: 0.05720844537274187\n",
            "Batch: 4480 Loss: 0.04363146917904949\n",
            "Batch: 4800 Loss: 0.037203501439364635\n",
            "Batch: 5120 Loss: 0.03925202455194016\n",
            "Batch: 5440 Loss: 0.04259058667435124\n",
            "Batch: 5760 Loss: 0.03393413419983259\n",
            "Batch: 6080 Loss: 0.036653270775019\n",
            "Batch: 6400 Loss: 0.07520712963981967\n",
            "Batch: 6720 Loss: 0.040249469014836906\n",
            "Batch: 7040 Loss: 0.02938773937918279\n",
            "Batch: 7360 Loss: 0.05446946657824561\n",
            "Batch: 7680 Loss: 0.06587144849396877\n",
            "Batch: 8000 Loss: 0.03694051171143541\n",
            "Batch: 8320 Loss: 0.04192585844665814\n",
            "Batch: 8640 Loss: 0.0354336704242162\n",
            "Batch: 8960 Loss: 0.06295742285127044\n",
            "Batch: 9280 Loss: 0.03739337309385506\n",
            "Batch: 9600 Loss: 0.04241595795839614\n",
            "Batch: 9920 Loss: 0.05226925589061288\n",
            "Batch: 10240 Loss: 0.03619628329346452\n",
            "Batch: 10560 Loss: 0.036421790003998915\n",
            "Batch: 10880 Loss: 0.0446884762347138\n",
            "Epoch: 314\n",
            "Batch: 0 Loss: 0.08257638055432007\n",
            "Batch: 320 Loss: 0.04060934794120573\n",
            "Batch: 640 Loss: 0.029278151074405116\n",
            "Batch: 960 Loss: 0.03000437984206003\n",
            "Batch: 1280 Loss: 0.06969778049692787\n",
            "Batch: 1600 Loss: 0.04816580584582572\n",
            "Batch: 1920 Loss: 0.06184534115286347\n",
            "Batch: 2240 Loss: 0.04183111672802579\n",
            "Batch: 2560 Loss: 0.05482032772191167\n",
            "Batch: 2880 Loss: 0.05786762955963934\n",
            "Batch: 3200 Loss: 0.06974417358238584\n",
            "Batch: 3520 Loss: 0.06686574694733038\n",
            "Batch: 3840 Loss: 0.035995460361595205\n",
            "Batch: 4160 Loss: 0.03962628492784914\n",
            "Batch: 4480 Loss: 0.042712161749314886\n",
            "Batch: 4800 Loss: 0.05627463094362604\n",
            "Batch: 5120 Loss: 0.034779874412255926\n",
            "Batch: 5440 Loss: 0.0619301159672396\n",
            "Batch: 5760 Loss: 0.03158736678152682\n",
            "Batch: 6080 Loss: 0.028644544951506746\n",
            "Batch: 6400 Loss: 0.04197207473574893\n",
            "Batch: 6720 Loss: 0.0486694655351457\n",
            "Batch: 7040 Loss: 0.062218089651853034\n",
            "Batch: 7360 Loss: 0.038104742211527085\n",
            "Batch: 7680 Loss: 0.03837984804910947\n",
            "Batch: 8000 Loss: 0.0449735504117828\n",
            "Batch: 8320 Loss: 0.05976187096848207\n",
            "Batch: 8640 Loss: 0.039411052778826776\n",
            "Batch: 8960 Loss: 0.07434427662851056\n",
            "Batch: 9280 Loss: 0.052981429333035415\n",
            "Batch: 9600 Loss: 0.05545120883123563\n",
            "Batch: 9920 Loss: 0.054492836895363356\n",
            "Batch: 10240 Loss: 0.053199860458486736\n",
            "Batch: 10560 Loss: 0.0536080002626794\n",
            "Batch: 10880 Loss: 0.024798028151689022\n",
            "Epoch: 315\n",
            "Batch: 0 Loss: 0.05179002393556647\n",
            "Batch: 320 Loss: 0.04352813035151658\n",
            "Batch: 640 Loss: 0.04814178847521367\n",
            "Batch: 960 Loss: 0.027046040628781267\n",
            "Batch: 1280 Loss: 0.05711365484264925\n",
            "Batch: 1600 Loss: 0.04553820254533387\n",
            "Batch: 1920 Loss: 0.03715530897546709\n",
            "Batch: 2240 Loss: 0.04767323353139505\n",
            "Batch: 2560 Loss: 0.07983475859470762\n",
            "Batch: 2880 Loss: 0.03733853200773613\n",
            "Batch: 3200 Loss: 0.054276053937265686\n",
            "Batch: 3520 Loss: 0.0529684280663459\n",
            "Batch: 3840 Loss: 0.04756550706894061\n",
            "Batch: 4160 Loss: 0.039859956332304415\n",
            "Batch: 4480 Loss: 0.053212729457262874\n",
            "Batch: 4800 Loss: 0.03980977916763931\n",
            "Batch: 5120 Loss: 0.06271339592809677\n",
            "Batch: 5440 Loss: 0.07047030109400433\n",
            "Batch: 5760 Loss: 0.04426695444321087\n",
            "Batch: 6080 Loss: 0.03831777536674675\n",
            "Batch: 6400 Loss: 0.03560025536432459\n",
            "Batch: 6720 Loss: 0.03080892864187365\n",
            "Batch: 7040 Loss: 0.06693554254243239\n",
            "Batch: 7360 Loss: 0.07734413248911491\n",
            "Batch: 7680 Loss: 0.04092963124545003\n",
            "Batch: 8000 Loss: 0.04773369403319061\n",
            "Batch: 8320 Loss: 0.04272500570476501\n",
            "Batch: 8640 Loss: 0.042689814483040975\n",
            "Batch: 8960 Loss: 0.04583763860567913\n",
            "Batch: 9280 Loss: 0.059489989105004235\n",
            "Batch: 9600 Loss: 0.04281162522474504\n",
            "Batch: 9920 Loss: 0.04404175652561425\n",
            "Batch: 10240 Loss: 0.056639896132374246\n",
            "Batch: 10560 Loss: 0.045398114103899406\n",
            "Batch: 10880 Loss: 0.042631440270952586\n",
            "Epoch: 316\n",
            "Batch: 0 Loss: 0.05841942675424935\n",
            "Batch: 320 Loss: 0.03445712167427446\n",
            "Batch: 640 Loss: 0.04399315805130548\n",
            "Batch: 960 Loss: 0.05048506550183881\n",
            "Batch: 1280 Loss: 0.043416243330778884\n",
            "Batch: 1600 Loss: 0.03884017905579626\n",
            "Batch: 1920 Loss: 0.04439996780558994\n",
            "Batch: 2240 Loss: 0.056357184806291774\n",
            "Batch: 2560 Loss: 0.029733590404115003\n",
            "Batch: 2880 Loss: 0.05624725836389208\n",
            "Batch: 3200 Loss: 0.04919203648156718\n",
            "Batch: 3520 Loss: 0.06961819533266025\n",
            "Batch: 3840 Loss: 0.06108887916584169\n",
            "Batch: 4160 Loss: 0.03971764524965815\n",
            "Batch: 4480 Loss: 0.05271324771865904\n",
            "Batch: 4800 Loss: 0.04806515815779286\n",
            "Batch: 5120 Loss: 0.02901953894523148\n",
            "Batch: 5440 Loss: 0.0454840584729687\n",
            "Batch: 5760 Loss: 0.04617445816295597\n",
            "Batch: 6080 Loss: 0.029969669778386603\n",
            "Batch: 6400 Loss: 0.0317856246295213\n",
            "Batch: 6720 Loss: 0.03932079598765857\n",
            "Batch: 7040 Loss: 0.04874713323225738\n",
            "Batch: 7360 Loss: 0.05064259716459682\n",
            "Batch: 7680 Loss: 0.04166185709570904\n",
            "Batch: 8000 Loss: 0.06605602029071451\n",
            "Batch: 8320 Loss: 0.05529967865461299\n",
            "Batch: 8640 Loss: 0.05666057032626951\n",
            "Batch: 8960 Loss: 0.07222313540854093\n",
            "Batch: 9280 Loss: 0.06988990295560227\n",
            "Batch: 9600 Loss: 0.04829327345515882\n",
            "Batch: 9920 Loss: 0.04760052871128276\n",
            "Batch: 10240 Loss: 0.06182867188321318\n",
            "Batch: 10560 Loss: 0.03875906013219404\n",
            "Batch: 10880 Loss: 0.029469303115199352\n",
            "Epoch: 317\n",
            "Batch: 0 Loss: 0.0672754407450924\n",
            "Batch: 320 Loss: 0.03812290152348495\n",
            "Batch: 640 Loss: 0.042534781277806556\n",
            "Batch: 960 Loss: 0.04799712117514007\n",
            "Batch: 1280 Loss: 0.058470242483328404\n",
            "Batch: 1600 Loss: 0.042978802897614014\n",
            "Batch: 1920 Loss: 0.0642683443647617\n",
            "Batch: 2240 Loss: 0.03501345987102476\n",
            "Batch: 2560 Loss: 0.0487273251037771\n",
            "Batch: 2880 Loss: 0.05866285284073995\n",
            "Batch: 3200 Loss: 0.0512582245970948\n",
            "Batch: 3520 Loss: 0.05576260208458656\n",
            "Batch: 3840 Loss: 0.03904939568997193\n",
            "Batch: 4160 Loss: 0.04082446723614947\n",
            "Batch: 4480 Loss: 0.031139779410575524\n",
            "Batch: 4800 Loss: 0.0325729983609326\n",
            "Batch: 5120 Loss: 0.056624383929058685\n",
            "Batch: 5440 Loss: 0.06749337506131003\n",
            "Batch: 5760 Loss: 0.07228895528895311\n",
            "Batch: 6080 Loss: 0.06140684008776354\n",
            "Batch: 6400 Loss: 0.051683197668865415\n",
            "Batch: 6720 Loss: 0.05493315316447112\n",
            "Batch: 7040 Loss: 0.04243350158819742\n",
            "Batch: 7360 Loss: 0.07520402712977751\n",
            "Batch: 7680 Loss: 0.055867100268807326\n",
            "Batch: 8000 Loss: 0.04558998383066049\n",
            "Batch: 8320 Loss: 0.05593257047679848\n",
            "Batch: 8640 Loss: 0.04350150591359761\n",
            "Batch: 8960 Loss: 0.03914378892938524\n",
            "Batch: 9280 Loss: 0.033405990055571405\n",
            "Batch: 9600 Loss: 0.04568195745391112\n",
            "Batch: 9920 Loss: 0.052214469460219515\n",
            "Batch: 10240 Loss: 0.046744950449675644\n",
            "Batch: 10560 Loss: 0.05527519208320724\n",
            "Batch: 10880 Loss: 0.03514497548371603\n",
            "Epoch: 318\n",
            "Batch: 0 Loss: 0.06492233052665763\n",
            "Batch: 320 Loss: 0.05975481982823278\n",
            "Batch: 640 Loss: 0.04823495322523111\n",
            "Batch: 960 Loss: 0.029009143673251612\n",
            "Batch: 1280 Loss: 0.048098655116069824\n",
            "Batch: 1600 Loss: 0.04238666295487983\n",
            "Batch: 1920 Loss: 0.03841007144327139\n",
            "Batch: 2240 Loss: 0.034053234566406576\n",
            "Batch: 2560 Loss: 0.03401304623638262\n",
            "Batch: 2880 Loss: 0.05091784475764735\n",
            "Batch: 3200 Loss: 0.027167233693433945\n",
            "Batch: 3520 Loss: 0.039846620999742795\n",
            "Batch: 3840 Loss: 0.04601863772306107\n",
            "Batch: 4160 Loss: 0.03988155345669209\n",
            "Batch: 4480 Loss: 0.053281413960454424\n",
            "Batch: 4800 Loss: 0.03691926885377652\n",
            "Batch: 5120 Loss: 0.05292224140434957\n",
            "Batch: 5440 Loss: 0.04320503403096965\n",
            "Batch: 5760 Loss: 0.049397080651837164\n",
            "Batch: 6080 Loss: 0.05538177539430747\n",
            "Batch: 6400 Loss: 0.043226957136009485\n",
            "Batch: 6720 Loss: 0.04533440342094823\n",
            "Batch: 7040 Loss: 0.0413776111131353\n",
            "Batch: 7360 Loss: 0.061739317097371994\n",
            "Batch: 7680 Loss: 0.057158978376631385\n",
            "Batch: 8000 Loss: 0.03153041792606173\n",
            "Batch: 8320 Loss: 0.048261159364817764\n",
            "Batch: 8640 Loss: 0.05411274101271914\n",
            "Batch: 8960 Loss: 0.0626681907938703\n",
            "Batch: 9280 Loss: 0.040343484131160616\n",
            "Batch: 9600 Loss: 0.026695455306851117\n",
            "Batch: 9920 Loss: 0.06218633788403845\n",
            "Batch: 10240 Loss: 0.04334394949123719\n",
            "Batch: 10560 Loss: 0.03592589987145138\n",
            "Batch: 10880 Loss: 0.03635354704616499\n",
            "Epoch: 319\n",
            "Batch: 0 Loss: 0.048325876126003066\n",
            "Batch: 320 Loss: 0.04842760344419324\n",
            "Batch: 640 Loss: 0.03488726204135209\n",
            "Batch: 960 Loss: 0.04073167072314896\n",
            "Batch: 1280 Loss: 0.04134641017533172\n",
            "Batch: 1600 Loss: 0.04381842238264234\n",
            "Batch: 1920 Loss: 0.0318389510709157\n",
            "Batch: 2240 Loss: 0.04424450148341392\n",
            "Batch: 2560 Loss: 0.0507438329792407\n",
            "Batch: 2880 Loss: 0.031625416182334845\n",
            "Batch: 3200 Loss: 0.040376359175435896\n",
            "Batch: 3520 Loss: 0.03409867809042197\n",
            "Batch: 3840 Loss: 0.0475257758148261\n",
            "Batch: 4160 Loss: 0.05747594097950129\n",
            "Batch: 4480 Loss: 0.06594009422343415\n",
            "Batch: 4800 Loss: 0.04844544057388178\n",
            "Batch: 5120 Loss: 0.042518102334925555\n",
            "Batch: 5440 Loss: 0.04334763792400082\n",
            "Batch: 5760 Loss: 0.056952030624467506\n",
            "Batch: 6080 Loss: 0.04147210704013653\n",
            "Batch: 6400 Loss: 0.05490314810425668\n",
            "Batch: 6720 Loss: 0.03137217184388502\n",
            "Batch: 7040 Loss: 0.06970127769515129\n",
            "Batch: 7360 Loss: 0.06926603972454687\n",
            "Batch: 7680 Loss: 0.04246523485860323\n",
            "Batch: 8000 Loss: 0.056753949103521696\n",
            "Batch: 8320 Loss: 0.05917857432336932\n",
            "Batch: 8640 Loss: 0.040505726923640004\n",
            "Batch: 8960 Loss: 0.05785493887616719\n",
            "Batch: 9280 Loss: 0.04993493886501611\n",
            "Batch: 9600 Loss: 0.051065015859454846\n",
            "Batch: 9920 Loss: 0.05581978458533378\n",
            "Batch: 10240 Loss: 0.05586537170157028\n",
            "Batch: 10560 Loss: 0.06919978544524986\n",
            "Batch: 10880 Loss: 0.07032806040262064\n",
            "Epoch: 320\n",
            "Batch: 0 Loss: 0.08124099024536276\n",
            "Batch: 320 Loss: 0.034555700289927986\n",
            "Batch: 640 Loss: 0.06598942549825304\n",
            "Batch: 960 Loss: 0.05065893254151585\n",
            "Batch: 1280 Loss: 0.039209303743699894\n",
            "Batch: 1600 Loss: 0.0436839591839959\n",
            "Batch: 1920 Loss: 0.07585555988858207\n",
            "Batch: 2240 Loss: 0.06459792484690273\n",
            "Batch: 2560 Loss: 0.048135466642422596\n",
            "Batch: 2880 Loss: 0.04620887545212264\n",
            "Batch: 3200 Loss: 0.04009934555628709\n",
            "Batch: 3520 Loss: 0.0572534307956385\n",
            "Batch: 3840 Loss: 0.03593851143538317\n",
            "Batch: 4160 Loss: 0.0467620940210132\n",
            "Batch: 4480 Loss: 0.054359289616851894\n",
            "Batch: 4800 Loss: 0.0637586979078109\n",
            "Batch: 5120 Loss: 0.06689752429447704\n",
            "Batch: 5440 Loss: 0.042819629571675184\n",
            "Batch: 5760 Loss: 0.05653979252204644\n",
            "Batch: 6080 Loss: 0.050079336618425487\n",
            "Batch: 6400 Loss: 0.056019770848953314\n",
            "Batch: 6720 Loss: 0.040971901068091475\n",
            "Batch: 7040 Loss: 0.04754157428303035\n",
            "Batch: 7360 Loss: 0.0727347939563424\n",
            "Batch: 7680 Loss: 0.05064741933962707\n",
            "Batch: 8000 Loss: 0.058422420395531166\n",
            "Batch: 8320 Loss: 0.03481963215989067\n",
            "Batch: 8640 Loss: 0.054368971914322516\n",
            "Batch: 8960 Loss: 0.029160253963729826\n",
            "Batch: 9280 Loss: 0.06634843685282188\n",
            "Batch: 9600 Loss: 0.047284912011735\n",
            "Batch: 9920 Loss: 0.05253292499067906\n",
            "Batch: 10240 Loss: 0.04015197135780972\n",
            "Batch: 10560 Loss: 0.041707271505651414\n",
            "Batch: 10880 Loss: 0.06448492327161637\n",
            "Epoch: 321\n",
            "Batch: 0 Loss: 0.06216766671801008\n",
            "Batch: 320 Loss: 0.05677581495117566\n",
            "Batch: 640 Loss: 0.04261512538329374\n",
            "Batch: 960 Loss: 0.058796472802176536\n",
            "Batch: 1280 Loss: 0.037270065265009235\n",
            "Batch: 1600 Loss: 0.04910458731694558\n",
            "Batch: 1920 Loss: 0.039722618135177676\n",
            "Batch: 2240 Loss: 0.05506060024825151\n",
            "Batch: 2560 Loss: 0.07249532689183798\n",
            "Batch: 2880 Loss: 0.037367399692701996\n",
            "Batch: 3200 Loss: 0.03872090974138273\n",
            "Batch: 3520 Loss: 0.05991062745116613\n",
            "Batch: 3840 Loss: 0.037953466614437735\n",
            "Batch: 4160 Loss: 0.04971727179196717\n",
            "Batch: 4480 Loss: 0.04903383526655665\n",
            "Batch: 4800 Loss: 0.05739207150793641\n",
            "Batch: 5120 Loss: 0.04391101350462401\n",
            "Batch: 5440 Loss: 0.04592674158056729\n",
            "Batch: 5760 Loss: 0.03644103776912444\n",
            "Batch: 6080 Loss: 0.05588667334562827\n",
            "Batch: 6400 Loss: 0.04617772936931953\n",
            "Batch: 6720 Loss: 0.036299182185352385\n",
            "Batch: 7040 Loss: 0.07308596027978168\n",
            "Batch: 7360 Loss: 0.05289324497837555\n",
            "Batch: 7680 Loss: 0.0742439252436898\n",
            "Batch: 8000 Loss: 0.054105227324174804\n",
            "Batch: 8320 Loss: 0.027981325165647265\n",
            "Batch: 8640 Loss: 0.05675508886231737\n",
            "Batch: 8960 Loss: 0.06361596238451576\n",
            "Batch: 9280 Loss: 0.047112336037126466\n",
            "Batch: 9600 Loss: 0.05600315218675876\n",
            "Batch: 9920 Loss: 0.04858257269594017\n",
            "Batch: 10240 Loss: 0.049885382964847895\n",
            "Batch: 10560 Loss: 0.041448041773194236\n",
            "Batch: 10880 Loss: 0.04057179958853151\n",
            "Epoch: 322\n",
            "Batch: 0 Loss: 0.05492901058413828\n",
            "Batch: 320 Loss: 0.0367326817280913\n",
            "Batch: 640 Loss: 0.05606011276224572\n",
            "Batch: 960 Loss: 0.05626621409735555\n",
            "Batch: 1280 Loss: 0.06617361479883133\n",
            "Batch: 1600 Loss: 0.05176732244908918\n",
            "Batch: 1920 Loss: 0.0698145527041364\n",
            "Batch: 2240 Loss: 0.04337772238791536\n",
            "Batch: 2560 Loss: 0.08469507317656169\n",
            "Batch: 2880 Loss: 0.0492903889833248\n",
            "Batch: 3200 Loss: 0.06211076744514771\n",
            "Batch: 3520 Loss: 0.04499530308413703\n",
            "Batch: 3840 Loss: 0.039184539827984866\n",
            "Batch: 4160 Loss: 0.03544658112595665\n",
            "Batch: 4480 Loss: 0.04739418629226088\n",
            "Batch: 4800 Loss: 0.053448517670218604\n",
            "Batch: 5120 Loss: 0.030447317017986097\n",
            "Batch: 5440 Loss: 0.03800432524508624\n",
            "Batch: 5760 Loss: 0.03743745710416179\n",
            "Batch: 6080 Loss: 0.0402446863786795\n",
            "Batch: 6400 Loss: 0.039325093477963216\n",
            "Batch: 6720 Loss: 0.0694613196290066\n",
            "Batch: 7040 Loss: 0.04056821434831835\n",
            "Batch: 7360 Loss: 0.02370035642582598\n",
            "Batch: 7680 Loss: 0.03575091364360411\n",
            "Batch: 8000 Loss: 0.04970923474551271\n",
            "Batch: 8320 Loss: 0.02709437726307656\n",
            "Batch: 8640 Loss: 0.054352101399434166\n",
            "Batch: 8960 Loss: 0.06626505945327824\n",
            "Batch: 9280 Loss: 0.05117903420298493\n",
            "Batch: 9600 Loss: 0.04944817134963369\n",
            "Batch: 9920 Loss: 0.045434101777945025\n",
            "Batch: 10240 Loss: 0.054784995935318145\n",
            "Batch: 10560 Loss: 0.06574309993899383\n",
            "Batch: 10880 Loss: 0.03866619460802032\n",
            "Epoch: 323\n",
            "Batch: 0 Loss: 0.05041407106956986\n",
            "Batch: 320 Loss: 0.06543269365291354\n",
            "Batch: 640 Loss: 0.05904058989910484\n",
            "Batch: 960 Loss: 0.034435825990470174\n",
            "Batch: 1280 Loss: 0.03682928642690373\n",
            "Batch: 1600 Loss: 0.027575299625685615\n",
            "Batch: 1920 Loss: 0.03145574813851983\n",
            "Batch: 2240 Loss: 0.04289904438024835\n",
            "Batch: 2560 Loss: 0.06731458091642568\n",
            "Batch: 2880 Loss: 0.029679244511173482\n",
            "Batch: 3200 Loss: 0.03744699265532698\n",
            "Batch: 3520 Loss: 0.041255863210493966\n",
            "Batch: 3840 Loss: 0.05280223543838137\n",
            "Batch: 4160 Loss: 0.04467163031258402\n",
            "Batch: 4480 Loss: 0.04410781827162579\n",
            "Batch: 4800 Loss: 0.04755429539885876\n",
            "Batch: 5120 Loss: 0.03133107382861827\n",
            "Batch: 5440 Loss: 0.03160511122579844\n",
            "Batch: 5760 Loss: 0.0555729619288493\n",
            "Batch: 6080 Loss: 0.04581933793969901\n",
            "Batch: 6400 Loss: 0.03531277173141618\n",
            "Batch: 6720 Loss: 0.038169440306025405\n",
            "Batch: 7040 Loss: 0.04056776151836612\n",
            "Batch: 7360 Loss: 0.05599775218628563\n",
            "Batch: 7680 Loss: 0.0579060218267643\n",
            "Batch: 8000 Loss: 0.046594805279641334\n",
            "Batch: 8320 Loss: 0.03287614751850826\n",
            "Batch: 8640 Loss: 0.061213788086848714\n",
            "Batch: 8960 Loss: 0.03554891847897624\n",
            "Batch: 9280 Loss: 0.07438795369864197\n",
            "Batch: 9600 Loss: 0.062093714417101195\n",
            "Batch: 9920 Loss: 0.049767570378636834\n",
            "Batch: 10240 Loss: 0.04750364006277055\n",
            "Batch: 10560 Loss: 0.04471458325123056\n",
            "Batch: 10880 Loss: 0.05125541756216231\n",
            "Epoch: 324\n",
            "Batch: 0 Loss: 0.05723503879119972\n",
            "Batch: 320 Loss: 0.06134866220854125\n",
            "Batch: 640 Loss: 0.07036952401695167\n",
            "Batch: 960 Loss: 0.03714518609620641\n",
            "Batch: 1280 Loss: 0.03926740963701411\n",
            "Batch: 1600 Loss: 0.04476713392402605\n",
            "Batch: 1920 Loss: 0.04894631163942478\n",
            "Batch: 2240 Loss: 0.0461855241149303\n",
            "Batch: 2560 Loss: 0.04580470598148738\n",
            "Batch: 2880 Loss: 0.07469650854357016\n",
            "Batch: 3200 Loss: 0.03952314172845927\n",
            "Batch: 3520 Loss: 0.044055444901911496\n",
            "Batch: 3840 Loss: 0.05000805232947678\n",
            "Batch: 4160 Loss: 0.05028680761343199\n",
            "Batch: 4480 Loss: 0.04951008602043895\n",
            "Batch: 4800 Loss: 0.05323668027067049\n",
            "Batch: 5120 Loss: 0.04002863878647764\n",
            "Batch: 5440 Loss: 0.05476252035097735\n",
            "Batch: 5760 Loss: 0.04084556341645946\n",
            "Batch: 6080 Loss: 0.042982253876005766\n",
            "Batch: 6400 Loss: 0.05039402893028808\n",
            "Batch: 6720 Loss: 0.045771949354980544\n",
            "Batch: 7040 Loss: 0.03436982779716506\n",
            "Batch: 7360 Loss: 0.06566385614296644\n",
            "Batch: 7680 Loss: 0.03072197054817917\n",
            "Batch: 8000 Loss: 0.04276992999493019\n",
            "Batch: 8320 Loss: 0.04380429802449491\n",
            "Batch: 8640 Loss: 0.06652873405941187\n",
            "Batch: 8960 Loss: 0.048363819124231176\n",
            "Batch: 9280 Loss: 0.06892391554058347\n",
            "Batch: 9600 Loss: 0.0672691348423323\n",
            "Batch: 9920 Loss: 0.038042775772421634\n",
            "Batch: 10240 Loss: 0.05040679198063767\n",
            "Batch: 10560 Loss: 0.037063391856409\n",
            "Batch: 10880 Loss: 0.06885755517718009\n",
            "Epoch: 325\n",
            "Batch: 0 Loss: 0.05240644542240343\n",
            "Batch: 320 Loss: 0.0367521329420318\n",
            "Batch: 640 Loss: 0.050218092480585064\n",
            "Batch: 960 Loss: 0.05851800212752625\n",
            "Batch: 1280 Loss: 0.05841739924661959\n",
            "Batch: 1600 Loss: 0.0625311646288979\n",
            "Batch: 1920 Loss: 0.06278479617992569\n",
            "Batch: 2240 Loss: 0.06242082738608411\n",
            "Batch: 2560 Loss: 0.0708076148564419\n",
            "Batch: 2880 Loss: 0.05635305699199495\n",
            "Batch: 3200 Loss: 0.0428260834615669\n",
            "Batch: 3520 Loss: 0.051994951334737326\n",
            "Batch: 3840 Loss: 0.05411313644326053\n",
            "Batch: 4160 Loss: 0.05463461660036147\n",
            "Batch: 4480 Loss: 0.08981977849818933\n",
            "Batch: 4800 Loss: 0.05765659776739345\n",
            "Batch: 5120 Loss: 0.04684203304857992\n",
            "Batch: 5440 Loss: 0.03854362623948151\n",
            "Batch: 5760 Loss: 0.041784866818878594\n",
            "Batch: 6080 Loss: 0.04036870321469253\n",
            "Batch: 6400 Loss: 0.03664410686821738\n",
            "Batch: 6720 Loss: 0.051959384883043594\n",
            "Batch: 7040 Loss: 0.03300822297193423\n",
            "Batch: 7360 Loss: 0.02984241102588156\n",
            "Batch: 7680 Loss: 0.03957528143725847\n",
            "Batch: 8000 Loss: 0.04774716199462802\n",
            "Batch: 8320 Loss: 0.05129455875550997\n",
            "Batch: 8640 Loss: 0.05045587274856646\n",
            "Batch: 8960 Loss: 0.03542763878510718\n",
            "Batch: 9280 Loss: 0.034326128456728694\n",
            "Batch: 9600 Loss: 0.06385440528057235\n",
            "Batch: 9920 Loss: 0.0486307659791937\n",
            "Batch: 10240 Loss: 0.03389334960434299\n",
            "Batch: 10560 Loss: 0.029106009371663185\n",
            "Batch: 10880 Loss: 0.030977459454602327\n",
            "Epoch: 326\n",
            "Batch: 0 Loss: 0.053385826509915676\n",
            "Batch: 320 Loss: 0.029924702921447645\n",
            "Batch: 640 Loss: 0.05732915650959105\n",
            "Batch: 960 Loss: 0.04886501120753526\n",
            "Batch: 1280 Loss: 0.05721106653504235\n",
            "Batch: 1600 Loss: 0.031073521185268356\n",
            "Batch: 1920 Loss: 0.04555416184315003\n",
            "Batch: 2240 Loss: 0.0564689472610427\n",
            "Batch: 2560 Loss: 0.0656010174658321\n",
            "Batch: 2880 Loss: 0.042325815314796636\n",
            "Batch: 3200 Loss: 0.07875523771290399\n",
            "Batch: 3520 Loss: 0.0682812012836069\n",
            "Batch: 3840 Loss: 0.06558916075171378\n",
            "Batch: 4160 Loss: 0.04093248397693416\n",
            "Batch: 4480 Loss: 0.041545051274773225\n",
            "Batch: 4800 Loss: 0.0512134939466309\n",
            "Batch: 5120 Loss: 0.04966356944935877\n",
            "Batch: 5440 Loss: 0.05438074824594873\n",
            "Batch: 5760 Loss: 0.05032259271707129\n",
            "Batch: 6080 Loss: 0.06101433764097065\n",
            "Batch: 6400 Loss: 0.050199464576223755\n",
            "Batch: 6720 Loss: 0.036414205463990504\n",
            "Batch: 7040 Loss: 0.05095249909755783\n",
            "Batch: 7360 Loss: 0.04386215077815538\n",
            "Batch: 7680 Loss: 0.048172245244248596\n",
            "Batch: 8000 Loss: 0.054138930946741715\n",
            "Batch: 8320 Loss: 0.03824531499982586\n",
            "Batch: 8640 Loss: 0.039422623264547194\n",
            "Batch: 8960 Loss: 0.042500683623877634\n",
            "Batch: 9280 Loss: 0.037745381694727694\n",
            "Batch: 9600 Loss: 0.057352186001419156\n",
            "Batch: 9920 Loss: 0.038871207667569824\n",
            "Batch: 10240 Loss: 0.0485430656569244\n",
            "Batch: 10560 Loss: 0.07532714826907244\n",
            "Batch: 10880 Loss: 0.050691771780983365\n",
            "Epoch: 327\n",
            "Batch: 0 Loss: 0.04525286534844589\n",
            "Batch: 320 Loss: 0.03149560256054942\n",
            "Batch: 640 Loss: 0.04489564247238731\n",
            "Batch: 960 Loss: 0.04309943788496816\n",
            "Batch: 1280 Loss: 0.07259020559803549\n",
            "Batch: 1600 Loss: 0.035183711436937846\n",
            "Batch: 1920 Loss: 0.06276772289919183\n",
            "Batch: 2240 Loss: 0.08135828317186129\n",
            "Batch: 2560 Loss: 0.06114453431239681\n",
            "Batch: 2880 Loss: 0.04877473628885227\n",
            "Batch: 3200 Loss: 0.060504598641641906\n",
            "Batch: 3520 Loss: 0.032202834633548845\n",
            "Batch: 3840 Loss: 0.032500573006222166\n",
            "Batch: 4160 Loss: 0.04009353490788733\n",
            "Batch: 4480 Loss: 0.05758363000724697\n",
            "Batch: 4800 Loss: 0.03907337236563524\n",
            "Batch: 5120 Loss: 0.03388887617220554\n",
            "Batch: 5440 Loss: 0.039386754386954534\n",
            "Batch: 5760 Loss: 0.04977649509121789\n",
            "Batch: 6080 Loss: 0.057870216272961954\n",
            "Batch: 6400 Loss: 0.06688589521954066\n",
            "Batch: 6720 Loss: 0.03172252678942523\n",
            "Batch: 7040 Loss: 0.033301223790142456\n",
            "Batch: 7360 Loss: 0.04084136732852618\n",
            "Batch: 7680 Loss: 0.06473165706085912\n",
            "Batch: 8000 Loss: 0.06284845976769786\n",
            "Batch: 8320 Loss: 0.05624332382892206\n",
            "Batch: 8640 Loss: 0.06463010323499706\n",
            "Batch: 8960 Loss: 0.04570650735748063\n",
            "Batch: 9280 Loss: 0.04686419978745497\n",
            "Batch: 9600 Loss: 0.03327512895322611\n",
            "Batch: 9920 Loss: 0.04950801960419079\n",
            "Batch: 10240 Loss: 0.0699575246703994\n",
            "Batch: 10560 Loss: 0.06935408660071038\n",
            "Batch: 10880 Loss: 0.0624355968274996\n",
            "Epoch: 328\n",
            "Batch: 0 Loss: 0.04018478548900323\n",
            "Batch: 320 Loss: 0.034063243060796895\n",
            "Batch: 640 Loss: 0.046482907899854285\n",
            "Batch: 960 Loss: 0.04125500505006174\n",
            "Batch: 1280 Loss: 0.033212637226446066\n",
            "Batch: 1600 Loss: 0.05908802621459883\n",
            "Batch: 1920 Loss: 0.060765857799749357\n",
            "Batch: 2240 Loss: 0.05113348547251584\n",
            "Batch: 2560 Loss: 0.052470994399271484\n",
            "Batch: 2880 Loss: 0.05309345590091651\n",
            "Batch: 3200 Loss: 0.053230891080148064\n",
            "Batch: 3520 Loss: 0.047772997232679984\n",
            "Batch: 3840 Loss: 0.0417917100855558\n",
            "Batch: 4160 Loss: 0.035872078995506866\n",
            "Batch: 4480 Loss: 0.05218593927221393\n",
            "Batch: 4800 Loss: 0.061857711256829456\n",
            "Batch: 5120 Loss: 0.0476364407510311\n",
            "Batch: 5440 Loss: 0.06354418883166407\n",
            "Batch: 5760 Loss: 0.05586185716929937\n",
            "Batch: 6080 Loss: 0.024841489847026145\n",
            "Batch: 6400 Loss: 0.062273201026982306\n",
            "Batch: 6720 Loss: 0.0447893007915639\n",
            "Batch: 7040 Loss: 0.0495756213362467\n",
            "Batch: 7360 Loss: 0.03898142420083836\n",
            "Batch: 7680 Loss: 0.0450193006181999\n",
            "Batch: 8000 Loss: 0.028168160828568253\n",
            "Batch: 8320 Loss: 0.05569914597277967\n",
            "Batch: 8640 Loss: 0.046904207229899166\n",
            "Batch: 8960 Loss: 0.046358590404004384\n",
            "Batch: 9280 Loss: 0.04276271090266201\n",
            "Batch: 9600 Loss: 0.03901617626759832\n",
            "Batch: 9920 Loss: 0.055636968698252735\n",
            "Batch: 10240 Loss: 0.04496701051842083\n",
            "Batch: 10560 Loss: 0.04662455539000075\n",
            "Batch: 10880 Loss: 0.046226488460453484\n",
            "Epoch: 329\n",
            "Batch: 0 Loss: 0.05734295947121968\n",
            "Batch: 320 Loss: 0.0535530527655568\n",
            "Batch: 640 Loss: 0.05368030321055033\n",
            "Batch: 960 Loss: 0.03692192979936364\n",
            "Batch: 1280 Loss: 0.05462093998920568\n",
            "Batch: 1600 Loss: 0.05288474905753038\n",
            "Batch: 1920 Loss: 0.04695375571617962\n",
            "Batch: 2240 Loss: 0.07963892134705801\n",
            "Batch: 2560 Loss: 0.05114586110606929\n",
            "Batch: 2880 Loss: 0.06958634747131141\n",
            "Batch: 3200 Loss: 0.0599592721284658\n",
            "Batch: 3520 Loss: 0.0790855127608897\n",
            "Batch: 3840 Loss: 0.09487021825571014\n",
            "Batch: 4160 Loss: 0.043798516046172756\n",
            "Batch: 4480 Loss: 0.02869499073712295\n",
            "Batch: 4800 Loss: 0.05645348615154651\n",
            "Batch: 5120 Loss: 0.05366835545638665\n",
            "Batch: 5440 Loss: 0.057522544934943874\n",
            "Batch: 5760 Loss: 0.057577030354336964\n",
            "Batch: 6080 Loss: 0.03945725997907519\n",
            "Batch: 6400 Loss: 0.04879995514747498\n",
            "Batch: 6720 Loss: 0.04820860260068206\n",
            "Batch: 7040 Loss: 0.05885901440252335\n",
            "Batch: 7360 Loss: 0.06107909372039794\n",
            "Batch: 7680 Loss: 0.025395035235604206\n",
            "Batch: 8000 Loss: 0.04539622828820769\n",
            "Batch: 8320 Loss: 0.05845948788726355\n",
            "Batch: 8640 Loss: 0.07284385259072972\n",
            "Batch: 8960 Loss: 0.07084128807632147\n",
            "Batch: 9280 Loss: 0.03150673732072727\n",
            "Batch: 9600 Loss: 0.04496716869706066\n",
            "Batch: 9920 Loss: 0.0408262208562907\n",
            "Batch: 10240 Loss: 0.04245068797847508\n",
            "Batch: 10560 Loss: 0.030192824878907003\n",
            "Batch: 10880 Loss: 0.02749593332299434\n",
            "Epoch: 330\n",
            "Batch: 0 Loss: 0.05021414054298033\n",
            "Batch: 320 Loss: 0.04793096801053396\n",
            "Batch: 640 Loss: 0.04909073901759422\n",
            "Batch: 960 Loss: 0.047542005510716655\n",
            "Batch: 1280 Loss: 0.0454465751544724\n",
            "Batch: 1600 Loss: 0.05639661295447577\n",
            "Batch: 1920 Loss: 0.035998569924907936\n",
            "Batch: 2240 Loss: 0.05889220354956492\n",
            "Batch: 2560 Loss: 0.048471683282802595\n",
            "Batch: 2880 Loss: 0.04948085541767106\n",
            "Batch: 3200 Loss: 0.026449025779753065\n",
            "Batch: 3520 Loss: 0.0619279949965888\n",
            "Batch: 3840 Loss: 0.05139517694954542\n",
            "Batch: 4160 Loss: 0.06078120130275869\n",
            "Batch: 4480 Loss: 0.044893946570868086\n",
            "Batch: 4800 Loss: 0.036367668329336404\n",
            "Batch: 5120 Loss: 0.04745782459037968\n",
            "Batch: 5440 Loss: 0.06515961178265842\n",
            "Batch: 5760 Loss: 0.04680418163710398\n",
            "Batch: 6080 Loss: 0.03553461973633081\n",
            "Batch: 6400 Loss: 0.04077212330456498\n",
            "Batch: 6720 Loss: 0.04214400612433277\n",
            "Batch: 7040 Loss: 0.04178728731691849\n",
            "Batch: 7360 Loss: 0.03513434977618213\n",
            "Batch: 7680 Loss: 0.05395534898309028\n",
            "Batch: 8000 Loss: 0.05103811702160863\n",
            "Batch: 8320 Loss: 0.0462517444154711\n",
            "Batch: 8640 Loss: 0.036242224312964505\n",
            "Batch: 8960 Loss: 0.04550443887429157\n",
            "Batch: 9280 Loss: 0.03964524257876715\n",
            "Batch: 9600 Loss: 0.03419962751522811\n",
            "Batch: 9920 Loss: 0.05633446686915178\n",
            "Batch: 10240 Loss: 0.06860342762256157\n",
            "Batch: 10560 Loss: 0.05181581723613337\n",
            "Batch: 10880 Loss: 0.043307495624104936\n",
            "Epoch: 331\n",
            "Batch: 0 Loss: 0.03489878443032372\n",
            "Batch: 320 Loss: 0.05837953992834058\n",
            "Batch: 640 Loss: 0.048907451235191836\n",
            "Batch: 960 Loss: 0.027312537273441313\n",
            "Batch: 1280 Loss: 0.05340701408021351\n",
            "Batch: 1600 Loss: 0.054370738057444974\n",
            "Batch: 1920 Loss: 0.04722158091671106\n",
            "Batch: 2240 Loss: 0.06426522421625795\n",
            "Batch: 2560 Loss: 0.029069364211364374\n",
            "Batch: 2880 Loss: 0.041902289372430915\n",
            "Batch: 3200 Loss: 0.06214724232508466\n",
            "Batch: 3520 Loss: 0.0472227419650386\n",
            "Batch: 3840 Loss: 0.04164849873848414\n",
            "Batch: 4160 Loss: 0.03359839431130084\n",
            "Batch: 4480 Loss: 0.045163282669159865\n",
            "Batch: 4800 Loss: 0.03954779508671526\n",
            "Batch: 5120 Loss: 0.033594847313493055\n",
            "Batch: 5440 Loss: 0.03185083179711117\n",
            "Batch: 5760 Loss: 0.058229186427429296\n",
            "Batch: 6080 Loss: 0.05545180915879839\n",
            "Batch: 6400 Loss: 0.032114246358124425\n",
            "Batch: 6720 Loss: 0.04924259827443155\n",
            "Batch: 7040 Loss: 0.03146744245392172\n",
            "Batch: 7360 Loss: 0.04286626851219437\n",
            "Batch: 7680 Loss: 0.03960778205466171\n",
            "Batch: 8000 Loss: 0.03501090813873469\n",
            "Batch: 8320 Loss: 0.032308293053116596\n",
            "Batch: 8640 Loss: 0.03949289094687181\n",
            "Batch: 8960 Loss: 0.0530288363888215\n",
            "Batch: 9280 Loss: 0.05843825696281197\n",
            "Batch: 9600 Loss: 0.060150921905915845\n",
            "Batch: 9920 Loss: 0.04868514080886109\n",
            "Batch: 10240 Loss: 0.033314711026394314\n",
            "Batch: 10560 Loss: 0.02774897842265995\n",
            "Batch: 10880 Loss: 0.0725989765761421\n",
            "Epoch: 332\n",
            "Batch: 0 Loss: 0.054511894444229624\n",
            "Batch: 320 Loss: 0.04518653321438072\n",
            "Batch: 640 Loss: 0.040584617683237824\n",
            "Batch: 960 Loss: 0.03950896784051983\n",
            "Batch: 1280 Loss: 0.03714212185329795\n",
            "Batch: 1600 Loss: 0.06537291915702673\n",
            "Batch: 1920 Loss: 0.050662338599444086\n",
            "Batch: 2240 Loss: 0.038336568167881564\n",
            "Batch: 2560 Loss: 0.053897006252560414\n",
            "Batch: 2880 Loss: 0.0542841852799627\n",
            "Batch: 3200 Loss: 0.042607739071700304\n",
            "Batch: 3520 Loss: 0.04392713931447794\n",
            "Batch: 3840 Loss: 0.04880199945933664\n",
            "Batch: 4160 Loss: 0.06841006818796032\n",
            "Batch: 4480 Loss: 0.037484001892647736\n",
            "Batch: 4800 Loss: 0.0310852136833452\n",
            "Batch: 5120 Loss: 0.028168425771094074\n",
            "Batch: 5440 Loss: 0.05431155778916376\n",
            "Batch: 5760 Loss: 0.04829300596173933\n",
            "Batch: 6080 Loss: 0.03995377801141928\n",
            "Batch: 6400 Loss: 0.053348351753169584\n",
            "Batch: 6720 Loss: 0.058921036675582995\n",
            "Batch: 7040 Loss: 0.0661303590979307\n",
            "Batch: 7360 Loss: 0.0683833842221976\n",
            "Batch: 7680 Loss: 0.02988657538261861\n",
            "Batch: 8000 Loss: 0.04754326707286403\n",
            "Batch: 8320 Loss: 0.04380947548650737\n",
            "Batch: 8640 Loss: 0.03432349784891831\n",
            "Batch: 8960 Loss: 0.05812855067465882\n",
            "Batch: 9280 Loss: 0.048310037097397375\n",
            "Batch: 9600 Loss: 0.047351381516375066\n",
            "Batch: 9920 Loss: 0.041920367169740146\n",
            "Batch: 10240 Loss: 0.057039708423968424\n",
            "Batch: 10560 Loss: 0.05624642297178881\n",
            "Batch: 10880 Loss: 0.03682643127800048\n",
            "Epoch: 333\n",
            "Batch: 0 Loss: 0.04889351711407165\n",
            "Batch: 320 Loss: 0.05585410919272883\n",
            "Batch: 640 Loss: 0.04336360077195014\n",
            "Batch: 960 Loss: 0.039085919501050954\n",
            "Batch: 1280 Loss: 0.05409928665625603\n",
            "Batch: 1600 Loss: 0.03190352747208321\n",
            "Batch: 1920 Loss: 0.06765489309730872\n",
            "Batch: 2240 Loss: 0.054640628733906675\n",
            "Batch: 2560 Loss: 0.06348282792945299\n",
            "Batch: 2880 Loss: 0.03406579802431791\n",
            "Batch: 3200 Loss: 0.04841159792699143\n",
            "Batch: 3520 Loss: 0.04922450725500849\n",
            "Batch: 3840 Loss: 0.042058160062951544\n",
            "Batch: 4160 Loss: 0.04740154528099697\n",
            "Batch: 4480 Loss: 0.039825643342841284\n",
            "Batch: 4800 Loss: 0.0383172796640564\n",
            "Batch: 5120 Loss: 0.06901582953281835\n",
            "Batch: 5440 Loss: 0.04224456327055471\n",
            "Batch: 5760 Loss: 0.037123641351894485\n",
            "Batch: 6080 Loss: 0.05109248366810627\n",
            "Batch: 6400 Loss: 0.04425018219915306\n",
            "Batch: 6720 Loss: 0.05101190832874171\n",
            "Batch: 7040 Loss: 0.061434513132092784\n",
            "Batch: 7360 Loss: 0.04520883446764087\n",
            "Batch: 7680 Loss: 0.051215525778704835\n",
            "Batch: 8000 Loss: 0.06575555192890152\n",
            "Batch: 8320 Loss: 0.0469649408239817\n",
            "Batch: 8640 Loss: 0.03292519929067477\n",
            "Batch: 8960 Loss: 0.052076450498364936\n",
            "Batch: 9280 Loss: 0.059183227876825784\n",
            "Batch: 9600 Loss: 0.06140998525311112\n",
            "Batch: 9920 Loss: 0.05256620358186694\n",
            "Batch: 10240 Loss: 0.04834222003165831\n",
            "Batch: 10560 Loss: 0.052261340697607184\n",
            "Batch: 10880 Loss: 0.04189849282642229\n",
            "Epoch: 334\n",
            "Batch: 0 Loss: 0.036016926343471305\n",
            "Batch: 320 Loss: 0.051896631052488484\n",
            "Batch: 640 Loss: 0.031310762135666005\n",
            "Batch: 960 Loss: 0.04755930414876792\n",
            "Batch: 1280 Loss: 0.06957310080370682\n",
            "Batch: 1600 Loss: 0.0486824863788814\n",
            "Batch: 1920 Loss: 0.04258518986479941\n",
            "Batch: 2240 Loss: 0.04617825032825289\n",
            "Batch: 2560 Loss: 0.06006829882169137\n",
            "Batch: 2880 Loss: 0.03327310376043894\n",
            "Batch: 3200 Loss: 0.047955189164118256\n",
            "Batch: 3520 Loss: 0.06360026892306742\n",
            "Batch: 3840 Loss: 0.03125463982565994\n",
            "Batch: 4160 Loss: 0.04540521268450806\n",
            "Batch: 4480 Loss: 0.056207697810773394\n",
            "Batch: 4800 Loss: 0.05448702593249991\n",
            "Batch: 5120 Loss: 0.04199360103412554\n",
            "Batch: 5440 Loss: 0.05454829058786944\n",
            "Batch: 5760 Loss: 0.044915943013471585\n",
            "Batch: 6080 Loss: 0.055008651793632644\n",
            "Batch: 6400 Loss: 0.029095844901029417\n",
            "Batch: 6720 Loss: 0.03256106544640463\n",
            "Batch: 7040 Loss: 0.03541521277311887\n",
            "Batch: 7360 Loss: 0.034334774744093514\n",
            "Batch: 7680 Loss: 0.04526486734558606\n",
            "Batch: 8000 Loss: 0.06193650918589193\n",
            "Batch: 8320 Loss: 0.061638883717082336\n",
            "Batch: 8640 Loss: 0.05045320790504794\n",
            "Batch: 8960 Loss: 0.0418518073367038\n",
            "Batch: 9280 Loss: 0.05190830822342707\n",
            "Batch: 9600 Loss: 0.08710939216628434\n",
            "Batch: 9920 Loss: 0.05993146078181669\n",
            "Batch: 10240 Loss: 0.05686509401383375\n",
            "Batch: 10560 Loss: 0.03676131220335091\n",
            "Batch: 10880 Loss: 0.08632321175020904\n",
            "Epoch: 335\n",
            "Batch: 0 Loss: 0.05507933402281762\n",
            "Batch: 320 Loss: 0.05374726953184881\n",
            "Batch: 640 Loss: 0.05306476173712488\n",
            "Batch: 960 Loss: 0.04256715937468594\n",
            "Batch: 1280 Loss: 0.0401495181392623\n",
            "Batch: 1600 Loss: 0.05811852642695568\n",
            "Batch: 1920 Loss: 0.048779825228231134\n",
            "Batch: 2240 Loss: 0.04955597470168916\n",
            "Batch: 2560 Loss: 0.06201663100689613\n",
            "Batch: 2880 Loss: 0.054148267687311014\n",
            "Batch: 3200 Loss: 0.07848003802722738\n",
            "Batch: 3520 Loss: 0.06873489432749712\n",
            "Batch: 3840 Loss: 0.06113861788976251\n",
            "Batch: 4160 Loss: 0.029075771591301793\n",
            "Batch: 4480 Loss: 0.06014500381879883\n",
            "Batch: 4800 Loss: 0.08520674962982343\n",
            "Batch: 5120 Loss: 0.03285004881923232\n",
            "Batch: 5440 Loss: 0.05786543344173758\n",
            "Batch: 5760 Loss: 0.05158338315779133\n",
            "Batch: 6080 Loss: 0.050095329381417315\n",
            "Batch: 6400 Loss: 0.06606182787021962\n",
            "Batch: 6720 Loss: 0.0516194790333535\n",
            "Batch: 7040 Loss: 0.06243742051454595\n",
            "Batch: 7360 Loss: 0.05271312931299218\n",
            "Batch: 7680 Loss: 0.04904643793034965\n",
            "Batch: 8000 Loss: 0.05627480517537675\n",
            "Batch: 8320 Loss: 0.040203522854459876\n",
            "Batch: 8640 Loss: 0.05556737677298239\n",
            "Batch: 8960 Loss: 0.046966579563736954\n",
            "Batch: 9280 Loss: 0.049583773708202536\n",
            "Batch: 9600 Loss: 0.04514188702629078\n",
            "Batch: 9920 Loss: 0.06725351005994469\n",
            "Batch: 10240 Loss: 0.05208376401403932\n",
            "Batch: 10560 Loss: 0.05250298810986084\n",
            "Batch: 10880 Loss: 0.06670473265101094\n",
            "Epoch: 336\n",
            "Batch: 0 Loss: 0.0394142585912919\n",
            "Batch: 320 Loss: 0.05064518764577045\n",
            "Batch: 640 Loss: 0.04558752226723418\n",
            "Batch: 960 Loss: 0.05624003465886425\n",
            "Batch: 1280 Loss: 0.05233707084708044\n",
            "Batch: 1600 Loss: 0.06363765277894477\n",
            "Batch: 1920 Loss: 0.04077866812009842\n",
            "Batch: 2240 Loss: 0.040519535060100816\n",
            "Batch: 2560 Loss: 0.04257080270031219\n",
            "Batch: 2880 Loss: 0.06108422203599697\n",
            "Batch: 3200 Loss: 0.051934136973854746\n",
            "Batch: 3520 Loss: 0.06952273489773715\n",
            "Batch: 3840 Loss: 0.07172249219660061\n",
            "Batch: 4160 Loss: 0.07594133943983335\n",
            "Batch: 4480 Loss: 0.032646461970232146\n",
            "Batch: 4800 Loss: 0.03802441682937552\n",
            "Batch: 5120 Loss: 0.04606701259418335\n",
            "Batch: 5440 Loss: 0.06281514607916043\n",
            "Batch: 5760 Loss: 0.04651545563231053\n",
            "Batch: 6080 Loss: 0.033659445018648905\n",
            "Batch: 6400 Loss: 0.0628711495580366\n",
            "Batch: 6720 Loss: 0.04654596712071497\n",
            "Batch: 7040 Loss: 0.04219844591643521\n",
            "Batch: 7360 Loss: 0.037842385208809365\n",
            "Batch: 7680 Loss: 0.04224135514030372\n",
            "Batch: 8000 Loss: 0.030719327198368597\n",
            "Batch: 8320 Loss: 0.03809276511665489\n",
            "Batch: 8640 Loss: 0.05669605223767863\n",
            "Batch: 8960 Loss: 0.042566951109332604\n",
            "Batch: 9280 Loss: 0.03739511124490255\n",
            "Batch: 9600 Loss: 0.0567712625818244\n",
            "Batch: 9920 Loss: 0.042638942826032415\n",
            "Batch: 10240 Loss: 0.06073400428791145\n",
            "Batch: 10560 Loss: 0.04638379514875503\n",
            "Batch: 10880 Loss: 0.05312017795241404\n",
            "Epoch: 337\n",
            "Batch: 0 Loss: 0.061705906692259296\n",
            "Batch: 320 Loss: 0.06726472087501967\n",
            "Batch: 640 Loss: 0.0610436233684032\n",
            "Batch: 960 Loss: 0.05088605264981677\n",
            "Batch: 1280 Loss: 0.057391577853461766\n",
            "Batch: 1600 Loss: 0.05309549795215797\n",
            "Batch: 1920 Loss: 0.0446560891723066\n",
            "Batch: 2240 Loss: 0.05740147448035214\n",
            "Batch: 2560 Loss: 0.05657867510690333\n",
            "Batch: 2880 Loss: 0.06157009847201623\n",
            "Batch: 3200 Loss: 0.04969458517158015\n",
            "Batch: 3520 Loss: 0.05583072830206079\n",
            "Batch: 3840 Loss: 0.04335311173074617\n",
            "Batch: 4160 Loss: 0.07744797414959992\n",
            "Batch: 4480 Loss: 0.032473060166307154\n",
            "Batch: 4800 Loss: 0.03828126731482503\n",
            "Batch: 5120 Loss: 0.03038650702622119\n",
            "Batch: 5440 Loss: 0.05494895230737458\n",
            "Batch: 5760 Loss: 0.048012585464447345\n",
            "Batch: 6080 Loss: 0.04144111365879129\n",
            "Batch: 6400 Loss: 0.04389155287303474\n",
            "Batch: 6720 Loss: 0.05546267586325967\n",
            "Batch: 7040 Loss: 0.04419378415892548\n",
            "Batch: 7360 Loss: 0.035918594044251795\n",
            "Batch: 7680 Loss: 0.05777339219228217\n",
            "Batch: 8000 Loss: 0.0616057390083892\n",
            "Batch: 8320 Loss: 0.03866967426689418\n",
            "Batch: 8640 Loss: 0.05474459255662763\n",
            "Batch: 8960 Loss: 0.03570838486697297\n",
            "Batch: 9280 Loss: 0.047989409537265056\n",
            "Batch: 9600 Loss: 0.047154706409868165\n",
            "Batch: 9920 Loss: 0.059683360640403575\n",
            "Batch: 10240 Loss: 0.05393541475971308\n",
            "Batch: 10560 Loss: 0.03748260507030159\n",
            "Batch: 10880 Loss: 0.028819871257830098\n",
            "Epoch: 338\n",
            "Batch: 0 Loss: 0.04771421835913587\n",
            "Batch: 320 Loss: 0.02809651347991854\n",
            "Batch: 640 Loss: 0.03395793898693767\n",
            "Batch: 960 Loss: 0.05768110037178105\n",
            "Batch: 1280 Loss: 0.03773591760550357\n",
            "Batch: 1600 Loss: 0.07716829559317341\n",
            "Batch: 1920 Loss: 0.05548822406758134\n",
            "Batch: 2240 Loss: 0.04132256511033301\n",
            "Batch: 2560 Loss: 0.06208382360368095\n",
            "Batch: 2880 Loss: 0.0646564032540755\n",
            "Batch: 3200 Loss: 0.03936424423262165\n",
            "Batch: 3520 Loss: 0.05953487446805307\n",
            "Batch: 3840 Loss: 0.07148679774740817\n",
            "Batch: 4160 Loss: 0.04769155439843946\n",
            "Batch: 4480 Loss: 0.035082574909521935\n",
            "Batch: 4800 Loss: 0.04093191063193781\n",
            "Batch: 5120 Loss: 0.038008684547810395\n",
            "Batch: 5440 Loss: 0.04070047733483601\n",
            "Batch: 5760 Loss: 0.04155757291532056\n",
            "Batch: 6080 Loss: 0.05604889392501878\n",
            "Batch: 6400 Loss: 0.09140729862474746\n",
            "Batch: 6720 Loss: 0.08796394218704291\n",
            "Batch: 7040 Loss: 0.047232923754335424\n",
            "Batch: 7360 Loss: 0.035684828018566844\n",
            "Batch: 7680 Loss: 0.03972052470294424\n",
            "Batch: 8000 Loss: 0.035039462563705104\n",
            "Batch: 8320 Loss: 0.04255449910688601\n",
            "Batch: 8640 Loss: 0.04174693115982947\n",
            "Batch: 8960 Loss: 0.034115878890539984\n",
            "Batch: 9280 Loss: 0.05054906798897586\n",
            "Batch: 9600 Loss: 0.07299360448791926\n",
            "Batch: 9920 Loss: 0.027766055406674033\n",
            "Batch: 10240 Loss: 0.043288244213733645\n",
            "Batch: 10560 Loss: 0.052140532475995476\n",
            "Batch: 10880 Loss: 0.0676330424051536\n",
            "Epoch: 339\n",
            "Batch: 0 Loss: 0.04690676221541891\n",
            "Batch: 320 Loss: 0.04912126233235488\n",
            "Batch: 640 Loss: 0.047309463202810625\n",
            "Batch: 960 Loss: 0.05470260940044888\n",
            "Batch: 1280 Loss: 0.042827446064086505\n",
            "Batch: 1600 Loss: 0.072411520150115\n",
            "Batch: 1920 Loss: 0.07343625143729988\n",
            "Batch: 2240 Loss: 0.05214656695892711\n",
            "Batch: 2560 Loss: 0.06015003305432725\n",
            "Batch: 2880 Loss: 0.0628791020849847\n",
            "Batch: 3200 Loss: 0.04300447761576644\n",
            "Batch: 3520 Loss: 0.05568588289962215\n",
            "Batch: 3840 Loss: 0.059524763361591584\n",
            "Batch: 4160 Loss: 0.05378865226551229\n",
            "Batch: 4480 Loss: 0.0624532054453302\n",
            "Batch: 4800 Loss: 0.037072379774070935\n",
            "Batch: 5120 Loss: 0.06718121129295743\n",
            "Batch: 5440 Loss: 0.05403823789876131\n",
            "Batch: 5760 Loss: 0.04368545227452859\n",
            "Batch: 6080 Loss: 0.038755758974911965\n",
            "Batch: 6400 Loss: 0.056578422385008285\n",
            "Batch: 6720 Loss: 0.07308367618004666\n",
            "Batch: 7040 Loss: 0.05771195548107624\n",
            "Batch: 7360 Loss: 0.04636793702420046\n",
            "Batch: 7680 Loss: 0.04602589777361468\n",
            "Batch: 8000 Loss: 0.0352145589622778\n",
            "Batch: 8320 Loss: 0.05152169918690872\n",
            "Batch: 8640 Loss: 0.04253210063387313\n",
            "Batch: 8960 Loss: 0.05555451928769273\n",
            "Batch: 9280 Loss: 0.06222942469964575\n",
            "Batch: 9600 Loss: 0.04614218955134396\n",
            "Batch: 9920 Loss: 0.041706287748750445\n",
            "Batch: 10240 Loss: 0.06106470801408714\n",
            "Batch: 10560 Loss: 0.041468242624163626\n",
            "Batch: 10880 Loss: 0.052968064070364446\n",
            "Epoch: 340\n",
            "Batch: 0 Loss: 0.04837851779507033\n",
            "Batch: 320 Loss: 0.04021300389862928\n",
            "Batch: 640 Loss: 0.0500850514850571\n",
            "Batch: 960 Loss: 0.03276934298349716\n",
            "Batch: 1280 Loss: 0.06058584297982521\n",
            "Batch: 1600 Loss: 0.0507431693808175\n",
            "Batch: 1920 Loss: 0.03366283027360486\n",
            "Batch: 2240 Loss: 0.04229964407595149\n",
            "Batch: 2560 Loss: 0.03711383509278397\n",
            "Batch: 2880 Loss: 0.0468790699573753\n",
            "Batch: 3200 Loss: 0.043163637760175\n",
            "Batch: 3520 Loss: 0.031807874368984176\n",
            "Batch: 3840 Loss: 0.05025944939051583\n",
            "Batch: 4160 Loss: 0.03620050632237007\n",
            "Batch: 4480 Loss: 0.038527011757179395\n",
            "Batch: 4800 Loss: 0.0572869451289872\n",
            "Batch: 5120 Loss: 0.05963283991619501\n",
            "Batch: 5440 Loss: 0.04059819469142345\n",
            "Batch: 5760 Loss: 0.04858294030129122\n",
            "Batch: 6080 Loss: 0.039259376208103845\n",
            "Batch: 6400 Loss: 0.038105813896811266\n",
            "Batch: 6720 Loss: 0.03620513412704754\n",
            "Batch: 7040 Loss: 0.050863376484917434\n",
            "Batch: 7360 Loss: 0.061348545271410385\n",
            "Batch: 7680 Loss: 0.04304474950915904\n",
            "Batch: 8000 Loss: 0.04310808310465204\n",
            "Batch: 8320 Loss: 0.04336045461483029\n",
            "Batch: 8640 Loss: 0.05123563136693431\n",
            "Batch: 8960 Loss: 0.03371683581088132\n",
            "Batch: 9280 Loss: 0.05418404995024881\n",
            "Batch: 9600 Loss: 0.030886003992387757\n",
            "Batch: 9920 Loss: 0.05289171051654485\n",
            "Batch: 10240 Loss: 0.04388331039377394\n",
            "Batch: 10560 Loss: 0.0435914209600116\n",
            "Batch: 10880 Loss: 0.02825725993177384\n",
            "Epoch: 341\n",
            "Batch: 0 Loss: 0.03335497737041418\n",
            "Batch: 320 Loss: 0.055027094968260346\n",
            "Batch: 640 Loss: 0.042045619161795535\n",
            "Batch: 960 Loss: 0.06078157423035895\n",
            "Batch: 1280 Loss: 0.04931386172304764\n",
            "Batch: 1600 Loss: 0.04227055292469914\n",
            "Batch: 1920 Loss: 0.03955704476933308\n",
            "Batch: 2240 Loss: 0.05407829190012277\n",
            "Batch: 2560 Loss: 0.04255244134082866\n",
            "Batch: 2880 Loss: 0.047017934820873394\n",
            "Batch: 3200 Loss: 0.0528287054231391\n",
            "Batch: 3520 Loss: 0.06457813488277979\n",
            "Batch: 3840 Loss: 0.04652427790948431\n",
            "Batch: 4160 Loss: 0.06069294152509254\n",
            "Batch: 4480 Loss: 0.03876776264700915\n",
            "Batch: 4800 Loss: 0.0436439737815746\n",
            "Batch: 5120 Loss: 0.05810923755576876\n",
            "Batch: 5440 Loss: 0.04931105423390562\n",
            "Batch: 5760 Loss: 0.04835280780107533\n",
            "Batch: 6080 Loss: 0.05778941564032196\n",
            "Batch: 6400 Loss: 0.03267527283639747\n",
            "Batch: 6720 Loss: 0.05489748783324436\n",
            "Batch: 7040 Loss: 0.03972534826913985\n",
            "Batch: 7360 Loss: 0.03470319881551847\n",
            "Batch: 7680 Loss: 0.02841759125250759\n",
            "Batch: 8000 Loss: 0.06168597330183559\n",
            "Batch: 8320 Loss: 0.09413030102768606\n",
            "Batch: 8640 Loss: 0.0561305067112726\n",
            "Batch: 8960 Loss: 0.05537461403766548\n",
            "Batch: 9280 Loss: 0.03791769693082846\n",
            "Batch: 9600 Loss: 0.04318314887990399\n",
            "Batch: 9920 Loss: 0.04068914250728111\n",
            "Batch: 10240 Loss: 0.05712642503824333\n",
            "Batch: 10560 Loss: 0.05803758404163333\n",
            "Batch: 10880 Loss: 0.041990153458461174\n",
            "Epoch: 342\n",
            "Batch: 0 Loss: 0.03266374756871147\n",
            "Batch: 320 Loss: 0.03352605606171214\n",
            "Batch: 640 Loss: 0.05460862418722972\n",
            "Batch: 960 Loss: 0.03093458732193086\n",
            "Batch: 1280 Loss: 0.058788607285777485\n",
            "Batch: 1600 Loss: 0.07751820684155626\n",
            "Batch: 1920 Loss: 0.059777757471057895\n",
            "Batch: 2240 Loss: 0.0497634095864881\n",
            "Batch: 2560 Loss: 0.05000289113903928\n",
            "Batch: 2880 Loss: 0.04480877309516946\n",
            "Batch: 3200 Loss: 0.02904095124717476\n",
            "Batch: 3520 Loss: 0.056186563605133875\n",
            "Batch: 3840 Loss: 0.05348658201597046\n",
            "Batch: 4160 Loss: 0.04215585383696006\n",
            "Batch: 4480 Loss: 0.04806642294935613\n",
            "Batch: 4800 Loss: 0.03758443557411103\n",
            "Batch: 5120 Loss: 0.056156578663919446\n",
            "Batch: 5440 Loss: 0.051835041284484906\n",
            "Batch: 5760 Loss: 0.06309044264059302\n",
            "Batch: 6080 Loss: 0.047704573567201036\n",
            "Batch: 6400 Loss: 0.04495971529323563\n",
            "Batch: 6720 Loss: 0.05642853476760449\n",
            "Batch: 7040 Loss: 0.048828547875030624\n",
            "Batch: 7360 Loss: 0.032576619779452004\n",
            "Batch: 7680 Loss: 0.05935254603905232\n",
            "Batch: 8000 Loss: 0.043133146775830356\n",
            "Batch: 8320 Loss: 0.053080892050317255\n",
            "Batch: 8640 Loss: 0.03979243613657435\n",
            "Batch: 8960 Loss: 0.028730968185041374\n",
            "Batch: 9280 Loss: 0.050152557971247486\n",
            "Batch: 9600 Loss: 0.06726305266576299\n",
            "Batch: 9920 Loss: 0.05991264069633354\n",
            "Batch: 10240 Loss: 0.05088247688485601\n",
            "Batch: 10560 Loss: 0.040886469920885106\n",
            "Batch: 10880 Loss: 0.033742485066645914\n",
            "Epoch: 343\n",
            "Batch: 0 Loss: 0.047675685431007475\n",
            "Batch: 320 Loss: 0.03951797988869403\n",
            "Batch: 640 Loss: 0.039262099879989944\n",
            "Batch: 960 Loss: 0.055961790988426255\n",
            "Batch: 1280 Loss: 0.05732111727963137\n",
            "Batch: 1600 Loss: 0.04888156233841688\n",
            "Batch: 1920 Loss: 0.049566663904775814\n",
            "Batch: 2240 Loss: 0.05005286782714643\n",
            "Batch: 2560 Loss: 0.04188796084715799\n",
            "Batch: 2880 Loss: 0.057050297529521045\n",
            "Batch: 3200 Loss: 0.03906875400216789\n",
            "Batch: 3520 Loss: 0.05690708195955062\n",
            "Batch: 3840 Loss: 0.04159741314278269\n",
            "Batch: 4160 Loss: 0.029794644049130944\n",
            "Batch: 4480 Loss: 0.03297800153120148\n",
            "Batch: 4800 Loss: 0.061830507759890276\n",
            "Batch: 5120 Loss: 0.043320117532834795\n",
            "Batch: 5440 Loss: 0.028132755398016865\n",
            "Batch: 5760 Loss: 0.05886512204179624\n",
            "Batch: 6080 Loss: 0.07412007958837205\n",
            "Batch: 6400 Loss: 0.07211060620947961\n",
            "Batch: 6720 Loss: 0.06134607241569391\n",
            "Batch: 7040 Loss: 0.048296000882317974\n",
            "Batch: 7360 Loss: 0.043919732751436596\n",
            "Batch: 7680 Loss: 0.05319376198581541\n",
            "Batch: 8000 Loss: 0.06351335736421823\n",
            "Batch: 8320 Loss: 0.04788925014407377\n",
            "Batch: 8640 Loss: 0.0446755239573088\n",
            "Batch: 8960 Loss: 0.04446529494683104\n",
            "Batch: 9280 Loss: 0.06343755290230747\n",
            "Batch: 9600 Loss: 0.054573423625842\n",
            "Batch: 9920 Loss: 0.05098869604436947\n",
            "Batch: 10240 Loss: 0.0655805555245103\n",
            "Batch: 10560 Loss: 0.059548067419123135\n",
            "Batch: 10880 Loss: 0.02254865255859499\n",
            "Epoch: 344\n",
            "Batch: 0 Loss: 0.0477162828287896\n",
            "Batch: 320 Loss: 0.03437759013241764\n",
            "Batch: 640 Loss: 0.06245226552573715\n",
            "Batch: 960 Loss: 0.055864361727521725\n",
            "Batch: 1280 Loss: 0.03960860555680734\n",
            "Batch: 1600 Loss: 0.06597221173881311\n",
            "Batch: 1920 Loss: 0.031802621386471004\n",
            "Batch: 2240 Loss: 0.039195160555124606\n",
            "Batch: 2560 Loss: 0.05161806459408959\n",
            "Batch: 2880 Loss: 0.030748362728020534\n",
            "Batch: 3200 Loss: 0.0497720809266603\n",
            "Batch: 3520 Loss: 0.05740012334448797\n",
            "Batch: 3840 Loss: 0.06446693721295867\n",
            "Batch: 4160 Loss: 0.05056660673092232\n",
            "Batch: 4480 Loss: 0.05451537387437104\n",
            "Batch: 4800 Loss: 0.05386520427842463\n",
            "Batch: 5120 Loss: 0.042568308644113054\n",
            "Batch: 5440 Loss: 0.03783220179347331\n",
            "Batch: 5760 Loss: 0.034923535442352764\n",
            "Batch: 6080 Loss: 0.05886974727483138\n",
            "Batch: 6400 Loss: 0.03666679548071053\n",
            "Batch: 6720 Loss: 0.03680503319710565\n",
            "Batch: 7040 Loss: 0.043766656707405004\n",
            "Batch: 7360 Loss: 0.06262125153054397\n",
            "Batch: 7680 Loss: 0.05715165911141562\n",
            "Batch: 8000 Loss: 0.03147690284445563\n",
            "Batch: 8320 Loss: 0.03557506908366728\n",
            "Batch: 8640 Loss: 0.04449786915869863\n",
            "Batch: 8960 Loss: 0.045558334217328085\n",
            "Batch: 9280 Loss: 0.045210266316343554\n",
            "Batch: 9600 Loss: 0.03426892354670842\n",
            "Batch: 9920 Loss: 0.09774532862779699\n",
            "Batch: 10240 Loss: 0.04661666226683776\n",
            "Batch: 10560 Loss: 0.054058024352464894\n",
            "Batch: 10880 Loss: 0.04260390781763954\n",
            "Epoch: 345\n",
            "Batch: 0 Loss: 0.03386845298698675\n",
            "Batch: 320 Loss: 0.05588772479551801\n",
            "Batch: 640 Loss: 0.04921248957632037\n",
            "Batch: 960 Loss: 0.05496754198715352\n",
            "Batch: 1280 Loss: 0.06809672050493726\n",
            "Batch: 1600 Loss: 0.043739873227189516\n",
            "Batch: 1920 Loss: 0.03556722879063197\n",
            "Batch: 2240 Loss: 0.037306620337657505\n",
            "Batch: 2560 Loss: 0.04838245212075344\n",
            "Batch: 2880 Loss: 0.03989351061935763\n",
            "Batch: 3200 Loss: 0.040751271489881835\n",
            "Batch: 3520 Loss: 0.05335536956663195\n",
            "Batch: 3840 Loss: 0.054107840947367744\n",
            "Batch: 4160 Loss: 0.04158492885116444\n",
            "Batch: 4480 Loss: 0.036195868791222184\n",
            "Batch: 4800 Loss: 0.08211482504259204\n",
            "Batch: 5120 Loss: 0.07593945484508242\n",
            "Batch: 5440 Loss: 0.04088287966396713\n",
            "Batch: 5760 Loss: 0.03988114140365601\n",
            "Batch: 6080 Loss: 0.04552221616265881\n",
            "Batch: 6400 Loss: 0.058813605495916396\n",
            "Batch: 6720 Loss: 0.03750512058381417\n",
            "Batch: 7040 Loss: 0.05042123822516642\n",
            "Batch: 7360 Loss: 0.05207839610805493\n",
            "Batch: 7680 Loss: 0.05299428356193435\n",
            "Batch: 8000 Loss: 0.056644290357191936\n",
            "Batch: 8320 Loss: 0.06592526333470067\n",
            "Batch: 8640 Loss: 0.03826856791356705\n",
            "Batch: 8960 Loss: 0.04037068927577274\n",
            "Batch: 9280 Loss: 0.03683012501782717\n",
            "Batch: 9600 Loss: 0.04620718461629659\n",
            "Batch: 9920 Loss: 0.045539679332123065\n",
            "Batch: 10240 Loss: 0.06374236158770032\n",
            "Batch: 10560 Loss: 0.04609324860480586\n",
            "Batch: 10880 Loss: 0.03954817142930704\n",
            "Epoch: 346\n",
            "Batch: 0 Loss: 0.03723610391433239\n",
            "Batch: 320 Loss: 0.04534084118103004\n",
            "Batch: 640 Loss: 0.0362569982176367\n",
            "Batch: 960 Loss: 0.06560649150737255\n",
            "Batch: 1280 Loss: 0.04651326680683647\n",
            "Batch: 1600 Loss: 0.03879266691264256\n",
            "Batch: 1920 Loss: 0.053762617897720764\n",
            "Batch: 2240 Loss: 0.05134755136740654\n",
            "Batch: 2560 Loss: 0.04636009834426557\n",
            "Batch: 2880 Loss: 0.028601126699596486\n",
            "Batch: 3200 Loss: 0.056587457145682724\n",
            "Batch: 3520 Loss: 0.0724586409372527\n",
            "Batch: 3840 Loss: 0.07765712310992152\n",
            "Batch: 4160 Loss: 0.03148349069512905\n",
            "Batch: 4480 Loss: 0.03068345658678603\n",
            "Batch: 4800 Loss: 0.03915971033853565\n",
            "Batch: 5120 Loss: 0.045979604367721104\n",
            "Batch: 5440 Loss: 0.03976844303456518\n",
            "Batch: 5760 Loss: 0.05744474099364676\n",
            "Batch: 6080 Loss: 0.03661603740729033\n",
            "Batch: 6400 Loss: 0.05677352055919279\n",
            "Batch: 6720 Loss: 0.037235946967093383\n",
            "Batch: 7040 Loss: 0.06186199963788333\n",
            "Batch: 7360 Loss: 0.051986937157811344\n",
            "Batch: 7680 Loss: 0.05012522917116238\n",
            "Batch: 8000 Loss: 0.04631606903284749\n",
            "Batch: 8320 Loss: 0.045501919391610024\n",
            "Batch: 8640 Loss: 0.04748146615302033\n",
            "Batch: 8960 Loss: 0.05930729424060577\n",
            "Batch: 9280 Loss: 0.04300642875347842\n",
            "Batch: 9600 Loss: 0.06248199584704884\n",
            "Batch: 9920 Loss: 0.04134261938770995\n",
            "Batch: 10240 Loss: 0.03804895600176771\n",
            "Batch: 10560 Loss: 0.03867168829108982\n",
            "Batch: 10880 Loss: 0.05478957014207971\n",
            "Epoch: 347\n",
            "Batch: 0 Loss: 0.038564142701458394\n",
            "Batch: 320 Loss: 0.04981930691586844\n",
            "Batch: 640 Loss: 0.03965547410004252\n",
            "Batch: 960 Loss: 0.05651405275883406\n",
            "Batch: 1280 Loss: 0.0338474153058094\n",
            "Batch: 1600 Loss: 0.07321562355656927\n",
            "Batch: 1920 Loss: 0.03197011947356316\n",
            "Batch: 2240 Loss: 0.04608618422631601\n",
            "Batch: 2560 Loss: 0.05432826077761623\n",
            "Batch: 2880 Loss: 0.04472548138486655\n",
            "Batch: 3200 Loss: 0.036157453299783704\n",
            "Batch: 3520 Loss: 0.03798320606334571\n",
            "Batch: 3840 Loss: 0.0425673670271421\n",
            "Batch: 4160 Loss: 0.05875312567386118\n",
            "Batch: 4480 Loss: 0.0490541786490418\n",
            "Batch: 4800 Loss: 0.03651402913384628\n",
            "Batch: 5120 Loss: 0.05498618743628574\n",
            "Batch: 5440 Loss: 0.041817701255215944\n",
            "Batch: 5760 Loss: 0.04240302703197262\n",
            "Batch: 6080 Loss: 0.06710425334285386\n",
            "Batch: 6400 Loss: 0.03435675737493354\n",
            "Batch: 6720 Loss: 0.04719937070912011\n",
            "Batch: 7040 Loss: 0.042949037246733775\n",
            "Batch: 7360 Loss: 0.027063027299423312\n",
            "Batch: 7680 Loss: 0.03906883551544726\n",
            "Batch: 8000 Loss: 0.059537018663386926\n",
            "Batch: 8320 Loss: 0.0461628689132739\n",
            "Batch: 8640 Loss: 0.057146168295615224\n",
            "Batch: 8960 Loss: 0.036836967718088885\n",
            "Batch: 9280 Loss: 0.05736823291676263\n",
            "Batch: 9600 Loss: 0.04444986723666346\n",
            "Batch: 9920 Loss: 0.04039895945372599\n",
            "Batch: 10240 Loss: 0.03223928511648523\n",
            "Batch: 10560 Loss: 0.035239519590488246\n",
            "Batch: 10880 Loss: 0.06658963343651031\n",
            "Epoch: 348\n",
            "Batch: 0 Loss: 0.04324152703548179\n",
            "Batch: 320 Loss: 0.043395434464869494\n",
            "Batch: 640 Loss: 0.040614939024929374\n",
            "Batch: 960 Loss: 0.029778835621656334\n",
            "Batch: 1280 Loss: 0.04003924032901001\n",
            "Batch: 1600 Loss: 0.05480863353475352\n",
            "Batch: 1920 Loss: 0.051924919855399154\n",
            "Batch: 2240 Loss: 0.045789703657487975\n",
            "Batch: 2560 Loss: 0.04545646054942825\n",
            "Batch: 2880 Loss: 0.037497942993813635\n",
            "Batch: 3200 Loss: 0.03860680210545991\n",
            "Batch: 3520 Loss: 0.05109441422483655\n",
            "Batch: 3840 Loss: 0.03318554800913478\n",
            "Batch: 4160 Loss: 0.04739278772386727\n",
            "Batch: 4480 Loss: 0.028202882572786015\n",
            "Batch: 4800 Loss: 0.029963843045479548\n",
            "Batch: 5120 Loss: 0.06135920193267431\n",
            "Batch: 5440 Loss: 0.11030363308031864\n",
            "Batch: 5760 Loss: 0.0692964265812017\n",
            "Batch: 6080 Loss: 0.049460610889558804\n",
            "Batch: 6400 Loss: 0.05732443910208055\n",
            "Batch: 6720 Loss: 0.03810745724054746\n",
            "Batch: 7040 Loss: 0.04460654460293171\n",
            "Batch: 7360 Loss: 0.05993173336250401\n",
            "Batch: 7680 Loss: 0.033277372337555476\n",
            "Batch: 8000 Loss: 0.04215884921771193\n",
            "Batch: 8320 Loss: 0.04341697613047925\n",
            "Batch: 8640 Loss: 0.05133386574550239\n",
            "Batch: 8960 Loss: 0.04478610452746967\n",
            "Batch: 9280 Loss: 0.05556835438163883\n",
            "Batch: 9600 Loss: 0.038652123712695045\n",
            "Batch: 9920 Loss: 0.05322862602362414\n",
            "Batch: 10240 Loss: 0.08383947030691691\n",
            "Batch: 10560 Loss: 0.044080809634089814\n",
            "Batch: 10880 Loss: 0.043024962138710085\n",
            "Epoch: 349\n",
            "Batch: 0 Loss: 0.05291409844808454\n",
            "Batch: 320 Loss: 0.06593958533665224\n",
            "Batch: 640 Loss: 0.034320002323460067\n",
            "Batch: 960 Loss: 0.03344741464502035\n",
            "Batch: 1280 Loss: 0.03510248179511382\n",
            "Batch: 1600 Loss: 0.03674261418427355\n",
            "Batch: 1920 Loss: 0.05357926616959705\n",
            "Batch: 2240 Loss: 0.06782335951840449\n",
            "Batch: 2560 Loss: 0.05721136634573041\n",
            "Batch: 2880 Loss: 0.06762633098822611\n",
            "Batch: 3200 Loss: 0.045253162311914896\n",
            "Batch: 3520 Loss: 0.03881291404998519\n",
            "Batch: 3840 Loss: 0.05267031985380525\n",
            "Batch: 4160 Loss: 0.050756608765670314\n",
            "Batch: 4480 Loss: 0.04866669265006364\n",
            "Batch: 4800 Loss: 0.028605120528331675\n",
            "Batch: 5120 Loss: 0.03778033609066618\n",
            "Batch: 5440 Loss: 0.03685060677552582\n",
            "Batch: 5760 Loss: 0.03795583358659812\n",
            "Batch: 6080 Loss: 0.04468816430817167\n",
            "Batch: 6400 Loss: 0.0605769016712461\n",
            "Batch: 6720 Loss: 0.07240811338443821\n",
            "Batch: 7040 Loss: 0.030776594319022122\n",
            "Batch: 7360 Loss: 0.03778972380946006\n",
            "Batch: 7680 Loss: 0.05957628899235683\n",
            "Batch: 8000 Loss: 0.05167302736167619\n",
            "Batch: 8320 Loss: 0.04448288721512226\n",
            "Batch: 8640 Loss: 0.05101295566789488\n",
            "Batch: 8960 Loss: 0.05693709856459538\n",
            "Batch: 9280 Loss: 0.04632687870817034\n",
            "Batch: 9600 Loss: 0.05967859343406331\n",
            "Batch: 9920 Loss: 0.029899877596421116\n",
            "Batch: 10240 Loss: 0.04053443942003898\n",
            "Batch: 10560 Loss: 0.04807843000456294\n",
            "Batch: 10880 Loss: 0.051891434001346964\n",
            "Epoch: 350\n",
            "Batch: 0 Loss: 0.03878972402633655\n",
            "Batch: 320 Loss: 0.047411434251507435\n",
            "Batch: 640 Loss: 0.042598821960177044\n",
            "Batch: 960 Loss: 0.04401454723836034\n",
            "Batch: 1280 Loss: 0.030126586581542998\n",
            "Batch: 1600 Loss: 0.03393516848636683\n",
            "Batch: 1920 Loss: 0.06668143123911024\n",
            "Batch: 2240 Loss: 0.048459057178393314\n",
            "Batch: 2560 Loss: 0.05934017867994604\n",
            "Batch: 2880 Loss: 0.027469277627158735\n",
            "Batch: 3200 Loss: 0.03480852598234004\n",
            "Batch: 3520 Loss: 0.059796839631152794\n",
            "Batch: 3840 Loss: 0.033423655757877965\n",
            "Batch: 4160 Loss: 0.06090715880127362\n",
            "Batch: 4480 Loss: 0.028986141712449214\n",
            "Batch: 4800 Loss: 0.05599568663818871\n",
            "Batch: 5120 Loss: 0.04111590915990433\n",
            "Batch: 5440 Loss: 0.05452160845498375\n",
            "Batch: 5760 Loss: 0.036879421136862496\n",
            "Batch: 6080 Loss: 0.05269423843634767\n",
            "Batch: 6400 Loss: 0.03203062292641694\n",
            "Batch: 6720 Loss: 0.04688754530452076\n",
            "Batch: 7040 Loss: 0.052361948945855566\n",
            "Batch: 7360 Loss: 0.046640474997966225\n",
            "Batch: 7680 Loss: 0.036147985429702864\n",
            "Batch: 8000 Loss: 0.04692984462922783\n",
            "Batch: 8320 Loss: 0.05226273973224615\n",
            "Batch: 8640 Loss: 0.04895493504332214\n",
            "Batch: 8960 Loss: 0.03456543950166788\n",
            "Batch: 9280 Loss: 0.05986776644798111\n",
            "Batch: 9600 Loss: 0.03058783740454902\n",
            "Batch: 9920 Loss: 0.07207881205133697\n",
            "Batch: 10240 Loss: 0.06803946969213812\n",
            "Batch: 10560 Loss: 0.06521356725513978\n",
            "Batch: 10880 Loss: 0.04405092341050856\n",
            "Epoch: 351\n",
            "Batch: 0 Loss: 0.04238509887597748\n",
            "Batch: 320 Loss: 0.04397179871270084\n",
            "Batch: 640 Loss: 0.05962152865409125\n",
            "Batch: 960 Loss: 0.041601216558678746\n",
            "Batch: 1280 Loss: 0.03956551772308221\n",
            "Batch: 1600 Loss: 0.0336787436791542\n",
            "Batch: 1920 Loss: 0.04025796014948675\n",
            "Batch: 2240 Loss: 0.092210134838758\n",
            "Batch: 2560 Loss: 0.05235660423782212\n",
            "Batch: 2880 Loss: 0.0430987225724079\n",
            "Batch: 3200 Loss: 0.06243102262646834\n",
            "Batch: 3520 Loss: 0.05248394378916109\n",
            "Batch: 3840 Loss: 0.0529220404118794\n",
            "Batch: 4160 Loss: 0.034692763740916295\n",
            "Batch: 4480 Loss: 0.028446738211113775\n",
            "Batch: 4800 Loss: 0.052344883665275094\n",
            "Batch: 5120 Loss: 0.04173310401587852\n",
            "Batch: 5440 Loss: 0.05401866274207711\n",
            "Batch: 5760 Loss: 0.0627584533320976\n",
            "Batch: 6080 Loss: 0.030180143171798523\n",
            "Batch: 6400 Loss: 0.03729098702869167\n",
            "Batch: 6720 Loss: 0.05762161598237631\n",
            "Batch: 7040 Loss: 0.03981919061119106\n",
            "Batch: 7360 Loss: 0.04079607058691399\n",
            "Batch: 7680 Loss: 0.04139859465507742\n",
            "Batch: 8000 Loss: 0.0474539771687012\n",
            "Batch: 8320 Loss: 0.048899518116119596\n",
            "Batch: 8640 Loss: 0.04789388065211874\n",
            "Batch: 8960 Loss: 0.03421028961688799\n",
            "Batch: 9280 Loss: 0.04207669821758278\n",
            "Batch: 9600 Loss: 0.07004486530870803\n",
            "Batch: 9920 Loss: 0.02971623044836265\n",
            "Batch: 10240 Loss: 0.04733218182778801\n",
            "Batch: 10560 Loss: 0.044773271445173055\n",
            "Batch: 10880 Loss: 0.04487315812031555\n",
            "Epoch: 352\n",
            "Batch: 0 Loss: 0.04584086823159259\n",
            "Batch: 320 Loss: 0.04442458700109101\n",
            "Batch: 640 Loss: 0.030238963751240626\n",
            "Batch: 960 Loss: 0.05857519260285976\n",
            "Batch: 1280 Loss: 0.050263952649083204\n",
            "Batch: 1600 Loss: 0.04994013802767852\n",
            "Batch: 1920 Loss: 0.03904234278809449\n",
            "Batch: 2240 Loss: 0.03384489532577582\n",
            "Batch: 2560 Loss: 0.034975046063534944\n",
            "Batch: 2880 Loss: 0.03545411181330148\n",
            "Batch: 3200 Loss: 0.05745481387819262\n",
            "Batch: 3520 Loss: 0.05670286963537283\n",
            "Batch: 3840 Loss: 0.033942434032908475\n",
            "Batch: 4160 Loss: 0.03680178565277886\n",
            "Batch: 4480 Loss: 0.0530467489434831\n",
            "Batch: 4800 Loss: 0.05450874337377078\n",
            "Batch: 5120 Loss: 0.036725154438845914\n",
            "Batch: 5440 Loss: 0.07673488875369355\n",
            "Batch: 5760 Loss: 0.05460259780662976\n",
            "Batch: 6080 Loss: 0.029530728833210975\n",
            "Batch: 6400 Loss: 0.05165655525406751\n",
            "Batch: 6720 Loss: 0.06280711778370399\n",
            "Batch: 7040 Loss: 0.0347887474150747\n",
            "Batch: 7360 Loss: 0.04621693298338917\n",
            "Batch: 7680 Loss: 0.04614177695008481\n",
            "Batch: 8000 Loss: 0.04383962385344068\n",
            "Batch: 8320 Loss: 0.05821086523087884\n",
            "Batch: 8640 Loss: 0.08182902929364952\n",
            "Batch: 8960 Loss: 0.04405649494822904\n",
            "Batch: 9280 Loss: 0.04320046472076146\n",
            "Batch: 9600 Loss: 0.050131555865371324\n",
            "Batch: 9920 Loss: 0.0436346229392618\n",
            "Batch: 10240 Loss: 0.07049070215913857\n",
            "Batch: 10560 Loss: 0.06045201490121535\n",
            "Batch: 10880 Loss: 0.05808243915117423\n",
            "Epoch: 353\n",
            "Batch: 0 Loss: 0.04693494297317832\n",
            "Batch: 320 Loss: 0.05174971270580323\n",
            "Batch: 640 Loss: 0.047894161280649834\n",
            "Batch: 960 Loss: 0.046761583360114635\n",
            "Batch: 1280 Loss: 0.0357301810683704\n",
            "Batch: 1600 Loss: 0.07635977246914342\n",
            "Batch: 1920 Loss: 0.04250720017024228\n",
            "Batch: 2240 Loss: 0.03312033858677338\n",
            "Batch: 2560 Loss: 0.046793929663207695\n",
            "Batch: 2880 Loss: 0.041228522105712345\n",
            "Batch: 3200 Loss: 0.04467768736076085\n",
            "Batch: 3520 Loss: 0.034033359884621625\n",
            "Batch: 3840 Loss: 0.04667901417607262\n",
            "Batch: 4160 Loss: 0.05754197838022508\n",
            "Batch: 4480 Loss: 0.036801212671528694\n",
            "Batch: 4800 Loss: 0.06237005816228471\n",
            "Batch: 5120 Loss: 0.06656092526673203\n",
            "Batch: 5440 Loss: 0.05066590813579272\n",
            "Batch: 5760 Loss: 0.031856804786014416\n",
            "Batch: 6080 Loss: 0.0526150963500098\n",
            "Batch: 6400 Loss: 0.05033901511481235\n",
            "Batch: 6720 Loss: 0.040424068510438615\n",
            "Batch: 7040 Loss: 0.037295073474498175\n",
            "Batch: 7360 Loss: 0.038107594754102976\n",
            "Batch: 7680 Loss: 0.06351313039805372\n",
            "Batch: 8000 Loss: 0.04566382937095911\n",
            "Batch: 8320 Loss: 0.0536181178660689\n",
            "Batch: 8640 Loss: 0.05545205022031514\n",
            "Batch: 8960 Loss: 0.034783191126719425\n",
            "Batch: 9280 Loss: 0.06004152966967972\n",
            "Batch: 9600 Loss: 0.053136104160885725\n",
            "Batch: 9920 Loss: 0.03645619650507475\n",
            "Batch: 10240 Loss: 0.07175068070046652\n",
            "Batch: 10560 Loss: 0.042946874369485376\n",
            "Batch: 10880 Loss: 0.030794785310741958\n",
            "Epoch: 354\n",
            "Batch: 0 Loss: 0.0543082764799919\n",
            "Batch: 320 Loss: 0.03919346618650556\n",
            "Batch: 640 Loss: 0.039113633517705665\n",
            "Batch: 960 Loss: 0.05150045861469632\n",
            "Batch: 1280 Loss: 0.06294596862703632\n",
            "Batch: 1600 Loss: 0.052765261824052244\n",
            "Batch: 1920 Loss: 0.030404607262736055\n",
            "Batch: 2240 Loss: 0.065568462953697\n",
            "Batch: 2560 Loss: 0.0361622646657977\n",
            "Batch: 2880 Loss: 0.0667041224737883\n",
            "Batch: 3200 Loss: 0.04283806097679752\n",
            "Batch: 3520 Loss: 0.04617192388257818\n",
            "Batch: 3840 Loss: 0.03987953504497447\n",
            "Batch: 4160 Loss: 0.03859463882760967\n",
            "Batch: 4480 Loss: 0.05708934682526565\n",
            "Batch: 4800 Loss: 0.034841949717894394\n",
            "Batch: 5120 Loss: 0.032003799460554545\n",
            "Batch: 5440 Loss: 0.040425278127409384\n",
            "Batch: 5760 Loss: 0.04275418445920476\n",
            "Batch: 6080 Loss: 0.0407155239667008\n",
            "Batch: 6400 Loss: 0.060949657697122565\n",
            "Batch: 6720 Loss: 0.03980489941205562\n",
            "Batch: 7040 Loss: 0.05015256143441613\n",
            "Batch: 7360 Loss: 0.03268999460180623\n",
            "Batch: 7680 Loss: 0.055872178824752886\n",
            "Batch: 8000 Loss: 0.039018188670623406\n",
            "Batch: 8320 Loss: 0.044527700964088895\n",
            "Batch: 8640 Loss: 0.038143031245855\n",
            "Batch: 8960 Loss: 0.05352410893666902\n",
            "Batch: 9280 Loss: 0.0638315153852691\n",
            "Batch: 9600 Loss: 0.037608686459978835\n",
            "Batch: 9920 Loss: 0.038628590495070995\n",
            "Batch: 10240 Loss: 0.04360946656067097\n",
            "Batch: 10560 Loss: 0.05424869704675994\n",
            "Batch: 10880 Loss: 0.047405189958830946\n",
            "Epoch: 355\n",
            "Batch: 0 Loss: 0.05214636726758032\n",
            "Batch: 320 Loss: 0.03550886788164333\n",
            "Batch: 640 Loss: 0.047728087659084026\n",
            "Batch: 960 Loss: 0.04038639500406008\n",
            "Batch: 1280 Loss: 0.0364218923106277\n",
            "Batch: 1600 Loss: 0.030666484447636134\n",
            "Batch: 1920 Loss: 0.02761863716718591\n",
            "Batch: 2240 Loss: 0.0521204430176635\n",
            "Batch: 2560 Loss: 0.049582369333392745\n",
            "Batch: 2880 Loss: 0.08182956936917804\n",
            "Batch: 3200 Loss: 0.04908217764764104\n",
            "Batch: 3520 Loss: 0.06056641065704508\n",
            "Batch: 3840 Loss: 0.05246677079898213\n",
            "Batch: 4160 Loss: 0.057686915128251026\n",
            "Batch: 4480 Loss: 0.04630512128387471\n",
            "Batch: 4800 Loss: 0.04497056380224616\n",
            "Batch: 5120 Loss: 0.02990450463655826\n",
            "Batch: 5440 Loss: 0.05948071781226101\n",
            "Batch: 5760 Loss: 0.037576791729810856\n",
            "Batch: 6080 Loss: 0.04970157471027516\n",
            "Batch: 6400 Loss: 0.04926402293276189\n",
            "Batch: 6720 Loss: 0.053854327747565635\n",
            "Batch: 7040 Loss: 0.03897714026670354\n",
            "Batch: 7360 Loss: 0.04039001554483052\n",
            "Batch: 7680 Loss: 0.04325467295372605\n",
            "Batch: 8000 Loss: 0.04581557616962384\n",
            "Batch: 8320 Loss: 0.048398344062801676\n",
            "Batch: 8640 Loss: 0.03897103206332263\n",
            "Batch: 8960 Loss: 0.043117191527614396\n",
            "Batch: 9280 Loss: 0.0390036835386288\n",
            "Batch: 9600 Loss: 0.055289666572815456\n",
            "Batch: 9920 Loss: 0.05347087367210392\n",
            "Batch: 10240 Loss: 0.08038884084785997\n",
            "Batch: 10560 Loss: 0.05862361561474218\n",
            "Batch: 10880 Loss: 0.07596778995742538\n",
            "Epoch: 356\n",
            "Batch: 0 Loss: 0.04519949770991495\n",
            "Batch: 320 Loss: 0.04220291612886008\n",
            "Batch: 640 Loss: 0.05069219291332519\n",
            "Batch: 960 Loss: 0.03243966635145842\n",
            "Batch: 1280 Loss: 0.06502687716730465\n",
            "Batch: 1600 Loss: 0.059328793342177555\n",
            "Batch: 1920 Loss: 0.03264783803508578\n",
            "Batch: 2240 Loss: 0.06061186407293584\n",
            "Batch: 2560 Loss: 0.049948932846661395\n",
            "Batch: 2880 Loss: 0.0382151807295279\n",
            "Batch: 3200 Loss: 0.038720144919733544\n",
            "Batch: 3520 Loss: 0.04545063154237755\n",
            "Batch: 3840 Loss: 0.03305214622640562\n",
            "Batch: 4160 Loss: 0.05580522221934374\n",
            "Batch: 4480 Loss: 0.04541936960562383\n",
            "Batch: 4800 Loss: 0.04612005350394265\n",
            "Batch: 5120 Loss: 0.05190408215262202\n",
            "Batch: 5440 Loss: 0.05116223438736664\n",
            "Batch: 5760 Loss: 0.055157961594153704\n",
            "Batch: 6080 Loss: 0.05378131265602691\n",
            "Batch: 6400 Loss: 0.03982390631248702\n",
            "Batch: 6720 Loss: 0.040301225727204376\n",
            "Batch: 7040 Loss: 0.028412434713796154\n",
            "Batch: 7360 Loss: 0.034500838938771435\n",
            "Batch: 7680 Loss: 0.05323073284407731\n",
            "Batch: 8000 Loss: 0.04222401245170385\n",
            "Batch: 8320 Loss: 0.03298693038698243\n",
            "Batch: 8640 Loss: 0.04829124528181908\n",
            "Batch: 8960 Loss: 0.059041309704546456\n",
            "Batch: 9280 Loss: 0.042148808199755805\n",
            "Batch: 9600 Loss: 0.06096533872930198\n",
            "Batch: 9920 Loss: 0.044321809593240495\n",
            "Batch: 10240 Loss: 0.04915375849559832\n",
            "Batch: 10560 Loss: 0.04555971647293041\n",
            "Batch: 10880 Loss: 0.05940905154053222\n",
            "Epoch: 357\n",
            "Batch: 0 Loss: 0.046091386495997456\n",
            "Batch: 320 Loss: 0.06171314339030077\n",
            "Batch: 640 Loss: 0.04877260245377071\n",
            "Batch: 960 Loss: 0.0663997393294092\n",
            "Batch: 1280 Loss: 0.043955676574723934\n",
            "Batch: 1600 Loss: 0.06567944578075412\n",
            "Batch: 1920 Loss: 0.04867936015440357\n",
            "Batch: 2240 Loss: 0.03611138471590239\n",
            "Batch: 2560 Loss: 0.04822886242976217\n",
            "Batch: 2880 Loss: 0.037342294439496306\n",
            "Batch: 3200 Loss: 0.03409055813284045\n",
            "Batch: 3520 Loss: 0.0576721344583957\n",
            "Batch: 3840 Loss: 0.05376984690657921\n",
            "Batch: 4160 Loss: 0.05368705395655997\n",
            "Batch: 4480 Loss: 0.054722873343594367\n",
            "Batch: 4800 Loss: 0.03243966491869497\n",
            "Batch: 5120 Loss: 0.046250331280045305\n",
            "Batch: 5440 Loss: 0.03201607038845035\n",
            "Batch: 5760 Loss: 0.035151579745226716\n",
            "Batch: 6080 Loss: 0.04878410470328237\n",
            "Batch: 6400 Loss: 0.033236405670843586\n",
            "Batch: 6720 Loss: 0.03973462244884572\n",
            "Batch: 7040 Loss: 0.042460344106356164\n",
            "Batch: 7360 Loss: 0.05802259555716738\n",
            "Batch: 7680 Loss: 0.029396658267113494\n",
            "Batch: 8000 Loss: 0.05067438728890438\n",
            "Batch: 8320 Loss: 0.06650030877009247\n",
            "Batch: 8640 Loss: 0.03825362350835901\n",
            "Batch: 8960 Loss: 0.05193073226903122\n",
            "Batch: 9280 Loss: 0.0423912426632508\n",
            "Batch: 9600 Loss: 0.03363810104112168\n",
            "Batch: 9920 Loss: 0.053950580604707685\n",
            "Batch: 10240 Loss: 0.05628830921837484\n",
            "Batch: 10560 Loss: 0.057667051692016594\n",
            "Batch: 10880 Loss: 0.029504621900613648\n",
            "Epoch: 358\n",
            "Batch: 0 Loss: 0.06205816908966912\n",
            "Batch: 320 Loss: 0.0459493427555117\n",
            "Batch: 640 Loss: 0.03970531417191918\n",
            "Batch: 960 Loss: 0.0327656696578551\n",
            "Batch: 1280 Loss: 0.031804333871353146\n",
            "Batch: 1600 Loss: 0.04423900119326011\n",
            "Batch: 1920 Loss: 0.037626717272377916\n",
            "Batch: 2240 Loss: 0.0642215246661579\n",
            "Batch: 2560 Loss: 0.05777358244526562\n",
            "Batch: 2880 Loss: 0.03634066212108458\n",
            "Batch: 3200 Loss: 0.04552478311188035\n",
            "Batch: 3520 Loss: 0.040417321578991396\n",
            "Batch: 3840 Loss: 0.03730520286765555\n",
            "Batch: 4160 Loss: 0.04814791545402056\n",
            "Batch: 4480 Loss: 0.03039104449821084\n",
            "Batch: 4800 Loss: 0.03410590523774392\n",
            "Batch: 5120 Loss: 0.0601255491287767\n",
            "Batch: 5440 Loss: 0.04410205784497527\n",
            "Batch: 5760 Loss: 0.05562502676653876\n",
            "Batch: 6080 Loss: 0.05281395184854688\n",
            "Batch: 6400 Loss: 0.0511335501348263\n",
            "Batch: 6720 Loss: 0.0448879986719688\n",
            "Batch: 7040 Loss: 0.05778364842031984\n",
            "Batch: 7360 Loss: 0.0414820550849809\n",
            "Batch: 7680 Loss: 0.07074751020459967\n",
            "Batch: 8000 Loss: 0.04937802284971835\n",
            "Batch: 8320 Loss: 0.04774458501785127\n",
            "Batch: 8640 Loss: 0.05381866947051705\n",
            "Batch: 8960 Loss: 0.053986856531646\n",
            "Batch: 9280 Loss: 0.031776109414967606\n",
            "Batch: 9600 Loss: 0.03428375582310753\n",
            "Batch: 9920 Loss: 0.06341271981716848\n",
            "Batch: 10240 Loss: 0.045550438116728376\n",
            "Batch: 10560 Loss: 0.05954663535813004\n",
            "Batch: 10880 Loss: 0.030987679618962854\n",
            "Epoch: 359\n",
            "Batch: 0 Loss: 0.03159006492750988\n",
            "Batch: 320 Loss: 0.04755565494208948\n",
            "Batch: 640 Loss: 0.05176918678692666\n",
            "Batch: 960 Loss: 0.02591669209665466\n",
            "Batch: 1280 Loss: 0.042546909145388265\n",
            "Batch: 1600 Loss: 0.06932845372310507\n",
            "Batch: 1920 Loss: 0.03309443177551293\n",
            "Batch: 2240 Loss: 0.06298054614442299\n",
            "Batch: 2560 Loss: 0.03606756899671568\n",
            "Batch: 2880 Loss: 0.03575993245867288\n",
            "Batch: 3200 Loss: 0.04355872453728454\n",
            "Batch: 3520 Loss: 0.05301409174633045\n",
            "Batch: 3840 Loss: 0.06353021372542274\n",
            "Batch: 4160 Loss: 0.05339527638288894\n",
            "Batch: 4480 Loss: 0.043259628692160164\n",
            "Batch: 4800 Loss: 0.06352924327438633\n",
            "Batch: 5120 Loss: 0.05846455347781325\n",
            "Batch: 5440 Loss: 0.06420551159268877\n",
            "Batch: 5760 Loss: 0.04977580064024862\n",
            "Batch: 6080 Loss: 0.061514099112510955\n",
            "Batch: 6400 Loss: 0.04284022381210697\n",
            "Batch: 6720 Loss: 0.05017081617597538\n",
            "Batch: 7040 Loss: 0.05193015685235305\n",
            "Batch: 7360 Loss: 0.06154985156288282\n",
            "Batch: 7680 Loss: 0.04454114564680497\n",
            "Batch: 8000 Loss: 0.05462334032821733\n",
            "Batch: 8320 Loss: 0.03841924020046261\n",
            "Batch: 8640 Loss: 0.0533211117942392\n",
            "Batch: 8960 Loss: 0.03481422841228336\n",
            "Batch: 9280 Loss: 0.032708296673854755\n",
            "Batch: 9600 Loss: 0.05303334844319032\n",
            "Batch: 9920 Loss: 0.031772404720580204\n",
            "Batch: 10240 Loss: 0.035384653225926675\n",
            "Batch: 10560 Loss: 0.03788170538350384\n",
            "Batch: 10880 Loss: 0.033802841130451504\n",
            "Epoch: 360\n",
            "Batch: 0 Loss: 0.04955709134632475\n",
            "Batch: 320 Loss: 0.05625529176818501\n",
            "Batch: 640 Loss: 0.05113532615257215\n",
            "Batch: 960 Loss: 0.059371029859884794\n",
            "Batch: 1280 Loss: 0.042560019766378444\n",
            "Batch: 1600 Loss: 0.04112008932191796\n",
            "Batch: 1920 Loss: 0.03366299965103617\n",
            "Batch: 2240 Loss: 0.0509948097636997\n",
            "Batch: 2560 Loss: 0.05100925365014256\n",
            "Batch: 2880 Loss: 0.05982987570157015\n",
            "Batch: 3200 Loss: 0.038720168250671\n",
            "Batch: 3520 Loss: 0.05141445610370872\n",
            "Batch: 3840 Loss: 0.08112266744270968\n",
            "Batch: 4160 Loss: 0.05001032996239265\n",
            "Batch: 4480 Loss: 0.05630450782406761\n",
            "Batch: 4800 Loss: 0.05683448955533472\n",
            "Batch: 5120 Loss: 0.04334798035477756\n",
            "Batch: 5440 Loss: 0.03621318739314256\n",
            "Batch: 5760 Loss: 0.046232079005554795\n",
            "Batch: 6080 Loss: 0.04690872941509744\n",
            "Batch: 6400 Loss: 0.03640512364895839\n",
            "Batch: 6720 Loss: 0.03985152328303221\n",
            "Batch: 7040 Loss: 0.032027509169371624\n",
            "Batch: 7360 Loss: 0.04613260536988474\n",
            "Batch: 7680 Loss: 0.05111403155416766\n",
            "Batch: 8000 Loss: 0.036034880941143654\n",
            "Batch: 8320 Loss: 0.03366973098158546\n",
            "Batch: 8640 Loss: 0.03665831533431289\n",
            "Batch: 8960 Loss: 0.04906914521150826\n",
            "Batch: 9280 Loss: 0.059524005367561555\n",
            "Batch: 9600 Loss: 0.04608385385353316\n",
            "Batch: 9920 Loss: 0.039704280508580984\n",
            "Batch: 10240 Loss: 0.033877357493718015\n",
            "Batch: 10560 Loss: 0.03991597945198137\n",
            "Batch: 10880 Loss: 0.042214349041192\n",
            "Epoch: 361\n",
            "Batch: 0 Loss: 0.05605211722989369\n",
            "Batch: 320 Loss: 0.035954201417011944\n",
            "Batch: 640 Loss: 0.042274251236412694\n",
            "Batch: 960 Loss: 0.05552634206092954\n",
            "Batch: 1280 Loss: 0.04496674503341198\n",
            "Batch: 1600 Loss: 0.07512775050999096\n",
            "Batch: 1920 Loss: 0.07002265996842479\n",
            "Batch: 2240 Loss: 0.08390280943556469\n",
            "Batch: 2560 Loss: 0.05350874706953826\n",
            "Batch: 2880 Loss: 0.04383232853136065\n",
            "Batch: 3200 Loss: 0.0485009336322367\n",
            "Batch: 3520 Loss: 0.06925135889997805\n",
            "Batch: 3840 Loss: 0.038879025955033834\n",
            "Batch: 4160 Loss: 0.04703543268867414\n",
            "Batch: 4480 Loss: 0.05432874428156463\n",
            "Batch: 4800 Loss: 0.05012818058376513\n",
            "Batch: 5120 Loss: 0.047691853573153926\n",
            "Batch: 5440 Loss: 0.05895936062729499\n",
            "Batch: 5760 Loss: 0.03443691602234103\n",
            "Batch: 6080 Loss: 0.03071508890351198\n",
            "Batch: 6400 Loss: 0.04776019500718038\n",
            "Batch: 6720 Loss: 0.03802632844400473\n",
            "Batch: 7040 Loss: 0.04470724420715468\n",
            "Batch: 7360 Loss: 0.027980672893001386\n",
            "Batch: 7680 Loss: 0.04021396949355937\n",
            "Batch: 8000 Loss: 0.05512634507043099\n",
            "Batch: 8320 Loss: 0.04721313499880622\n",
            "Batch: 8640 Loss: 0.0437818470445475\n",
            "Batch: 8960 Loss: 0.044940881542114286\n",
            "Batch: 9280 Loss: 0.042860326179662164\n",
            "Batch: 9600 Loss: 0.040990507008796216\n",
            "Batch: 9920 Loss: 0.06434421345777272\n",
            "Batch: 10240 Loss: 0.052517821069675355\n",
            "Batch: 10560 Loss: 0.06146213846935459\n",
            "Batch: 10880 Loss: 0.048484159166431055\n",
            "Epoch: 362\n",
            "Batch: 0 Loss: 0.04462317246764636\n",
            "Batch: 320 Loss: 0.06037753245618151\n",
            "Batch: 640 Loss: 0.06444388297442652\n",
            "Batch: 960 Loss: 0.0677503451397862\n",
            "Batch: 1280 Loss: 0.04684026471768568\n",
            "Batch: 1600 Loss: 0.04568300102106837\n",
            "Batch: 1920 Loss: 0.029700790063943727\n",
            "Batch: 2240 Loss: 0.04026219082888851\n",
            "Batch: 2560 Loss: 0.05617817321527998\n",
            "Batch: 2880 Loss: 0.03815116123914817\n",
            "Batch: 3200 Loss: 0.059976384188371624\n",
            "Batch: 3520 Loss: 0.03770775924676316\n",
            "Batch: 3840 Loss: 0.08112852536803426\n",
            "Batch: 4160 Loss: 0.035649355732831595\n",
            "Batch: 4480 Loss: 0.05423350161006976\n",
            "Batch: 4800 Loss: 0.04204750725191724\n",
            "Batch: 5120 Loss: 0.07197169989460336\n",
            "Batch: 5440 Loss: 0.056196384975176206\n",
            "Batch: 5760 Loss: 0.07713510871536226\n",
            "Batch: 6080 Loss: 0.03714901068013321\n",
            "Batch: 6400 Loss: 0.07024648234531307\n",
            "Batch: 6720 Loss: 0.033328117888504864\n",
            "Batch: 7040 Loss: 0.03595894556085082\n",
            "Batch: 7360 Loss: 0.03074128741540468\n",
            "Batch: 7680 Loss: 0.047380226819543576\n",
            "Batch: 8000 Loss: 0.04608735743306037\n",
            "Batch: 8320 Loss: 0.035950540709708274\n",
            "Batch: 8640 Loss: 0.05092471236369331\n",
            "Batch: 8960 Loss: 0.05166656949894747\n",
            "Batch: 9280 Loss: 0.028197517501382437\n",
            "Batch: 9600 Loss: 0.05026914705419431\n",
            "Batch: 9920 Loss: 0.033299343948556194\n",
            "Batch: 10240 Loss: 0.04100987435495323\n",
            "Batch: 10560 Loss: 0.03746460979602143\n",
            "Batch: 10880 Loss: 0.06634264528157602\n",
            "Epoch: 363\n",
            "Batch: 0 Loss: 0.03274967671035197\n",
            "Batch: 320 Loss: 0.03556348388618017\n",
            "Batch: 640 Loss: 0.03552782611618899\n",
            "Batch: 960 Loss: 0.039432600851549454\n",
            "Batch: 1280 Loss: 0.030653352215045983\n",
            "Batch: 1600 Loss: 0.06494798122779259\n",
            "Batch: 1920 Loss: 0.0567498689666462\n",
            "Batch: 2240 Loss: 0.04563148283676035\n",
            "Batch: 2560 Loss: 0.04491011684255881\n",
            "Batch: 2880 Loss: 0.03548948241625076\n",
            "Batch: 3200 Loss: 0.047717160292448095\n",
            "Batch: 3520 Loss: 0.03508237602191914\n",
            "Batch: 3840 Loss: 0.04750374025144722\n",
            "Batch: 4160 Loss: 0.03314392269333524\n",
            "Batch: 4480 Loss: 0.03820578003996182\n",
            "Batch: 4800 Loss: 0.0351675505423101\n",
            "Batch: 5120 Loss: 0.044527847731448\n",
            "Batch: 5440 Loss: 0.04645585595039389\n",
            "Batch: 5760 Loss: 0.049816819489929054\n",
            "Batch: 6080 Loss: 0.03699805718102877\n",
            "Batch: 6400 Loss: 0.047106119859135494\n",
            "Batch: 6720 Loss: 0.05632173414280141\n",
            "Batch: 7040 Loss: 0.03818927677858781\n",
            "Batch: 7360 Loss: 0.044152921616679906\n",
            "Batch: 7680 Loss: 0.07006414114193912\n",
            "Batch: 8000 Loss: 0.05253477922762897\n",
            "Batch: 8320 Loss: 0.060053660630703845\n",
            "Batch: 8640 Loss: 0.0603649298058687\n",
            "Batch: 8960 Loss: 0.06100605059412892\n",
            "Batch: 9280 Loss: 0.02765553490854722\n",
            "Batch: 9600 Loss: 0.04900630772179181\n",
            "Batch: 9920 Loss: 0.04577286432244719\n",
            "Batch: 10240 Loss: 0.05895470553115987\n",
            "Batch: 10560 Loss: 0.029458009815643702\n",
            "Batch: 10880 Loss: 0.030441626768116938\n",
            "Epoch: 364\n",
            "Batch: 0 Loss: 0.04866042850614097\n",
            "Batch: 320 Loss: 0.07579336424411247\n",
            "Batch: 640 Loss: 0.04769676329673625\n",
            "Batch: 960 Loss: 0.04828638366872865\n",
            "Batch: 1280 Loss: 0.040972883822052754\n",
            "Batch: 1600 Loss: 0.06419468945371185\n",
            "Batch: 1920 Loss: 0.054075345047956726\n",
            "Batch: 2240 Loss: 0.05013254596598557\n",
            "Batch: 2560 Loss: 0.04105649135136188\n",
            "Batch: 2880 Loss: 0.04855131737201133\n",
            "Batch: 3200 Loss: 0.04721270079154233\n",
            "Batch: 3520 Loss: 0.041670823292562754\n",
            "Batch: 3840 Loss: 0.04343070591084869\n",
            "Batch: 4160 Loss: 0.05492614961690835\n",
            "Batch: 4480 Loss: 0.056262860001208816\n",
            "Batch: 4800 Loss: 0.053977784109052795\n",
            "Batch: 5120 Loss: 0.033876146960225235\n",
            "Batch: 5440 Loss: 0.055875277534090696\n",
            "Batch: 5760 Loss: 0.0438227491866396\n",
            "Batch: 6080 Loss: 0.04707794616005103\n",
            "Batch: 6400 Loss: 0.07826544823745854\n",
            "Batch: 6720 Loss: 0.05051950044874329\n",
            "Batch: 7040 Loss: 0.04162339840420245\n",
            "Batch: 7360 Loss: 0.038371182794697256\n",
            "Batch: 7680 Loss: 0.05154221986412358\n",
            "Batch: 8000 Loss: 0.04830284033207376\n",
            "Batch: 8320 Loss: 0.05239007437429053\n",
            "Batch: 8640 Loss: 0.03683023997121318\n",
            "Batch: 8960 Loss: 0.06527699627656935\n",
            "Batch: 9280 Loss: 0.06592241600511127\n",
            "Batch: 9600 Loss: 0.057181267420780266\n",
            "Batch: 9920 Loss: 0.05452762988186003\n",
            "Batch: 10240 Loss: 0.06398450630358042\n",
            "Batch: 10560 Loss: 0.03454507920578125\n",
            "Batch: 10880 Loss: 0.04037637954111826\n",
            "Epoch: 365\n",
            "Batch: 0 Loss: 0.04266402490716705\n",
            "Batch: 320 Loss: 0.05891099177706749\n",
            "Batch: 640 Loss: 0.0395610011701361\n",
            "Batch: 960 Loss: 0.04304205873538922\n",
            "Batch: 1280 Loss: 0.03406761691644204\n",
            "Batch: 1600 Loss: 0.037360959301569775\n",
            "Batch: 1920 Loss: 0.06929770090876104\n",
            "Batch: 2240 Loss: 0.038865866172527457\n",
            "Batch: 2560 Loss: 0.044089616751627016\n",
            "Batch: 2880 Loss: 0.05546469284800589\n",
            "Batch: 3200 Loss: 0.07546998555296593\n",
            "Batch: 3520 Loss: 0.03949830571611991\n",
            "Batch: 3840 Loss: 0.0472054987696941\n",
            "Batch: 4160 Loss: 0.06650300756579915\n",
            "Batch: 4480 Loss: 0.037030625080057\n",
            "Batch: 4800 Loss: 0.03949743953654661\n",
            "Batch: 5120 Loss: 0.0388690884449485\n",
            "Batch: 5440 Loss: 0.04844659814719301\n",
            "Batch: 5760 Loss: 0.03609314841349216\n",
            "Batch: 6080 Loss: 0.02740718409501323\n",
            "Batch: 6400 Loss: 0.03876008582233644\n",
            "Batch: 6720 Loss: 0.05705443562442636\n",
            "Batch: 7040 Loss: 0.029072658730183717\n",
            "Batch: 7360 Loss: 0.04082924990227623\n",
            "Batch: 7680 Loss: 0.0798114879779153\n",
            "Batch: 8000 Loss: 0.040457423363758474\n",
            "Batch: 8320 Loss: 0.036689198855290844\n",
            "Batch: 8640 Loss: 0.043199706271951584\n",
            "Batch: 8960 Loss: 0.043814433424273935\n",
            "Batch: 9280 Loss: 0.039181816853774705\n",
            "Batch: 9600 Loss: 0.028414144487537294\n",
            "Batch: 9920 Loss: 0.054126346863270794\n",
            "Batch: 10240 Loss: 0.042615370156474965\n",
            "Batch: 10560 Loss: 0.05640459312755893\n",
            "Batch: 10880 Loss: 0.030317047262573525\n",
            "Epoch: 366\n",
            "Batch: 0 Loss: 0.048295242908338004\n",
            "Batch: 320 Loss: 0.045272939351855106\n",
            "Batch: 640 Loss: 0.06093029347544584\n",
            "Batch: 960 Loss: 0.04210751281532499\n",
            "Batch: 1280 Loss: 0.032773584915430996\n",
            "Batch: 1600 Loss: 0.048049366017178075\n",
            "Batch: 1920 Loss: 0.05658891743326449\n",
            "Batch: 2240 Loss: 0.059434356088869975\n",
            "Batch: 2560 Loss: 0.04074741318792911\n",
            "Batch: 2880 Loss: 0.02958384035289444\n",
            "Batch: 3200 Loss: 0.036852931544669155\n",
            "Batch: 3520 Loss: 0.041542229150940635\n",
            "Batch: 3840 Loss: 0.03166322730161542\n",
            "Batch: 4160 Loss: 0.036881340946053755\n",
            "Batch: 4480 Loss: 0.03968869725745099\n",
            "Batch: 4800 Loss: 0.049381112341603146\n",
            "Batch: 5120 Loss: 0.04933007586981118\n",
            "Batch: 5440 Loss: 0.04856907789083409\n",
            "Batch: 5760 Loss: 0.06352372543547828\n",
            "Batch: 6080 Loss: 0.04737536438649316\n",
            "Batch: 6400 Loss: 0.05364475477102501\n",
            "Batch: 6720 Loss: 0.05157368847556613\n",
            "Batch: 7040 Loss: 0.0530680542718186\n",
            "Batch: 7360 Loss: 0.05265610511298481\n",
            "Batch: 7680 Loss: 0.047968844005704836\n",
            "Batch: 8000 Loss: 0.04186743077042503\n",
            "Batch: 8320 Loss: 0.057097872019673875\n",
            "Batch: 8640 Loss: 0.06489915096917605\n",
            "Batch: 8960 Loss: 0.035867229348103255\n",
            "Batch: 9280 Loss: 0.0449993109116691\n",
            "Batch: 9600 Loss: 0.08908479270215669\n",
            "Batch: 9920 Loss: 0.07352584592027124\n",
            "Batch: 10240 Loss: 0.04930641319818432\n",
            "Batch: 10560 Loss: 0.04417857338683978\n",
            "Batch: 10880 Loss: 0.045038161345286494\n",
            "Epoch: 367\n",
            "Batch: 0 Loss: 0.02850795296728981\n",
            "Batch: 320 Loss: 0.04592568781263826\n",
            "Batch: 640 Loss: 0.07782633928556525\n",
            "Batch: 960 Loss: 0.05261977600178406\n",
            "Batch: 1280 Loss: 0.0363189288422229\n",
            "Batch: 1600 Loss: 0.0622971340605862\n",
            "Batch: 1920 Loss: 0.04684375626668423\n",
            "Batch: 2240 Loss: 0.040921237519524645\n",
            "Batch: 2560 Loss: 0.04663420112731128\n",
            "Batch: 2880 Loss: 0.05701877951440547\n",
            "Batch: 3200 Loss: 0.03386326851249923\n",
            "Batch: 3520 Loss: 0.03245849966806597\n",
            "Batch: 3840 Loss: 0.044189546471545806\n",
            "Batch: 4160 Loss: 0.07268198249245357\n",
            "Batch: 4480 Loss: 0.039665478639768235\n",
            "Batch: 4800 Loss: 0.047719506971475485\n",
            "Batch: 5120 Loss: 0.04639033724229576\n",
            "Batch: 5440 Loss: 0.03564676502933326\n",
            "Batch: 5760 Loss: 0.04903356641303494\n",
            "Batch: 6080 Loss: 0.05106425667905952\n",
            "Batch: 6400 Loss: 0.0391198888764923\n",
            "Batch: 6720 Loss: 0.04861026871918797\n",
            "Batch: 7040 Loss: 0.04074648616812191\n",
            "Batch: 7360 Loss: 0.04748176174852183\n",
            "Batch: 7680 Loss: 0.06586772565372691\n",
            "Batch: 8000 Loss: 0.07297023219874742\n",
            "Batch: 8320 Loss: 0.050555291680080536\n",
            "Batch: 8640 Loss: 0.061508851216322546\n",
            "Batch: 8960 Loss: 0.03448691144007913\n",
            "Batch: 9280 Loss: 0.05223542516690574\n",
            "Batch: 9600 Loss: 0.06224027879416612\n",
            "Batch: 9920 Loss: 0.055405068646593086\n",
            "Batch: 10240 Loss: 0.0397610057866847\n",
            "Batch: 10560 Loss: 0.06304277394774963\n",
            "Batch: 10880 Loss: 0.030965666694875382\n",
            "Epoch: 368\n",
            "Batch: 0 Loss: 0.051244277319670455\n",
            "Batch: 320 Loss: 0.044749998733665836\n",
            "Batch: 640 Loss: 0.04017273092444311\n",
            "Batch: 960 Loss: 0.033009796740728674\n",
            "Batch: 1280 Loss: 0.04982795119759259\n",
            "Batch: 1600 Loss: 0.04448262047971805\n",
            "Batch: 1920 Loss: 0.055539169215624046\n",
            "Batch: 2240 Loss: 0.027473473763938692\n",
            "Batch: 2560 Loss: 0.05938663172064335\n",
            "Batch: 2880 Loss: 0.04428185264327958\n",
            "Batch: 3200 Loss: 0.04759913325537184\n",
            "Batch: 3520 Loss: 0.06297081909532046\n",
            "Batch: 3840 Loss: 0.07087045267846406\n",
            "Batch: 4160 Loss: 0.04565538493595401\n",
            "Batch: 4480 Loss: 0.04602065667608599\n",
            "Batch: 4800 Loss: 0.041178953949190286\n",
            "Batch: 5120 Loss: 0.06265889097678212\n",
            "Batch: 5440 Loss: 0.05705712533074414\n",
            "Batch: 5760 Loss: 0.047974472744484215\n",
            "Batch: 6080 Loss: 0.03638004064959899\n",
            "Batch: 6400 Loss: 0.04006699053691201\n",
            "Batch: 6720 Loss: 0.0780667030441877\n",
            "Batch: 7040 Loss: 0.04432334848317772\n",
            "Batch: 7360 Loss: 0.05261127725197999\n",
            "Batch: 7680 Loss: 0.04423476270622685\n",
            "Batch: 8000 Loss: 0.06196100057323932\n",
            "Batch: 8320 Loss: 0.06678988281679911\n",
            "Batch: 8640 Loss: 0.07727835933369885\n",
            "Batch: 8960 Loss: 0.03292008000653801\n",
            "Batch: 9280 Loss: 0.042174380394655035\n",
            "Batch: 9600 Loss: 0.04468680141293077\n",
            "Batch: 9920 Loss: 0.06809807194786216\n",
            "Batch: 10240 Loss: 0.07847734089851655\n",
            "Batch: 10560 Loss: 0.057825463123797737\n",
            "Batch: 10880 Loss: 0.04720970530759436\n",
            "Epoch: 369\n",
            "Batch: 0 Loss: 0.04841779572915895\n",
            "Batch: 320 Loss: 0.051159202293063394\n",
            "Batch: 640 Loss: 0.045500236803540525\n",
            "Batch: 960 Loss: 0.042651904043160216\n",
            "Batch: 1280 Loss: 0.04379149849907518\n",
            "Batch: 1600 Loss: 0.057036982279392044\n",
            "Batch: 1920 Loss: 0.05610761135518105\n",
            "Batch: 2240 Loss: 0.06444430265572686\n",
            "Batch: 2560 Loss: 0.04086995825108834\n",
            "Batch: 2880 Loss: 0.04519977144143844\n",
            "Batch: 3200 Loss: 0.03301359338582358\n",
            "Batch: 3520 Loss: 0.04520152296303871\n",
            "Batch: 3840 Loss: 0.04771079270370748\n",
            "Batch: 4160 Loss: 0.02901868397172666\n",
            "Batch: 4480 Loss: 0.05680383700610722\n",
            "Batch: 4800 Loss: 0.04270514115867046\n",
            "Batch: 5120 Loss: 0.059024828106860376\n",
            "Batch: 5440 Loss: 0.03462686881216932\n",
            "Batch: 5760 Loss: 0.03795766585399774\n",
            "Batch: 6080 Loss: 0.048122219345090984\n",
            "Batch: 6400 Loss: 0.027172865166536202\n",
            "Batch: 6720 Loss: 0.04272857606443582\n",
            "Batch: 7040 Loss: 0.037310090480766486\n",
            "Batch: 7360 Loss: 0.041140005144023606\n",
            "Batch: 7680 Loss: 0.06523370158632422\n",
            "Batch: 8000 Loss: 0.05724581827163754\n",
            "Batch: 8320 Loss: 0.04684615896009985\n",
            "Batch: 8640 Loss: 0.03561150367193844\n",
            "Batch: 8960 Loss: 0.0362672686062312\n",
            "Batch: 9280 Loss: 0.04460233631774008\n",
            "Batch: 9600 Loss: 0.05300272326492387\n",
            "Batch: 9920 Loss: 0.05415639081013832\n",
            "Batch: 10240 Loss: 0.042063297160112194\n",
            "Batch: 10560 Loss: 0.054257053917060795\n",
            "Batch: 10880 Loss: 0.05753005959759496\n",
            "Epoch: 370\n",
            "Batch: 0 Loss: 0.04756634552542538\n",
            "Batch: 320 Loss: 0.02659545512456605\n",
            "Batch: 640 Loss: 0.04136166717710955\n",
            "Batch: 960 Loss: 0.03470195696020653\n",
            "Batch: 1280 Loss: 0.07119939847694458\n",
            "Batch: 1600 Loss: 0.06038154401261657\n",
            "Batch: 1920 Loss: 0.036235933433822407\n",
            "Batch: 2240 Loss: 0.07292603073728121\n",
            "Batch: 2560 Loss: 0.045216145191589605\n",
            "Batch: 2880 Loss: 0.03844305873671511\n",
            "Batch: 3200 Loss: 0.047326213978366345\n",
            "Batch: 3520 Loss: 0.06870816896929403\n",
            "Batch: 3840 Loss: 0.06675802919448247\n",
            "Batch: 4160 Loss: 0.04810212825281081\n",
            "Batch: 4480 Loss: 0.028965116254160598\n",
            "Batch: 4800 Loss: 0.03998124311112188\n",
            "Batch: 5120 Loss: 0.032843355929754424\n",
            "Batch: 5440 Loss: 0.06382898534585182\n",
            "Batch: 5760 Loss: 0.029820617095925508\n",
            "Batch: 6080 Loss: 0.036074935658078414\n",
            "Batch: 6400 Loss: 0.04971790897829265\n",
            "Batch: 6720 Loss: 0.07262905880025744\n",
            "Batch: 7040 Loss: 0.03740045277154324\n",
            "Batch: 7360 Loss: 0.05389614097317556\n",
            "Batch: 7680 Loss: 0.03757615417352028\n",
            "Batch: 8000 Loss: 0.03485244808084189\n",
            "Batch: 8320 Loss: 0.06406780015864139\n",
            "Batch: 8640 Loss: 0.06520822944808663\n",
            "Batch: 8960 Loss: 0.05871552261858374\n",
            "Batch: 9280 Loss: 0.0443222044802825\n",
            "Batch: 9600 Loss: 0.042497049209859236\n",
            "Batch: 9920 Loss: 0.05054171665873633\n",
            "Batch: 10240 Loss: 0.062200320016173136\n",
            "Batch: 10560 Loss: 0.0530349370702833\n",
            "Batch: 10880 Loss: 0.05653259795204215\n",
            "Epoch: 371\n",
            "Batch: 0 Loss: 0.046743867427343516\n",
            "Batch: 320 Loss: 0.041826631134953024\n",
            "Batch: 640 Loss: 0.0534523037397228\n",
            "Batch: 960 Loss: 0.06020391583897991\n",
            "Batch: 1280 Loss: 0.04200405180041289\n",
            "Batch: 1600 Loss: 0.06875758796343799\n",
            "Batch: 1920 Loss: 0.0312913155010643\n",
            "Batch: 2240 Loss: 0.03686505093883318\n",
            "Batch: 2560 Loss: 0.046899374228202624\n",
            "Batch: 2880 Loss: 0.04306109556615545\n",
            "Batch: 3200 Loss: 0.04794742741818685\n",
            "Batch: 3520 Loss: 0.08052279410983111\n",
            "Batch: 3840 Loss: 0.07487944561146664\n",
            "Batch: 4160 Loss: 0.026572552579095035\n",
            "Batch: 4480 Loss: 0.05746467886851642\n",
            "Batch: 4800 Loss: 0.05159662462217709\n",
            "Batch: 5120 Loss: 0.0371474772026085\n",
            "Batch: 5440 Loss: 0.04340735757780913\n",
            "Batch: 5760 Loss: 0.06079134190874415\n",
            "Batch: 6080 Loss: 0.030479252512348995\n",
            "Batch: 6400 Loss: 0.04215011253128462\n",
            "Batch: 6720 Loss: 0.05581440462081333\n",
            "Batch: 7040 Loss: 0.05418008031236409\n",
            "Batch: 7360 Loss: 0.043277186812553166\n",
            "Batch: 7680 Loss: 0.04645905655601423\n",
            "Batch: 8000 Loss: 0.050159146037844535\n",
            "Batch: 8320 Loss: 0.05299029499747987\n",
            "Batch: 8640 Loss: 0.03416940960305488\n",
            "Batch: 8960 Loss: 0.04300517060860264\n",
            "Batch: 9280 Loss: 0.043092953459897665\n",
            "Batch: 9600 Loss: 0.04813966211429847\n",
            "Batch: 9920 Loss: 0.05275715974448994\n",
            "Batch: 10240 Loss: 0.03802144462886185\n",
            "Batch: 10560 Loss: 0.049218925589574594\n",
            "Batch: 10880 Loss: 0.05345681169449623\n",
            "Epoch: 372\n",
            "Batch: 0 Loss: 0.042285164778125404\n",
            "Batch: 320 Loss: 0.07350914265559556\n",
            "Batch: 640 Loss: 0.039662608063689916\n",
            "Batch: 960 Loss: 0.04176282464090837\n",
            "Batch: 1280 Loss: 0.03237344533659093\n",
            "Batch: 1600 Loss: 0.036678122885614\n",
            "Batch: 1920 Loss: 0.03971559056726263\n",
            "Batch: 2240 Loss: 0.039110625019831285\n",
            "Batch: 2560 Loss: 0.03472024993575162\n",
            "Batch: 2880 Loss: 0.05745318568476748\n",
            "Batch: 3200 Loss: 0.03132353573793889\n",
            "Batch: 3520 Loss: 0.03956343605884129\n",
            "Batch: 3840 Loss: 0.04067708699252581\n",
            "Batch: 4160 Loss: 0.05883637412381907\n",
            "Batch: 4480 Loss: 0.03304839358837405\n",
            "Batch: 4800 Loss: 0.05952199291990764\n",
            "Batch: 5120 Loss: 0.055367167777004525\n",
            "Batch: 5440 Loss: 0.05018941090556085\n",
            "Batch: 5760 Loss: 0.06291602789609912\n",
            "Batch: 6080 Loss: 0.05591404126268687\n",
            "Batch: 6400 Loss: 0.035154720885634094\n",
            "Batch: 6720 Loss: 0.041995787153893474\n",
            "Batch: 7040 Loss: 0.049059317630681124\n",
            "Batch: 7360 Loss: 0.045950550205156024\n",
            "Batch: 7680 Loss: 0.0525833744050926\n",
            "Batch: 8000 Loss: 0.04346771923064328\n",
            "Batch: 8320 Loss: 0.06745816379785247\n",
            "Batch: 8640 Loss: 0.06740135185856773\n",
            "Batch: 8960 Loss: 0.04180443324873605\n",
            "Batch: 9280 Loss: 0.04566000561705099\n",
            "Batch: 9600 Loss: 0.049792803929511964\n",
            "Batch: 9920 Loss: 0.05384296369856622\n",
            "Batch: 10240 Loss: 0.04790558021317008\n",
            "Batch: 10560 Loss: 0.07065206467954496\n",
            "Batch: 10880 Loss: 0.050085874870669164\n",
            "Epoch: 373\n",
            "Batch: 0 Loss: 0.05080179035717007\n",
            "Batch: 320 Loss: 0.047422366422443386\n",
            "Batch: 640 Loss: 0.04907775962120593\n",
            "Batch: 960 Loss: 0.03270925634200945\n",
            "Batch: 1280 Loss: 0.046976633356340294\n",
            "Batch: 1600 Loss: 0.05022147978505715\n",
            "Batch: 1920 Loss: 0.043138768868953535\n",
            "Batch: 2240 Loss: 0.04759316081759181\n",
            "Batch: 2560 Loss: 0.07055401487740477\n",
            "Batch: 2880 Loss: 0.04639942366993695\n",
            "Batch: 3200 Loss: 0.045404860315744325\n",
            "Batch: 3520 Loss: 0.06068662727333438\n",
            "Batch: 3840 Loss: 0.05679013528537513\n",
            "Batch: 4160 Loss: 0.08084773345953193\n",
            "Batch: 4480 Loss: 0.03792736296109905\n",
            "Batch: 4800 Loss: 0.03480855095535544\n",
            "Batch: 5120 Loss: 0.044231007431987206\n",
            "Batch: 5440 Loss: 0.07355808885516858\n",
            "Batch: 5760 Loss: 0.04326626002644835\n",
            "Batch: 6080 Loss: 0.048687530239207626\n",
            "Batch: 6400 Loss: 0.032105693710857384\n",
            "Batch: 6720 Loss: 0.04137002103296872\n",
            "Batch: 7040 Loss: 0.06071597308469215\n",
            "Batch: 7360 Loss: 0.0309916665882132\n",
            "Batch: 7680 Loss: 0.07304323828583648\n",
            "Batch: 8000 Loss: 0.059430310631564606\n",
            "Batch: 8320 Loss: 0.046116551058985385\n",
            "Batch: 8640 Loss: 0.06551453542247783\n",
            "Batch: 8960 Loss: 0.06682708354524902\n",
            "Batch: 9280 Loss: 0.03735653052218203\n",
            "Batch: 9600 Loss: 0.05222143686872504\n",
            "Batch: 9920 Loss: 0.06054549359402367\n",
            "Batch: 10240 Loss: 0.032827741195442836\n",
            "Batch: 10560 Loss: 0.049173133974341275\n",
            "Batch: 10880 Loss: 0.028353109624664245\n",
            "Epoch: 374\n",
            "Batch: 0 Loss: 0.04402806013756767\n",
            "Batch: 320 Loss: 0.0536578121073172\n",
            "Batch: 640 Loss: 0.05104792828225767\n",
            "Batch: 960 Loss: 0.04083566318657283\n",
            "Batch: 1280 Loss: 0.043424745697741435\n",
            "Batch: 1600 Loss: 0.043372042041541145\n",
            "Batch: 1920 Loss: 0.05395257412119183\n",
            "Batch: 2240 Loss: 0.043776982740461094\n",
            "Batch: 2560 Loss: 0.025857012806661715\n",
            "Batch: 2880 Loss: 0.046782780644119185\n",
            "Batch: 3200 Loss: 0.08465523652845626\n",
            "Batch: 3520 Loss: 0.04480968896706247\n",
            "Batch: 3840 Loss: 0.04467494156119669\n",
            "Batch: 4160 Loss: 0.030045395518896498\n",
            "Batch: 4480 Loss: 0.04394201659574081\n",
            "Batch: 4800 Loss: 0.05681398455251778\n",
            "Batch: 5120 Loss: 0.06760371405261005\n",
            "Batch: 5440 Loss: 0.03853756745855443\n",
            "Batch: 5760 Loss: 0.04856815992677681\n",
            "Batch: 6080 Loss: 0.06889080698964749\n",
            "Batch: 6400 Loss: 0.052664840742339555\n",
            "Batch: 6720 Loss: 0.025141968675518895\n",
            "Batch: 7040 Loss: 0.044836707972475756\n",
            "Batch: 7360 Loss: 0.059978611218852514\n",
            "Batch: 7680 Loss: 0.06143537623536103\n",
            "Batch: 8000 Loss: 0.04642271969265038\n",
            "Batch: 8320 Loss: 0.07478694746264836\n",
            "Batch: 8640 Loss: 0.07497083326687966\n",
            "Batch: 8960 Loss: 0.04327491805521821\n",
            "Batch: 9280 Loss: 0.042797947208419285\n",
            "Batch: 9600 Loss: 0.057683082669516855\n",
            "Batch: 9920 Loss: 0.03327122884776341\n",
            "Batch: 10240 Loss: 0.059280213170937776\n",
            "Batch: 10560 Loss: 0.07061667427054347\n",
            "Batch: 10880 Loss: 0.04389791187419829\n",
            "Epoch: 375\n",
            "Batch: 0 Loss: 0.03930212439887433\n",
            "Batch: 320 Loss: 0.058588605162312284\n",
            "Batch: 640 Loss: 0.033975243666537036\n",
            "Batch: 960 Loss: 0.05884666791562418\n",
            "Batch: 1280 Loss: 0.06973583625101241\n",
            "Batch: 1600 Loss: 0.04974375099823243\n",
            "Batch: 1920 Loss: 0.042723819196522926\n",
            "Batch: 2240 Loss: 0.05128645284341958\n",
            "Batch: 2560 Loss: 0.033976359351375426\n",
            "Batch: 2880 Loss: 0.05371741428625747\n",
            "Batch: 3200 Loss: 0.055567049568986414\n",
            "Batch: 3520 Loss: 0.07571382428417672\n",
            "Batch: 3840 Loss: 0.05070477257620071\n",
            "Batch: 4160 Loss: 0.07063698165487929\n",
            "Batch: 4480 Loss: 0.03713119497764253\n",
            "Batch: 4800 Loss: 0.08489182469137291\n",
            "Batch: 5120 Loss: 0.08976873690912426\n",
            "Batch: 5440 Loss: 0.06954992504659\n",
            "Batch: 5760 Loss: 0.043717069603047956\n",
            "Batch: 6080 Loss: 0.04889075361033544\n",
            "Batch: 6400 Loss: 0.03214635013514599\n",
            "Batch: 6720 Loss: 0.042095519186644906\n",
            "Batch: 7040 Loss: 0.04927401055399547\n",
            "Batch: 7360 Loss: 0.03145490388639654\n",
            "Batch: 7680 Loss: 0.03757846531825241\n",
            "Batch: 8000 Loss: 0.05397287181340041\n",
            "Batch: 8320 Loss: 0.05226986628264627\n",
            "Batch: 8640 Loss: 0.06166255271289231\n",
            "Batch: 8960 Loss: 0.03941221467912823\n",
            "Batch: 9280 Loss: 0.039615454300096255\n",
            "Batch: 9600 Loss: 0.02566065931379371\n",
            "Batch: 9920 Loss: 0.041017693179263064\n",
            "Batch: 10240 Loss: 0.028793758474759092\n",
            "Batch: 10560 Loss: 0.05633416255628222\n",
            "Batch: 10880 Loss: 0.024736091286986683\n",
            "Epoch: 376\n",
            "Batch: 0 Loss: 0.031209696706697727\n",
            "Batch: 320 Loss: 0.05550280223528921\n",
            "Batch: 640 Loss: 0.040261434988052916\n",
            "Batch: 960 Loss: 0.058334581518176394\n",
            "Batch: 1280 Loss: 0.05723167002100955\n",
            "Batch: 1600 Loss: 0.0440718999118426\n",
            "Batch: 1920 Loss: 0.05074732606302951\n",
            "Batch: 2240 Loss: 0.049519861504462304\n",
            "Batch: 2560 Loss: 0.04170498683172001\n",
            "Batch: 2880 Loss: 0.047778277314503986\n",
            "Batch: 3200 Loss: 0.05568205494781638\n",
            "Batch: 3520 Loss: 0.04670255692144175\n",
            "Batch: 3840 Loss: 0.03826852387615706\n",
            "Batch: 4160 Loss: 0.04634034252215473\n",
            "Batch: 4480 Loss: 0.025380758058459164\n",
            "Batch: 4800 Loss: 0.03755385910378678\n",
            "Batch: 5120 Loss: 0.03915194398275623\n",
            "Batch: 5440 Loss: 0.06917316295784569\n",
            "Batch: 5760 Loss: 0.06465404155998748\n",
            "Batch: 6080 Loss: 0.045028602681512514\n",
            "Batch: 6400 Loss: 0.03820790776061009\n",
            "Batch: 6720 Loss: 0.052966553817162906\n",
            "Batch: 7040 Loss: 0.051490147443711755\n",
            "Batch: 7360 Loss: 0.03025560073948097\n",
            "Batch: 7680 Loss: 0.05141355188060807\n",
            "Batch: 8000 Loss: 0.05562305434756614\n",
            "Batch: 8320 Loss: 0.0519803118166491\n",
            "Batch: 8640 Loss: 0.03504783001148875\n",
            "Batch: 8960 Loss: 0.03540800190794917\n",
            "Batch: 9280 Loss: 0.04224653625687057\n",
            "Batch: 9600 Loss: 0.033364136117050774\n",
            "Batch: 9920 Loss: 0.06662037547639645\n",
            "Batch: 10240 Loss: 0.06736024944986105\n",
            "Batch: 10560 Loss: 0.06266037931597879\n",
            "Batch: 10880 Loss: 0.03408336020488761\n",
            "Epoch: 377\n",
            "Batch: 0 Loss: 0.039210605072228746\n",
            "Batch: 320 Loss: 0.039685670608694774\n",
            "Batch: 640 Loss: 0.0456137786255565\n",
            "Batch: 960 Loss: 0.044279055139519276\n",
            "Batch: 1280 Loss: 0.043806129944312634\n",
            "Batch: 1600 Loss: 0.050822216995856595\n",
            "Batch: 1920 Loss: 0.07630509011464694\n",
            "Batch: 2240 Loss: 0.06846012775824348\n",
            "Batch: 2560 Loss: 0.0335938340280031\n",
            "Batch: 2880 Loss: 0.04616303722886317\n",
            "Batch: 3200 Loss: 0.03512941586342435\n",
            "Batch: 3520 Loss: 0.030799024006080124\n",
            "Batch: 3840 Loss: 0.041017190624946444\n",
            "Batch: 4160 Loss: 0.055816908142684266\n",
            "Batch: 4480 Loss: 0.05159939967939524\n",
            "Batch: 4800 Loss: 0.05329071747361912\n",
            "Batch: 5120 Loss: 0.06018384894766969\n",
            "Batch: 5440 Loss: 0.04055167658123332\n",
            "Batch: 5760 Loss: 0.03731156398678345\n",
            "Batch: 6080 Loss: 0.04606627740361594\n",
            "Batch: 6400 Loss: 0.07157410331052778\n",
            "Batch: 6720 Loss: 0.053273871182628434\n",
            "Batch: 7040 Loss: 0.06178006680349487\n",
            "Batch: 7360 Loss: 0.053053998714960166\n",
            "Batch: 7680 Loss: 0.029030517227649013\n",
            "Batch: 8000 Loss: 0.033270498901003906\n",
            "Batch: 8320 Loss: 0.03583982879444023\n",
            "Batch: 8640 Loss: 0.057088074484520475\n",
            "Batch: 8960 Loss: 0.042689669419178115\n",
            "Batch: 9280 Loss: 0.0694717402844284\n",
            "Batch: 9600 Loss: 0.04562502604916567\n",
            "Batch: 9920 Loss: 0.0480924273343716\n",
            "Batch: 10240 Loss: 0.046679433159411095\n",
            "Batch: 10560 Loss: 0.03700997149033108\n",
            "Batch: 10880 Loss: 0.04990705575976659\n",
            "Epoch: 378\n",
            "Batch: 0 Loss: 0.06080736734314998\n",
            "Batch: 320 Loss: 0.037773395193691064\n",
            "Batch: 640 Loss: 0.047561623623680424\n",
            "Batch: 960 Loss: 0.043184292472559185\n",
            "Batch: 1280 Loss: 0.051653138215154056\n",
            "Batch: 1600 Loss: 0.06233900095962147\n",
            "Batch: 1920 Loss: 0.032716603516733694\n",
            "Batch: 2240 Loss: 0.0362006118382913\n",
            "Batch: 2560 Loss: 0.03315234658509505\n",
            "Batch: 2880 Loss: 0.046857139691955346\n",
            "Batch: 3200 Loss: 0.04528878994347275\n",
            "Batch: 3520 Loss: 0.06404006968934708\n",
            "Batch: 3840 Loss: 0.04225604967885741\n",
            "Batch: 4160 Loss: 0.08799342169782051\n",
            "Batch: 4480 Loss: 0.04371927855625403\n",
            "Batch: 4800 Loss: 0.0455723353941236\n",
            "Batch: 5120 Loss: 0.044745302369946074\n",
            "Batch: 5440 Loss: 0.046945964790827244\n",
            "Batch: 5760 Loss: 0.03522309429162807\n",
            "Batch: 6080 Loss: 0.05377650144751165\n",
            "Batch: 6400 Loss: 0.0644859417983108\n",
            "Batch: 6720 Loss: 0.036774391951428065\n",
            "Batch: 7040 Loss: 0.08111281313369477\n",
            "Batch: 7360 Loss: 0.057296916431072334\n",
            "Batch: 7680 Loss: 0.04746440480539811\n",
            "Batch: 8000 Loss: 0.04620126149890732\n",
            "Batch: 8320 Loss: 0.03778794934529654\n",
            "Batch: 8640 Loss: 0.04952712087888304\n",
            "Batch: 8960 Loss: 0.04973868339755465\n",
            "Batch: 9280 Loss: 0.05336260222360116\n",
            "Batch: 9600 Loss: 0.03945480491113445\n",
            "Batch: 9920 Loss: 0.06288497921738144\n",
            "Batch: 10240 Loss: 0.060438946198186044\n",
            "Batch: 10560 Loss: 0.04838115487556372\n",
            "Batch: 10880 Loss: 0.05518308740212793\n",
            "Epoch: 379\n",
            "Batch: 0 Loss: 0.06013172130639071\n",
            "Batch: 320 Loss: 0.04763080270076771\n",
            "Batch: 640 Loss: 0.0322760019230884\n",
            "Batch: 960 Loss: 0.047508243467492425\n",
            "Batch: 1280 Loss: 0.051342299706675566\n",
            "Batch: 1600 Loss: 0.03970471901608855\n",
            "Batch: 1920 Loss: 0.04964709429246241\n",
            "Batch: 2240 Loss: 0.04304383318767503\n",
            "Batch: 2560 Loss: 0.04173231802038293\n",
            "Batch: 2880 Loss: 0.03294160892785937\n",
            "Batch: 3200 Loss: 0.053387571828050394\n",
            "Batch: 3520 Loss: 0.048603308920402835\n",
            "Batch: 3840 Loss: 0.07025470153516941\n",
            "Batch: 4160 Loss: 0.04316155203557905\n",
            "Batch: 4480 Loss: 0.046749889820860965\n",
            "Batch: 4800 Loss: 0.04701253265406035\n",
            "Batch: 5120 Loss: 0.04217973055778008\n",
            "Batch: 5440 Loss: 0.048607485464989185\n",
            "Batch: 5760 Loss: 0.03626865072541458\n",
            "Batch: 6080 Loss: 0.049357359223741716\n",
            "Batch: 6400 Loss: 0.04665812796949683\n",
            "Batch: 6720 Loss: 0.03873028987786831\n",
            "Batch: 7040 Loss: 0.04028753014267437\n",
            "Batch: 7360 Loss: 0.04266941364904587\n",
            "Batch: 7680 Loss: 0.04843126070745095\n",
            "Batch: 8000 Loss: 0.029978519300265776\n",
            "Batch: 8320 Loss: 0.03783136883015811\n",
            "Batch: 8640 Loss: 0.06586845499136759\n",
            "Batch: 8960 Loss: 0.036680004599123975\n",
            "Batch: 9280 Loss: 0.04638983516099812\n",
            "Batch: 9600 Loss: 0.05002105398009389\n",
            "Batch: 9920 Loss: 0.04341519157586311\n",
            "Batch: 10240 Loss: 0.06739216951679387\n",
            "Batch: 10560 Loss: 0.05712049644033653\n",
            "Batch: 10880 Loss: 0.04733171708426157\n",
            "Epoch: 380\n",
            "Batch: 0 Loss: 0.045792790655843595\n",
            "Batch: 320 Loss: 0.029376854000634434\n",
            "Batch: 640 Loss: 0.053517775260931294\n",
            "Batch: 960 Loss: 0.057282093461504896\n",
            "Batch: 1280 Loss: 0.042735847987473236\n",
            "Batch: 1600 Loss: 0.04127743868762667\n",
            "Batch: 1920 Loss: 0.040420734675335246\n",
            "Batch: 2240 Loss: 0.06257598541146334\n",
            "Batch: 2560 Loss: 0.06837050831283112\n",
            "Batch: 2880 Loss: 0.03321602632056928\n",
            "Batch: 3200 Loss: 0.06401070196713916\n",
            "Batch: 3520 Loss: 0.09253943987541886\n",
            "Batch: 3840 Loss: 0.05172098031455737\n",
            "Batch: 4160 Loss: 0.06288769982602743\n",
            "Batch: 4480 Loss: 0.04882954134305567\n",
            "Batch: 4800 Loss: 0.06921932230773162\n",
            "Batch: 5120 Loss: 0.06447611914007254\n",
            "Batch: 5440 Loss: 0.053868150027183384\n",
            "Batch: 5760 Loss: 0.043592703647549864\n",
            "Batch: 6080 Loss: 0.041496898018980814\n",
            "Batch: 6400 Loss: 0.029761862630600554\n",
            "Batch: 6720 Loss: 0.06672859586648099\n",
            "Batch: 7040 Loss: 0.05527849547715774\n",
            "Batch: 7360 Loss: 0.05107802506696097\n",
            "Batch: 7680 Loss: 0.054488936309017126\n",
            "Batch: 8000 Loss: 0.057846948454289115\n",
            "Batch: 8320 Loss: 0.04764398175319808\n",
            "Batch: 8640 Loss: 0.05654796110576937\n",
            "Batch: 8960 Loss: 0.04509031607116445\n",
            "Batch: 9280 Loss: 0.029116746869176646\n",
            "Batch: 9600 Loss: 0.046724003254016024\n",
            "Batch: 9920 Loss: 0.03783106490250625\n",
            "Batch: 10240 Loss: 0.035520823207818276\n",
            "Batch: 10560 Loss: 0.041834972697142855\n",
            "Batch: 10880 Loss: 0.027995462550326068\n",
            "Epoch: 381\n",
            "Batch: 0 Loss: 0.06287069749002498\n",
            "Batch: 320 Loss: 0.05293638601506577\n",
            "Batch: 640 Loss: 0.04532502867788895\n",
            "Batch: 960 Loss: 0.07368356044077087\n",
            "Batch: 1280 Loss: 0.04480224317824978\n",
            "Batch: 1600 Loss: 0.030922729975873347\n",
            "Batch: 1920 Loss: 0.07344825824199092\n",
            "Batch: 2240 Loss: 0.03637684276494457\n",
            "Batch: 2560 Loss: 0.06126626864695061\n",
            "Batch: 2880 Loss: 0.04190878229211594\n",
            "Batch: 3200 Loss: 0.044850995029284094\n",
            "Batch: 3520 Loss: 0.05559842197582374\n",
            "Batch: 3840 Loss: 0.04763011045189739\n",
            "Batch: 4160 Loss: 0.04991290349603921\n",
            "Batch: 4480 Loss: 0.055784170065987425\n",
            "Batch: 4800 Loss: 0.053380240893508096\n",
            "Batch: 5120 Loss: 0.046803867696314595\n",
            "Batch: 5440 Loss: 0.045464842057385164\n",
            "Batch: 5760 Loss: 0.05037039070234379\n",
            "Batch: 6080 Loss: 0.06652570469321828\n",
            "Batch: 6400 Loss: 0.0381556245548618\n",
            "Batch: 6720 Loss: 0.029203164422290127\n",
            "Batch: 7040 Loss: 0.032985589747559205\n",
            "Batch: 7360 Loss: 0.03648897033176387\n",
            "Batch: 7680 Loss: 0.04878930391594955\n",
            "Batch: 8000 Loss: 0.07155306588358983\n",
            "Batch: 8320 Loss: 0.04586620836051915\n",
            "Batch: 8640 Loss: 0.06486545276067343\n",
            "Batch: 8960 Loss: 0.07907194543759809\n",
            "Batch: 9280 Loss: 0.04657625377960266\n",
            "Batch: 9600 Loss: 0.06793821234496489\n",
            "Batch: 9920 Loss: 0.03799470856745835\n",
            "Batch: 10240 Loss: 0.05745234701192157\n",
            "Batch: 10560 Loss: 0.06362999382632194\n",
            "Batch: 10880 Loss: 0.06268603155349665\n",
            "Epoch: 382\n",
            "Batch: 0 Loss: 0.05294887012218762\n",
            "Batch: 320 Loss: 0.04923662536206379\n",
            "Batch: 640 Loss: 0.054634063659120484\n",
            "Batch: 960 Loss: 0.04756315011971014\n",
            "Batch: 1280 Loss: 0.040999185121428985\n",
            "Batch: 1600 Loss: 0.04070729943485787\n",
            "Batch: 1920 Loss: 0.08115587944787354\n",
            "Batch: 2240 Loss: 0.03712850397519751\n",
            "Batch: 2560 Loss: 0.044042215616923765\n",
            "Batch: 2880 Loss: 0.039075639084127076\n",
            "Batch: 3200 Loss: 0.04484700847356554\n",
            "Batch: 3520 Loss: 0.04658846500347406\n",
            "Batch: 3840 Loss: 0.0556709120364149\n",
            "Batch: 4160 Loss: 0.04742863343334899\n",
            "Batch: 4480 Loss: 0.050657520485801695\n",
            "Batch: 4800 Loss: 0.038841500420105535\n",
            "Batch: 5120 Loss: 0.04900970519882695\n",
            "Batch: 5440 Loss: 0.07178458578273884\n",
            "Batch: 5760 Loss: 0.05822532367223196\n",
            "Batch: 6080 Loss: 0.04512984695408349\n",
            "Batch: 6400 Loss: 0.04131259443729835\n",
            "Batch: 6720 Loss: 0.06462390203027474\n",
            "Batch: 7040 Loss: 0.058057236522120796\n",
            "Batch: 7360 Loss: 0.0767895565391763\n",
            "Batch: 7680 Loss: 0.0634705350795669\n",
            "Batch: 8000 Loss: 0.04946092085095193\n",
            "Batch: 8320 Loss: 0.0662527330453422\n",
            "Batch: 8640 Loss: 0.05621151842083341\n",
            "Batch: 8960 Loss: 0.045212618972666756\n",
            "Batch: 9280 Loss: 0.05432143671352212\n",
            "Batch: 9600 Loss: 0.0299282499590219\n",
            "Batch: 9920 Loss: 0.05193797983414078\n",
            "Batch: 10240 Loss: 0.03330759753251523\n",
            "Batch: 10560 Loss: 0.029914822952448752\n",
            "Batch: 10880 Loss: 0.055950169374808865\n",
            "Epoch: 383\n",
            "Batch: 0 Loss: 0.0454550949387529\n",
            "Batch: 320 Loss: 0.06476050335741262\n",
            "Batch: 640 Loss: 0.03369083575630462\n",
            "Batch: 960 Loss: 0.043293374520740086\n",
            "Batch: 1280 Loss: 0.028901801659126408\n",
            "Batch: 1600 Loss: 0.05004318029749849\n",
            "Batch: 1920 Loss: 0.04696465984813402\n",
            "Batch: 2240 Loss: 0.029770417551572212\n",
            "Batch: 2560 Loss: 0.02729943555570401\n",
            "Batch: 2880 Loss: 0.04439321420664669\n",
            "Batch: 3200 Loss: 0.03800514601982828\n",
            "Batch: 3520 Loss: 0.04425908371046089\n",
            "Batch: 3840 Loss: 0.04312237471635741\n",
            "Batch: 4160 Loss: 0.03219131440876803\n",
            "Batch: 4480 Loss: 0.054135054668921094\n",
            "Batch: 4800 Loss: 0.047450250887507484\n",
            "Batch: 5120 Loss: 0.04038573344666051\n",
            "Batch: 5440 Loss: 0.06849432769969485\n",
            "Batch: 5760 Loss: 0.04640667694667792\n",
            "Batch: 6080 Loss: 0.0406413517427256\n",
            "Batch: 6400 Loss: 0.06468008656445164\n",
            "Batch: 6720 Loss: 0.0447766680855197\n",
            "Batch: 7040 Loss: 0.02524368140664762\n",
            "Batch: 7360 Loss: 0.048768623684869214\n",
            "Batch: 7680 Loss: 0.08329522893973422\n",
            "Batch: 8000 Loss: 0.04254305144973879\n",
            "Batch: 8320 Loss: 0.030133012385720553\n",
            "Batch: 8640 Loss: 0.04109298549945408\n",
            "Batch: 8960 Loss: 0.039213978722897266\n",
            "Batch: 9280 Loss: 0.04951706977666539\n",
            "Batch: 9600 Loss: 0.03998269343769488\n",
            "Batch: 9920 Loss: 0.027962968675609587\n",
            "Batch: 10240 Loss: 0.047828610624850096\n",
            "Batch: 10560 Loss: 0.04681347649396008\n",
            "Batch: 10880 Loss: 0.04570834697145572\n",
            "Epoch: 384\n",
            "Batch: 0 Loss: 0.030612524555290965\n",
            "Batch: 320 Loss: 0.027854839480769628\n",
            "Batch: 640 Loss: 0.03477840448011625\n",
            "Batch: 960 Loss: 0.051223469850064304\n",
            "Batch: 1280 Loss: 0.062337113186551614\n",
            "Batch: 1600 Loss: 0.06224388808869936\n",
            "Batch: 1920 Loss: 0.054563687226431436\n",
            "Batch: 2240 Loss: 0.03586922693748597\n",
            "Batch: 2560 Loss: 0.041560182072706515\n",
            "Batch: 2880 Loss: 0.06041739611758127\n",
            "Batch: 3200 Loss: 0.06324626853333098\n",
            "Batch: 3520 Loss: 0.05250963157644074\n",
            "Batch: 3840 Loss: 0.05938899249579918\n",
            "Batch: 4160 Loss: 0.0336072021633321\n",
            "Batch: 4480 Loss: 0.06724643019860016\n",
            "Batch: 4800 Loss: 0.05652337842066338\n",
            "Batch: 5120 Loss: 0.04109101379206688\n",
            "Batch: 5440 Loss: 0.054601763725860925\n",
            "Batch: 5760 Loss: 0.04462119926327146\n",
            "Batch: 6080 Loss: 0.030525973724432644\n",
            "Batch: 6400 Loss: 0.05405994661642322\n",
            "Batch: 6720 Loss: 0.044666289407261174\n",
            "Batch: 7040 Loss: 0.04313346583370187\n",
            "Batch: 7360 Loss: 0.03930288226654409\n",
            "Batch: 7680 Loss: 0.05561431468568846\n",
            "Batch: 8000 Loss: 0.040670197868690736\n",
            "Batch: 8320 Loss: 0.047820975109025964\n",
            "Batch: 8640 Loss: 0.03328194812922959\n",
            "Batch: 8960 Loss: 0.06702928898090839\n",
            "Batch: 9280 Loss: 0.03611411086810899\n",
            "Batch: 9600 Loss: 0.03974633682522999\n",
            "Batch: 9920 Loss: 0.05621424284559759\n",
            "Batch: 10240 Loss: 0.05582981269093464\n",
            "Batch: 10560 Loss: 0.044951981995939465\n",
            "Batch: 10880 Loss: 0.06372380960972418\n",
            "Epoch: 385\n",
            "Batch: 0 Loss: 0.03850746243655785\n",
            "Batch: 320 Loss: 0.034797536370793636\n",
            "Batch: 640 Loss: 0.03734947298254545\n",
            "Batch: 960 Loss: 0.04399238282825689\n",
            "Batch: 1280 Loss: 0.04923414321008463\n",
            "Batch: 1600 Loss: 0.05692185063103264\n",
            "Batch: 1920 Loss: 0.05100652141460429\n",
            "Batch: 2240 Loss: 0.025983235588872892\n",
            "Batch: 2560 Loss: 0.0531115972490661\n",
            "Batch: 2880 Loss: 0.02435539731201678\n",
            "Batch: 3200 Loss: 0.05414190852999717\n",
            "Batch: 3520 Loss: 0.04350017242090672\n",
            "Batch: 3840 Loss: 0.05338367860868625\n",
            "Batch: 4160 Loss: 0.05987895815232979\n",
            "Batch: 4480 Loss: 0.0382829927035404\n",
            "Batch: 4800 Loss: 0.043385421814680934\n",
            "Batch: 5120 Loss: 0.03694411085944001\n",
            "Batch: 5440 Loss: 0.036799575164845194\n",
            "Batch: 5760 Loss: 0.03466542454064519\n",
            "Batch: 6080 Loss: 0.06479766637475912\n",
            "Batch: 6400 Loss: 0.053144231562618965\n",
            "Batch: 6720 Loss: 0.04651541268509361\n",
            "Batch: 7040 Loss: 0.030125689122983745\n",
            "Batch: 7360 Loss: 0.0336675786656334\n",
            "Batch: 7680 Loss: 0.04810726609199941\n",
            "Batch: 8000 Loss: 0.03572079132403638\n",
            "Batch: 8320 Loss: 0.036624976326338696\n",
            "Batch: 8640 Loss: 0.050629128249418266\n",
            "Batch: 8960 Loss: 0.027616644158773462\n",
            "Batch: 9280 Loss: 0.04886557182582291\n",
            "Batch: 9600 Loss: 0.04442596830178311\n",
            "Batch: 9920 Loss: 0.038191325349418305\n",
            "Batch: 10240 Loss: 0.07468343694641147\n",
            "Batch: 10560 Loss: 0.02944912385293436\n",
            "Batch: 10880 Loss: 0.05930285147974215\n",
            "Epoch: 386\n",
            "Batch: 0 Loss: 0.05832357578617713\n",
            "Batch: 320 Loss: 0.06239579505955468\n",
            "Batch: 640 Loss: 0.045923180158783286\n",
            "Batch: 960 Loss: 0.04167002084884544\n",
            "Batch: 1280 Loss: 0.0341044179436621\n",
            "Batch: 1600 Loss: 0.04310603598170455\n",
            "Batch: 1920 Loss: 0.0784734931012759\n",
            "Batch: 2240 Loss: 0.038855771318051055\n",
            "Batch: 2560 Loss: 0.05376643776899454\n",
            "Batch: 2880 Loss: 0.031170606297965316\n",
            "Batch: 3200 Loss: 0.03485006705436645\n",
            "Batch: 3520 Loss: 0.02841159166266985\n",
            "Batch: 3840 Loss: 0.03020284407753929\n",
            "Batch: 4160 Loss: 0.04345608266946104\n",
            "Batch: 4480 Loss: 0.04628716561777282\n",
            "Batch: 4800 Loss: 0.056632524087028846\n",
            "Batch: 5120 Loss: 0.05326647387911577\n",
            "Batch: 5440 Loss: 0.06771564487805266\n",
            "Batch: 5760 Loss: 0.047016993780887584\n",
            "Batch: 6080 Loss: 0.052727357792902906\n",
            "Batch: 6400 Loss: 0.0252238511136379\n",
            "Batch: 6720 Loss: 0.02923680076067451\n",
            "Batch: 7040 Loss: 0.04857157695755101\n",
            "Batch: 7360 Loss: 0.041766382304726506\n",
            "Batch: 7680 Loss: 0.05350789762972462\n",
            "Batch: 8000 Loss: 0.04220480977792155\n",
            "Batch: 8320 Loss: 0.05070976408434832\n",
            "Batch: 8640 Loss: 0.028622375656649428\n",
            "Batch: 8960 Loss: 0.05225297155940685\n",
            "Batch: 9280 Loss: 0.04201011511822814\n",
            "Batch: 9600 Loss: 0.03596987805883586\n",
            "Batch: 9920 Loss: 0.034814226194671456\n",
            "Batch: 10240 Loss: 0.03221100221714677\n",
            "Batch: 10560 Loss: 0.05381537060520254\n",
            "Batch: 10880 Loss: 0.08838107265026532\n",
            "Epoch: 387\n",
            "Batch: 0 Loss: 0.04596809537813991\n",
            "Batch: 320 Loss: 0.039696065506637485\n",
            "Batch: 640 Loss: 0.07127357752967821\n",
            "Batch: 960 Loss: 0.04290538104572455\n",
            "Batch: 1280 Loss: 0.0496476106205067\n",
            "Batch: 1600 Loss: 0.04359855983884554\n",
            "Batch: 1920 Loss: 0.04449752027080816\n",
            "Batch: 2240 Loss: 0.035975879375059125\n",
            "Batch: 2560 Loss: 0.04376523351842531\n",
            "Batch: 2880 Loss: 0.04414228013817821\n",
            "Batch: 3200 Loss: 0.06654407414396961\n",
            "Batch: 3520 Loss: 0.06127462974497334\n",
            "Batch: 3840 Loss: 0.047852114687165805\n",
            "Batch: 4160 Loss: 0.031155104046524426\n",
            "Batch: 4480 Loss: 0.046128527506082956\n",
            "Batch: 4800 Loss: 0.055966557394483044\n",
            "Batch: 5120 Loss: 0.028058615437792055\n",
            "Batch: 5440 Loss: 0.03976727419479707\n",
            "Batch: 5760 Loss: 0.04504848776427959\n",
            "Batch: 6080 Loss: 0.041075144794922\n",
            "Batch: 6400 Loss: 0.035675734631215344\n",
            "Batch: 6720 Loss: 0.054094759330858534\n",
            "Batch: 7040 Loss: 0.037111859183736905\n",
            "Batch: 7360 Loss: 0.05186655755343726\n",
            "Batch: 7680 Loss: 0.09022248534512828\n",
            "Batch: 8000 Loss: 0.03543132861768554\n",
            "Batch: 8320 Loss: 0.04724408641292831\n",
            "Batch: 8640 Loss: 0.04696334720663885\n",
            "Batch: 8960 Loss: 0.03938246120430529\n",
            "Batch: 9280 Loss: 0.05801045701952378\n",
            "Batch: 9600 Loss: 0.039557687005080125\n",
            "Batch: 9920 Loss: 0.0641608783319999\n",
            "Batch: 10240 Loss: 0.03650681031006741\n",
            "Batch: 10560 Loss: 0.0645053942563043\n",
            "Batch: 10880 Loss: 0.04066055825187151\n",
            "Epoch: 388\n",
            "Batch: 0 Loss: 0.05077369162344476\n",
            "Batch: 320 Loss: 0.047818855812505466\n",
            "Batch: 640 Loss: 0.04363558199634676\n",
            "Batch: 960 Loss: 0.06664665433427526\n",
            "Batch: 1280 Loss: 0.06727591917771278\n",
            "Batch: 1600 Loss: 0.030220709575132863\n",
            "Batch: 1920 Loss: 0.042385069958209995\n",
            "Batch: 2240 Loss: 0.04156498210911752\n",
            "Batch: 2560 Loss: 0.0611198990950222\n",
            "Batch: 2880 Loss: 0.040778020774997356\n",
            "Batch: 3200 Loss: 0.029171135777663504\n",
            "Batch: 3520 Loss: 0.05342625687262916\n",
            "Batch: 3840 Loss: 0.06761100346003009\n",
            "Batch: 4160 Loss: 0.06314790220256045\n",
            "Batch: 4480 Loss: 0.05594144951622845\n",
            "Batch: 4800 Loss: 0.0613500789669635\n",
            "Batch: 5120 Loss: 0.04476912817871791\n",
            "Batch: 5440 Loss: 0.05919164984979346\n",
            "Batch: 5760 Loss: 0.056413064005144814\n",
            "Batch: 6080 Loss: 0.03436930100782764\n",
            "Batch: 6400 Loss: 0.05157910102856841\n",
            "Batch: 6720 Loss: 0.045430666098526126\n",
            "Batch: 7040 Loss: 0.06044863851965267\n",
            "Batch: 7360 Loss: 0.06008249609976255\n",
            "Batch: 7680 Loss: 0.04132517742210514\n",
            "Batch: 8000 Loss: 0.030884188680802938\n",
            "Batch: 8320 Loss: 0.05926979463810279\n",
            "Batch: 8640 Loss: 0.03421651448661544\n",
            "Batch: 8960 Loss: 0.04395777072561429\n",
            "Batch: 9280 Loss: 0.054853116133654393\n",
            "Batch: 9600 Loss: 0.040699707195107154\n",
            "Batch: 9920 Loss: 0.037437042589739636\n",
            "Batch: 10240 Loss: 0.04020375670087229\n",
            "Batch: 10560 Loss: 0.04064097843828416\n",
            "Batch: 10880 Loss: 0.046965278111641064\n",
            "Epoch: 389\n",
            "Batch: 0 Loss: 0.05464013608494845\n",
            "Batch: 320 Loss: 0.033526794051539834\n",
            "Batch: 640 Loss: 0.04698383480503004\n",
            "Batch: 960 Loss: 0.05053884114497979\n",
            "Batch: 1280 Loss: 0.035303363365636525\n",
            "Batch: 1600 Loss: 0.04490200220317339\n",
            "Batch: 1920 Loss: 0.06637717004964065\n",
            "Batch: 2240 Loss: 0.048653174089459926\n",
            "Batch: 2560 Loss: 0.041581561677947576\n",
            "Batch: 2880 Loss: 0.06067650767894231\n",
            "Batch: 3200 Loss: 0.04836769574055016\n",
            "Batch: 3520 Loss: 0.074325086339317\n",
            "Batch: 3840 Loss: 0.044755849187171605\n",
            "Batch: 4160 Loss: 0.060563271247943876\n",
            "Batch: 4480 Loss: 0.04544286357446677\n",
            "Batch: 4800 Loss: 0.04712084857017204\n",
            "Batch: 5120 Loss: 0.060527081984557765\n",
            "Batch: 5440 Loss: 0.0700906424417895\n",
            "Batch: 5760 Loss: 0.035743804411156654\n",
            "Batch: 6080 Loss: 0.06884605388925981\n",
            "Batch: 6400 Loss: 0.026900329525630696\n",
            "Batch: 6720 Loss: 0.04397605600118263\n",
            "Batch: 7040 Loss: 0.04027661264973269\n",
            "Batch: 7360 Loss: 0.04792560490510097\n",
            "Batch: 7680 Loss: 0.04125375511683309\n",
            "Batch: 8000 Loss: 0.05345773214281256\n",
            "Batch: 8320 Loss: 0.04999698684834152\n",
            "Batch: 8640 Loss: 0.049486475390609894\n",
            "Batch: 8960 Loss: 0.028886363658577698\n",
            "Batch: 9280 Loss: 0.04563909066242162\n",
            "Batch: 9600 Loss: 0.035331891310782286\n",
            "Batch: 9920 Loss: 0.041527353291065966\n",
            "Batch: 10240 Loss: 0.040973677047529564\n",
            "Batch: 10560 Loss: 0.03222502486320635\n",
            "Batch: 10880 Loss: 0.0566503478490569\n",
            "Epoch: 390\n",
            "Batch: 0 Loss: 0.03229129245694184\n",
            "Batch: 320 Loss: 0.03996604942956997\n",
            "Batch: 640 Loss: 0.04546173596943103\n",
            "Batch: 960 Loss: 0.06649021659094939\n",
            "Batch: 1280 Loss: 0.05605092630697385\n",
            "Batch: 1600 Loss: 0.03242057408413298\n",
            "Batch: 1920 Loss: 0.05348268571960439\n",
            "Batch: 2240 Loss: 0.05806019842726018\n",
            "Batch: 2560 Loss: 0.046485973034965745\n",
            "Batch: 2880 Loss: 0.04029535121467966\n",
            "Batch: 3200 Loss: 0.04355579211857941\n",
            "Batch: 3520 Loss: 0.03375939310418547\n",
            "Batch: 3840 Loss: 0.06317020169280724\n",
            "Batch: 4160 Loss: 0.04895497274015743\n",
            "Batch: 4480 Loss: 0.028514340322292622\n",
            "Batch: 4800 Loss: 0.05156966753282982\n",
            "Batch: 5120 Loss: 0.03541596911482347\n",
            "Batch: 5440 Loss: 0.049077392809387915\n",
            "Batch: 5760 Loss: 0.057542509139040306\n",
            "Batch: 6080 Loss: 0.035808296193973396\n",
            "Batch: 6400 Loss: 0.027152061468311776\n",
            "Batch: 6720 Loss: 0.06169018255357352\n",
            "Batch: 7040 Loss: 0.07759223542752361\n",
            "Batch: 7360 Loss: 0.0392014606919311\n",
            "Batch: 7680 Loss: 0.03921398359856147\n",
            "Batch: 8000 Loss: 0.059982336850811974\n",
            "Batch: 8320 Loss: 0.035937794218963044\n",
            "Batch: 8640 Loss: 0.06495546110925927\n",
            "Batch: 8960 Loss: 0.05675081529361645\n",
            "Batch: 9280 Loss: 0.045684055731752\n",
            "Batch: 9600 Loss: 0.033353466575019884\n",
            "Batch: 9920 Loss: 0.05208719321285708\n",
            "Batch: 10240 Loss: 0.055777359318062054\n",
            "Batch: 10560 Loss: 0.09538840464154463\n",
            "Batch: 10880 Loss: 0.02874037215573339\n",
            "Epoch: 391\n",
            "Batch: 0 Loss: 0.035630253874453253\n",
            "Batch: 320 Loss: 0.03437968465137489\n",
            "Batch: 640 Loss: 0.027614313753911077\n",
            "Batch: 960 Loss: 0.05045968699560833\n",
            "Batch: 1280 Loss: 0.037780603387695115\n",
            "Batch: 1600 Loss: 0.04044924017117813\n",
            "Batch: 1920 Loss: 0.04928159586829222\n",
            "Batch: 2240 Loss: 0.025859248907664154\n",
            "Batch: 2560 Loss: 0.061646475989863905\n",
            "Batch: 2880 Loss: 0.05062421737767206\n",
            "Batch: 3200 Loss: 0.047684583357365744\n",
            "Batch: 3520 Loss: 0.033203392962381395\n",
            "Batch: 3840 Loss: 0.04160779849110884\n",
            "Batch: 4160 Loss: 0.044722417592923606\n",
            "Batch: 4480 Loss: 0.0502217366505657\n",
            "Batch: 4800 Loss: 0.05457270247768601\n",
            "Batch: 5120 Loss: 0.02860985060087191\n",
            "Batch: 5440 Loss: 0.06259890837168126\n",
            "Batch: 5760 Loss: 0.08143933747551975\n",
            "Batch: 6080 Loss: 0.08061027477127819\n",
            "Batch: 6400 Loss: 0.06403091180562928\n",
            "Batch: 6720 Loss: 0.051921622679394326\n",
            "Batch: 7040 Loss: 0.05590993823408042\n",
            "Batch: 7360 Loss: 0.028216247402383134\n",
            "Batch: 7680 Loss: 0.04847547812532658\n",
            "Batch: 8000 Loss: 0.03577851034575337\n",
            "Batch: 8320 Loss: 0.040593625212579906\n",
            "Batch: 8640 Loss: 0.05434527029143412\n",
            "Batch: 8960 Loss: 0.05761099371882021\n",
            "Batch: 9280 Loss: 0.04345119917520373\n",
            "Batch: 9600 Loss: 0.0490508148057137\n",
            "Batch: 9920 Loss: 0.03378429564095118\n",
            "Batch: 10240 Loss: 0.053740645785933856\n",
            "Batch: 10560 Loss: 0.04863680611026461\n",
            "Batch: 10880 Loss: 0.03004531358086302\n",
            "Epoch: 392\n",
            "Batch: 0 Loss: 0.04230743840055888\n",
            "Batch: 320 Loss: 0.05189595406436379\n",
            "Batch: 640 Loss: 0.031592365148885054\n",
            "Batch: 960 Loss: 0.0400749397305605\n",
            "Batch: 1280 Loss: 0.052516031238214536\n",
            "Batch: 1600 Loss: 0.04132728327159286\n",
            "Batch: 1920 Loss: 0.04055960772583485\n",
            "Batch: 2240 Loss: 0.024434750509562154\n",
            "Batch: 2560 Loss: 0.043869623852505174\n",
            "Batch: 2880 Loss: 0.04572590403848332\n",
            "Batch: 3200 Loss: 0.03556426833973409\n",
            "Batch: 3520 Loss: 0.07586088104031234\n",
            "Batch: 3840 Loss: 0.03105026181256245\n",
            "Batch: 4160 Loss: 0.05007106628336565\n",
            "Batch: 4480 Loss: 0.04459871295991084\n",
            "Batch: 4800 Loss: 0.05491928484826095\n",
            "Batch: 5120 Loss: 0.041717099218217875\n",
            "Batch: 5440 Loss: 0.043904923714037924\n",
            "Batch: 5760 Loss: 0.04079058173674348\n",
            "Batch: 6080 Loss: 0.03834947314031825\n",
            "Batch: 6400 Loss: 0.033906462851380374\n",
            "Batch: 6720 Loss: 0.04005334047065792\n",
            "Batch: 7040 Loss: 0.07068731600168178\n",
            "Batch: 7360 Loss: 0.06448305871325585\n",
            "Batch: 7680 Loss: 0.042412907656719186\n",
            "Batch: 8000 Loss: 0.06335612300924628\n",
            "Batch: 8320 Loss: 0.044327034222828944\n",
            "Batch: 8640 Loss: 0.03406982830133504\n",
            "Batch: 8960 Loss: 0.04898369835767843\n",
            "Batch: 9280 Loss: 0.04213670717184064\n",
            "Batch: 9600 Loss: 0.0704609630056214\n",
            "Batch: 9920 Loss: 0.04406465393207677\n",
            "Batch: 10240 Loss: 0.0820868863708535\n",
            "Batch: 10560 Loss: 0.03540387887684989\n",
            "Batch: 10880 Loss: 0.050440714433343\n",
            "Epoch: 393\n",
            "Batch: 0 Loss: 0.03375078562655132\n",
            "Batch: 320 Loss: 0.049814765360552994\n",
            "Batch: 640 Loss: 0.05128885465971047\n",
            "Batch: 960 Loss: 0.054088783001050855\n",
            "Batch: 1280 Loss: 0.04135935936742815\n",
            "Batch: 1600 Loss: 0.0556018602626168\n",
            "Batch: 1920 Loss: 0.04888441769293079\n",
            "Batch: 2240 Loss: 0.06181123860990461\n",
            "Batch: 2560 Loss: 0.06046783271905627\n",
            "Batch: 2880 Loss: 0.06249950500009764\n",
            "Batch: 3200 Loss: 0.041989426531075805\n",
            "Batch: 3520 Loss: 0.06048745181999243\n",
            "Batch: 3840 Loss: 0.06284750511056415\n",
            "Batch: 4160 Loss: 0.0554739027203569\n",
            "Batch: 4480 Loss: 0.050293333040068534\n",
            "Batch: 4800 Loss: 0.04099931789265611\n",
            "Batch: 5120 Loss: 0.08762049786333748\n",
            "Batch: 5440 Loss: 0.04695303306331631\n",
            "Batch: 5760 Loss: 0.02622874727003204\n",
            "Batch: 6080 Loss: 0.030188995930101485\n",
            "Batch: 6400 Loss: 0.04619698596141428\n",
            "Batch: 6720 Loss: 0.04856369166710156\n",
            "Batch: 7040 Loss: 0.04496305611983169\n",
            "Batch: 7360 Loss: 0.05880139795739986\n",
            "Batch: 7680 Loss: 0.03428450321502516\n",
            "Batch: 8000 Loss: 0.03398006332403778\n",
            "Batch: 8320 Loss: 0.03551556267478959\n",
            "Batch: 8640 Loss: 0.03785283543016193\n",
            "Batch: 8960 Loss: 0.03881717353262237\n",
            "Batch: 9280 Loss: 0.043602632695290834\n",
            "Batch: 9600 Loss: 0.058266757903985035\n",
            "Batch: 9920 Loss: 0.034110319358212134\n",
            "Batch: 10240 Loss: 0.0543451889087398\n",
            "Batch: 10560 Loss: 0.04916941376917419\n",
            "Batch: 10880 Loss: 0.06635746378052583\n",
            "Epoch: 394\n",
            "Batch: 0 Loss: 0.05638209652242794\n",
            "Batch: 320 Loss: 0.05708320309413047\n",
            "Batch: 640 Loss: 0.04858588092939616\n",
            "Batch: 960 Loss: 0.0620899158082618\n",
            "Batch: 1280 Loss: 0.044541788239864794\n",
            "Batch: 1600 Loss: 0.05529399162266831\n",
            "Batch: 1920 Loss: 0.06750029751300292\n",
            "Batch: 2240 Loss: 0.042263950027229484\n",
            "Batch: 2560 Loss: 0.04157023461025473\n",
            "Batch: 2880 Loss: 0.045053543751531776\n",
            "Batch: 3200 Loss: 0.06316103850863596\n",
            "Batch: 3520 Loss: 0.046680301436600125\n",
            "Batch: 3840 Loss: 0.05297583890360988\n",
            "Batch: 4160 Loss: 0.06510949621440427\n",
            "Batch: 4480 Loss: 0.047510561091080886\n",
            "Batch: 4800 Loss: 0.06664127937176327\n",
            "Batch: 5120 Loss: 0.03950924695288956\n",
            "Batch: 5440 Loss: 0.04881183909925425\n",
            "Batch: 5760 Loss: 0.044223225791486485\n",
            "Batch: 6080 Loss: 0.0441650278770322\n",
            "Batch: 6400 Loss: 0.053291310229536995\n",
            "Batch: 6720 Loss: 0.03239769847800357\n",
            "Batch: 7040 Loss: 0.04461423891750873\n",
            "Batch: 7360 Loss: 0.054922166710287934\n",
            "Batch: 7680 Loss: 0.0572483795997491\n",
            "Batch: 8000 Loss: 0.054106389381588335\n",
            "Batch: 8320 Loss: 0.04051675473346448\n",
            "Batch: 8640 Loss: 0.09207146410240222\n",
            "Batch: 8960 Loss: 0.04103347767766611\n",
            "Batch: 9280 Loss: 0.043074960404209765\n",
            "Batch: 9600 Loss: 0.033146898027740644\n",
            "Batch: 9920 Loss: 0.044970675444244025\n",
            "Batch: 10240 Loss: 0.03812554753661185\n",
            "Batch: 10560 Loss: 0.043053382012456225\n",
            "Batch: 10880 Loss: 0.02445601903289337\n",
            "Epoch: 395\n",
            "Batch: 0 Loss: 0.058556845074250276\n",
            "Batch: 320 Loss: 0.05656387148914597\n",
            "Batch: 640 Loss: 0.04672641630539038\n",
            "Batch: 960 Loss: 0.06338064712068554\n",
            "Batch: 1280 Loss: 0.0538965605841571\n",
            "Batch: 1600 Loss: 0.06027183078613241\n",
            "Batch: 1920 Loss: 0.08107671361287196\n",
            "Batch: 2240 Loss: 0.04686941209282199\n",
            "Batch: 2560 Loss: 0.05499246837010483\n",
            "Batch: 2880 Loss: 0.07474410709125777\n",
            "Batch: 3200 Loss: 0.07040043610633469\n",
            "Batch: 3520 Loss: 0.05586157882646633\n",
            "Batch: 3840 Loss: 0.07783281525432671\n",
            "Batch: 4160 Loss: 0.03318008728080386\n",
            "Batch: 4480 Loss: 0.05854148957197509\n",
            "Batch: 4800 Loss: 0.04079924130731749\n",
            "Batch: 5120 Loss: 0.031537702012419115\n",
            "Batch: 5440 Loss: 0.0582359699518968\n",
            "Batch: 5760 Loss: 0.04073671832978688\n",
            "Batch: 6080 Loss: 0.037771273727937885\n",
            "Batch: 6400 Loss: 0.04340565495459113\n",
            "Batch: 6720 Loss: 0.05524747432388857\n",
            "Batch: 7040 Loss: 0.045914528062630015\n",
            "Batch: 7360 Loss: 0.056910400596219414\n",
            "Batch: 7680 Loss: 0.03359073853905374\n",
            "Batch: 8000 Loss: 0.06709598798280157\n",
            "Batch: 8320 Loss: 0.03568981907633327\n",
            "Batch: 8640 Loss: 0.04112484036562582\n",
            "Batch: 8960 Loss: 0.04138928257832194\n",
            "Batch: 9280 Loss: 0.06864732175872894\n",
            "Batch: 9600 Loss: 0.04851333887359208\n",
            "Batch: 9920 Loss: 0.05605521866386138\n",
            "Batch: 10240 Loss: 0.04204721362111575\n",
            "Batch: 10560 Loss: 0.05214557059597049\n",
            "Batch: 10880 Loss: 0.07430557962725282\n",
            "Epoch: 396\n",
            "Batch: 0 Loss: 0.06565708115453912\n",
            "Batch: 320 Loss: 0.05771038281431185\n",
            "Batch: 640 Loss: 0.0470808934526825\n",
            "Batch: 960 Loss: 0.031204517158126364\n",
            "Batch: 1280 Loss: 0.050258526713727054\n",
            "Batch: 1600 Loss: 0.034220028480099196\n",
            "Batch: 1920 Loss: 0.040665442104854344\n",
            "Batch: 2240 Loss: 0.052629333958328195\n",
            "Batch: 2560 Loss: 0.04202491829187408\n",
            "Batch: 2880 Loss: 0.045719691730882674\n",
            "Batch: 3200 Loss: 0.05402968926652692\n",
            "Batch: 3520 Loss: 0.04115279496030602\n",
            "Batch: 3840 Loss: 0.04464805503123737\n",
            "Batch: 4160 Loss: 0.050388264372635674\n",
            "Batch: 4480 Loss: 0.045571853117506764\n",
            "Batch: 4800 Loss: 0.05250535064314296\n",
            "Batch: 5120 Loss: 0.04357163178269441\n",
            "Batch: 5440 Loss: 0.05435681772304158\n",
            "Batch: 5760 Loss: 0.05161062571586477\n",
            "Batch: 6080 Loss: 0.04733525451041663\n",
            "Batch: 6400 Loss: 0.041129228204086224\n",
            "Batch: 6720 Loss: 0.05476980936691952\n",
            "Batch: 7040 Loss: 0.039569846163197094\n",
            "Batch: 7360 Loss: 0.04046899096526956\n",
            "Batch: 7680 Loss: 0.02993640102731356\n",
            "Batch: 8000 Loss: 0.029312154710998205\n",
            "Batch: 8320 Loss: 0.05253976314414626\n",
            "Batch: 8640 Loss: 0.042127491963051565\n",
            "Batch: 8960 Loss: 0.05659330391322185\n",
            "Batch: 9280 Loss: 0.06365632020230647\n",
            "Batch: 9600 Loss: 0.044366165739968294\n",
            "Batch: 9920 Loss: 0.0715537363006289\n",
            "Batch: 10240 Loss: 0.04223292755882178\n",
            "Batch: 10560 Loss: 0.040549073949559755\n",
            "Batch: 10880 Loss: 0.057039304994030676\n",
            "Epoch: 397\n",
            "Batch: 0 Loss: 0.04060771919419973\n",
            "Batch: 320 Loss: 0.03838845557606563\n",
            "Batch: 640 Loss: 0.024340121251085582\n",
            "Batch: 960 Loss: 0.03455879915309245\n",
            "Batch: 1280 Loss: 0.05028667038329862\n",
            "Batch: 1600 Loss: 0.032479719952010815\n",
            "Batch: 1920 Loss: 0.043863171742065624\n",
            "Batch: 2240 Loss: 0.041829965152911215\n",
            "Batch: 2560 Loss: 0.0341746535526349\n",
            "Batch: 2880 Loss: 0.04125125452851411\n",
            "Batch: 3200 Loss: 0.03644163956701078\n",
            "Batch: 3520 Loss: 0.056899364641559404\n",
            "Batch: 3840 Loss: 0.030185730670286126\n",
            "Batch: 4160 Loss: 0.04466504305956379\n",
            "Batch: 4480 Loss: 0.046018439461997754\n",
            "Batch: 4800 Loss: 0.06096036233917984\n",
            "Batch: 5120 Loss: 0.05550157025139239\n",
            "Batch: 5440 Loss: 0.03742508620744892\n",
            "Batch: 5760 Loss: 0.0534876311862392\n",
            "Batch: 6080 Loss: 0.055719182615142235\n",
            "Batch: 6400 Loss: 0.03260133541207775\n",
            "Batch: 6720 Loss: 0.05545975203752419\n",
            "Batch: 7040 Loss: 0.06124970573873918\n",
            "Batch: 7360 Loss: 0.06286317189108151\n",
            "Batch: 7680 Loss: 0.05219031357648579\n",
            "Batch: 8000 Loss: 0.0464127457241245\n",
            "Batch: 8320 Loss: 0.053019980911825804\n",
            "Batch: 8640 Loss: 0.04876494515457798\n",
            "Batch: 8960 Loss: 0.02974223564072054\n",
            "Batch: 9280 Loss: 0.04615824195160138\n",
            "Batch: 9600 Loss: 0.04045775298133819\n",
            "Batch: 9920 Loss: 0.057766705960922954\n",
            "Batch: 10240 Loss: 0.049974962731511516\n",
            "Batch: 10560 Loss: 0.058668947444661945\n",
            "Batch: 10880 Loss: 0.045948782777081205\n",
            "Epoch: 398\n",
            "Batch: 0 Loss: 0.03268960348085205\n",
            "Batch: 320 Loss: 0.04345009973437766\n",
            "Batch: 640 Loss: 0.062413336449735214\n",
            "Batch: 960 Loss: 0.06576649161285099\n",
            "Batch: 1280 Loss: 0.03596074141860279\n",
            "Batch: 1600 Loss: 0.04883530910281152\n",
            "Batch: 1920 Loss: 0.0357230738582218\n",
            "Batch: 2240 Loss: 0.06447287771073781\n",
            "Batch: 2560 Loss: 0.02752095738998512\n",
            "Batch: 2880 Loss: 0.03864782294684078\n",
            "Batch: 3200 Loss: 0.04701598108057342\n",
            "Batch: 3520 Loss: 0.03197414759258032\n",
            "Batch: 3840 Loss: 0.055451180433213114\n",
            "Batch: 4160 Loss: 0.06938773925326842\n",
            "Batch: 4480 Loss: 0.059744021838935374\n",
            "Batch: 4800 Loss: 0.048313954976569054\n",
            "Batch: 5120 Loss: 0.07117656739952832\n",
            "Batch: 5440 Loss: 0.0509539023601517\n",
            "Batch: 5760 Loss: 0.07221679169000585\n",
            "Batch: 6080 Loss: 0.03693284392207607\n",
            "Batch: 6400 Loss: 0.0443551643439762\n",
            "Batch: 6720 Loss: 0.059569221980416726\n",
            "Batch: 7040 Loss: 0.03327489690968267\n",
            "Batch: 7360 Loss: 0.051177129875147276\n",
            "Batch: 7680 Loss: 0.033690156265209\n",
            "Batch: 8000 Loss: 0.053633304224607006\n",
            "Batch: 8320 Loss: 0.05657653643278944\n",
            "Batch: 8640 Loss: 0.031121844202116593\n",
            "Batch: 8960 Loss: 0.03281381690887273\n",
            "Batch: 9280 Loss: 0.07302908411666727\n",
            "Batch: 9600 Loss: 0.04489421944068406\n",
            "Batch: 9920 Loss: 0.03474061243269937\n",
            "Batch: 10240 Loss: 0.0653879794527536\n",
            "Batch: 10560 Loss: 0.05335593691703257\n",
            "Batch: 10880 Loss: 0.057033460960566065\n",
            "Epoch: 399\n",
            "Batch: 0 Loss: 0.05767853861777151\n",
            "Batch: 320 Loss: 0.036826398109797875\n",
            "Batch: 640 Loss: 0.05047940920892572\n",
            "Batch: 960 Loss: 0.03162288959051081\n",
            "Batch: 1280 Loss: 0.047364377169803004\n",
            "Batch: 1600 Loss: 0.10882984705899294\n",
            "Batch: 1920 Loss: 0.04247869257263421\n",
            "Batch: 2240 Loss: 0.04073394645988926\n",
            "Batch: 2560 Loss: 0.0553205583055317\n",
            "Batch: 2880 Loss: 0.06751218326210684\n",
            "Batch: 3200 Loss: 0.04297628711707534\n",
            "Batch: 3520 Loss: 0.04469667912252148\n",
            "Batch: 3840 Loss: 0.05919205050714706\n",
            "Batch: 4160 Loss: 0.06711572119147118\n",
            "Batch: 4480 Loss: 0.033498133356447926\n",
            "Batch: 4800 Loss: 0.0418515674389212\n",
            "Batch: 5120 Loss: 0.05376842689711163\n",
            "Batch: 5440 Loss: 0.06139579050717005\n",
            "Batch: 5760 Loss: 0.036854942962838456\n",
            "Batch: 6080 Loss: 0.04125601206790655\n",
            "Batch: 6400 Loss: 0.037796887710424325\n",
            "Batch: 6720 Loss: 0.05053822251822294\n",
            "Batch: 7040 Loss: 0.03500653920211691\n",
            "Batch: 7360 Loss: 0.05002311284000907\n",
            "Batch: 7680 Loss: 0.07040040038572282\n",
            "Batch: 8000 Loss: 0.05379180811992533\n",
            "Batch: 8320 Loss: 0.04616111156856903\n",
            "Batch: 8640 Loss: 0.032648073059993246\n",
            "Batch: 8960 Loss: 0.0375245430247612\n",
            "Batch: 9280 Loss: 0.059335782672707914\n",
            "Batch: 9600 Loss: 0.06027575832476146\n",
            "Batch: 9920 Loss: 0.03533003998718881\n",
            "Batch: 10240 Loss: 0.025600190144870968\n",
            "Batch: 10560 Loss: 0.030260823854612214\n",
            "Batch: 10880 Loss: 0.037533553698021055\n",
            "Epoch: 400\n",
            "Batch: 0 Loss: 0.03995669144717143\n",
            "Batch: 320 Loss: 0.059412990982806294\n",
            "Batch: 640 Loss: 0.040212288030422844\n",
            "Batch: 960 Loss: 0.041434785469705734\n",
            "Batch: 1280 Loss: 0.052903444272649945\n",
            "Batch: 1600 Loss: 0.041665360142524366\n",
            "Batch: 1920 Loss: 0.04820857041928047\n",
            "Batch: 2240 Loss: 0.05897470010487738\n",
            "Batch: 2560 Loss: 0.044239604505364974\n",
            "Batch: 2880 Loss: 0.04745481258052004\n",
            "Batch: 3200 Loss: 0.04576404592526379\n",
            "Batch: 3520 Loss: 0.04293517289154572\n",
            "Batch: 3840 Loss: 0.06103699407826535\n",
            "Batch: 4160 Loss: 0.04873237218864025\n",
            "Batch: 4480 Loss: 0.04379121996819528\n",
            "Batch: 4800 Loss: 0.051987181160095715\n",
            "Batch: 5120 Loss: 0.03714000913904825\n",
            "Batch: 5440 Loss: 0.05743014537768773\n",
            "Batch: 5760 Loss: 0.057456933428275\n",
            "Batch: 6080 Loss: 0.038662745892793456\n",
            "Batch: 6400 Loss: 0.04946168991066091\n",
            "Batch: 6720 Loss: 0.06391573165287955\n",
            "Batch: 7040 Loss: 0.03414498985710195\n",
            "Batch: 7360 Loss: 0.04998397918511354\n",
            "Batch: 7680 Loss: 0.07652501734291639\n",
            "Batch: 8000 Loss: 0.04405796243870234\n",
            "Batch: 8320 Loss: 0.04082077168265249\n",
            "Batch: 8640 Loss: 0.04665803653416727\n",
            "Batch: 8960 Loss: 0.049272699506060835\n",
            "Batch: 9280 Loss: 0.03396160309066203\n",
            "Batch: 9600 Loss: 0.054546079803158114\n",
            "Batch: 9920 Loss: 0.054167757485199905\n",
            "Batch: 10240 Loss: 0.038090439877722794\n",
            "Batch: 10560 Loss: 0.05535988332730392\n",
            "Batch: 10880 Loss: 0.03719436807241188\n"
          ]
        }
      ],
      "source": [
        "losses = diffusion.train(X_train, y_train, 400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "KfkFrab-aTHA",
        "outputId": "bd6176dc-9808-4f15-f973-05f152f12adf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1f71c63ba2d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZdB-m-1Vr_ZO"
      },
      "outputs": [],
      "source": [
        "diffusion.load_net(checkpoint_name_model = \"model_checkpoint.h50\", checkpoint_name_discriminator_model = \"discriminator_model_checkpoint.h50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tJIbVvFTMj0c"
      },
      "outputs": [],
      "source": [
        "all_samples, all_labels = diffusion.ddim_sample(200, w=0.7, size=64)\n",
        "\n",
        "for i in range(20):\n",
        "  samples, labels = diffusion.ddim_sample(200, w=0.7, size=64)\n",
        "  all_samples = np.concatenate((all_samples, samples), axis=0)\n",
        "  all_labels = np.concatenate((all_labels, labels), axis=0)\n",
        "\n",
        "all_samples = np.asarray(all_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_8AQqmoNYgF",
        "outputId": "6a03d7f2-b510-4a09-a002-075cac29dd24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1344, 30, 45)\n",
            "(1344,)\n",
            "(1215, 30, 45)\n",
            "(1215,)\n"
          ]
        }
      ],
      "source": [
        "print(all_samples.shape)\n",
        "print(all_labels.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z2ikPNNvDODK"
      },
      "outputs": [],
      "source": [
        "np.save('all_samples.npy', all_samples)\n",
        "np.save('all_labels.npy', all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n-5VMp5PSM0"
      },
      "outputs": [],
      "source": [
        "all_samples = np.load('all_samples.npy')\n",
        "all_labels = np.load('all_labels.npy')\n",
        "window_len = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfiQr1tqHn_w",
        "outputId": "f4715fc8-a42d-43de-b711-6f5c2cf5be3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "connections = [\n",
        "    (0, 1),   # Connect pelvis to right hip\n",
        "    (1, 2),   # Connect right hip to right knee\n",
        "    (2, 3),   # Connect right knee to right ankle\n",
        "    (0, 4),   # Connect pelvis to left hip\n",
        "    (4, 5),   # Connect left hip to left knee\n",
        "    (5, 6),   # Connect left knee to left ankle\n",
        "    (0, 7),  # Connect pelvis to neck\n",
        "    (7, 8), # Connect neck to head\n",
        "    (7, 9),   # Connect neck to right shoulder\n",
        "    (9, 10),   # Connect right shoulder to right elbow\n",
        "    (10, 11),  # Connect right elbow to right hand\n",
        "    (7, 12),   # Connect neck to left shoulder\n",
        "    (12, 13),  # Connect left shoulder to left elbow\n",
        "    (13, 14)    # Connect left elbow to left hand\n",
        "]\n",
        "\n",
        "\n",
        "def init():\n",
        "    ax.cla()\n",
        "    ax.set_xlim(-2, 2)  # Adjust the limits according to your data\n",
        "    ax.set_ylim(-2, 2)  # Adjust the limits according to your data\n",
        "    ax.set_zlim(-2, 2)  # Adjust the limits according to your data\n",
        "    return fig,\n",
        "\n",
        "def update(frame):\n",
        "    ax.cla()\n",
        "    ax.set_xlim(-2, 2)  # Adjust the limits according to your data\n",
        "    ax.set_ylim(-2, 2)  # Adjust the limits according to your data\n",
        "    ax.set_zlim(-2, 2)  # Adjust the limits according to your data\n",
        "    for connection in connections:\n",
        "        x = two_sec_window[frame, [connection[0], connection[1], connection[1]]][:, 0]\n",
        "        y = two_sec_window[frame, [connection[0], connection[1], connection[1]]][:, 1]\n",
        "        z = two_sec_window[frame, [connection[0], connection[1], connection[1]]][:, 2]\n",
        "\n",
        "        ax.plot(x, z, y, color='b')\n",
        "    return fig,\n",
        "\n",
        "i = np.random.randint(len(all_labels))\n",
        "\n",
        "#two_sec_window = X_train[i,  :]\n",
        "print(all_labels[i])\n",
        "two_sec_window = all_samples[i, :]\n",
        "two_sec_window = two_sec_window.reshape((1, window_len*15*3))\n",
        "two_sec_window = scaler.inverse_transform(two_sec_window)\n",
        "two_sec_window = two_sec_window.reshape((window_len ,15, 3))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Define the desired frames per second\n",
        "fps = 15\n",
        "\n",
        "# Calculate the interval in milliseconds based on the desired fps\n",
        "interval = 1000 / fps\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=two_sec_window.shape[0], init_func=init, blit=True, interval=interval)\n",
        "\n",
        "# Save the animation as an .mp4 file\n",
        "#ani.save('skeleton_animation_' + str(i) + '_' + str(all_labels[i]) +'.gif', writer='ffmpeg')\n",
        "\n",
        "HTML(ani.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aW2abc2my4-"
      },
      "outputs": [],
      "source": [
        "ani.save('skeleton_animation_' + str(i) + '_' + str(all_labels[i]) +'.gif', writer='ffmpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87x5xkMYy8ab"
      },
      "outputs": [],
      "source": [
        "diffusion.train_discriminator(all_samples, all_labels, 50)\n",
        "#diffusion.train_discriminator(X_test, y_test, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD4WZipD8E3a",
        "outputId": "31b6df78-2331-442f-e523-fd641a5aad63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 4s 167ms/step - loss: 3.4990e-06 - accuracy: 1.0000\n",
            "Test Loss: 3.498963678794098e-06\n",
            "Test Accuracy: 1.0\n",
            "21/21 [==============================] - 5s 229ms/step - loss: 0.6869 - accuracy: 0.8891\n",
            "Test Loss: 0.6869333386421204\n",
            "Test Accuracy: 0.8891369104385376\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "diffusion.discriminator_model.compile(\n",
        "    optimizer='adam',  # Or any other optimizer\n",
        "    loss='sparse_categorical_crossentropy',  # Or 'categorical_crossentropy', etc., depending on your label format\n",
        "    metrics=['accuracy']  # Optional, but useful for evaluation\n",
        ")\n",
        "\n",
        "evaluation = diffusion.discriminator_model.evaluate(X_test, y_test, batch_size=64, verbose=1)\n",
        "print(\"Test Loss:\", evaluation[0])\n",
        "print(\"Test Accuracy:\", evaluation[1])\n",
        "\n",
        "evaluation = diffusion.discriminator_model.evaluate(all_samples, all_labels, batch_size=64, verbose=1)\n",
        "print(\"Test Loss:\", evaluation[0])\n",
        "print(\"Test Accuracy:\", evaluation[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXAqLoGu3drO",
        "outputId": "25e89c20-7313-4fdf-fb56-c9871ac60911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1344, 1350)\n",
            "(1344,)\n",
            "(1215, 1350)\n",
            "(1215,)\n"
          ]
        }
      ],
      "source": [
        "all_samples_knn_input = all_samples.reshape(all_samples.shape[0], all_samples.shape[1]*all_samples.shape[2])\n",
        "X_test_knn_input = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
        "\n",
        "print(all_samples_knn_input.shape)\n",
        "print(all_labels.shape)\n",
        "\n",
        "print(X_test_knn_input.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH9LDOGz689f",
        "outputId": "b23ffe55-d061-48e2-b614-55b4c19cb427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.47%\n",
            "Accuracy: 97.17%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "k = 5\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "knn_classifier.fit(all_samples_knn_input, all_labels)\n",
        "\n",
        "y_pred = knn_classifier.predict(X_test_knn_input)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "y_pred = knn_classifier.predict(all_samples_knn_input)\n",
        "accuracy = accuracy_score(all_labels, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw8cWdlOt26f"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/drive/MyDrive/checkpoints"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}